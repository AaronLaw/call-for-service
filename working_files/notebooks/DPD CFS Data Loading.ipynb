{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Loading, Cleaning, and Normalization\n",
    "We need to load the data from .csv into Postgres.  We also need to normalize the data to make analysis easy.  We'll use Pandas to deal with the .csv loading and data storage.\n",
    "\n",
    "Files we need to load:\n",
    "- cfs_2014_inmain.csv (CFS data)\n",
    "- cfs_xxx2014_incilog.csv (CFS event data -- one for each month)\n",
    "- cfs_2014_lwmain.csv (incident data)\n",
    "- cfs_2014_lwmodop.csv (incident modus operandi data)\n",
    "- LWMAIN.THING.csv (incident lookup tables, where THING is one of the following: CSSTATUS, EMDIVISION, EMSECTION, EMUNIT, INSTSTATS, PREMISE, or WEAPON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use pandas and sqlalchemy to stuff the data into a local instance of postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import datetime as dt\n",
    "from IPython.display import display\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the tables before touching the data so they have all the proper constraints.  Pandas' to_sql method, while helpful, won't handle the constraints automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Database DDL\n",
    "\n",
    "Code to create the database schema is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHANGE CREDENTIALS AS APPROPRIATE\n",
    "engine = create_engine('postgresql://<USER_NAME>:<PASSWORD>@freyja.rtp.rti.org:5432/cfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_db():\n",
    "    \"\"\"\n",
    "    Remove and recreate tables to prepare for reloading the db\n",
    "    \"\"\"\n",
    "    engine.execute(\"DROP TABLE IF EXISTS note CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS call CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS call_log CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS ucr_desc CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS incident CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS modus_operandi CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS mo_item CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS bureau CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS case_status CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS division CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS unit CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS investigation_status CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS weapon CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS weapon_group CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS premise CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS premise_group CASCADE;\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE call\n",
    "    (\n",
    "      call_id bigint NOT NULL,\n",
    "      call_time timestamp without time zone,\n",
    "      call_dow bigint,\n",
    "      case_id text,\n",
    "      call_source text,\n",
    "      primary_unit text,\n",
    "      first_dispatched text,\n",
    "      street_num text,\n",
    "      street_name text,\n",
    "      city_desc text,\n",
    "      zip text,\n",
    "      crossroad1 text,\n",
    "      crossroad2 text,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      service text,\n",
    "      agency text,\n",
    "      beat text,\n",
    "      district text,\n",
    "      sector text,\n",
    "      business text,\n",
    "      nature_code text,\n",
    "      nature_desc text,\n",
    "      priority text,\n",
    "      report_only bigint,\n",
    "      cancelled bigint,\n",
    "      time_enroute timestamp without time zone,\n",
    "      time_finished timestamp without time zone,\n",
    "      first_unit_dispatch timestamp without time zone,\n",
    "      first_unit_enroute timestamp without time zone,\n",
    "      first_unit_arrive timestamp without time zone,\n",
    "      first_unit_transport timestamp without time zone,\n",
    "      last_unit_clear timestamp without time zone,\n",
    "      time_closed timestamp without time zone,\n",
    "      reporting_unit text,\n",
    "      close_code text,\n",
    "      close_comm text,\n",
    "      CONSTRAINT call_id_pkey PRIMARY KEY (call_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE note\n",
    "    (\n",
    "      note_id serial NOT NULL,\n",
    "      text text,\n",
    "      \"timestamp\" timestamp without time zone,\n",
    "      author text,\n",
    "      call_id bigint,\n",
    "      CONSTRAINT note_pkey PRIMARY KEY (note_id),\n",
    "      CONSTRAINT note_call_id_fkey FOREIGN KEY (call_id) REFERENCES call (call_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE call_log\n",
    "    (\n",
    "      call_log_id bigint NOT NULL,\n",
    "      transaction_code text,\n",
    "      transaction_desc text,\n",
    "      \"timestamp\" timestamp without time zone,\n",
    "      call_id bigint,\n",
    "      unit_code text,\n",
    "      radio_or_event text,\n",
    "      unitper_id bigint,\n",
    "      close_code text,\n",
    "      --CONSTRAINT call_log_call_id_fkey FOREIGN KEY (call_id) REFERENCES call (call_id) --nullable\n",
    "      CONSTRAINT call_log_pkey PRIMARY KEY (call_log_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE ucr_desc\n",
    "    (\n",
    "      ucr_long_desc text,\n",
    "      ucr_short_desc text NOT NULL,\n",
    "      CONSTRAINT ucr_desc_pkey PRIMARY KEY (ucr_short_desc)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE bureau\n",
    "    (\n",
    "      bureau_code text,\n",
    "      bureau_desc text,\n",
    "      CONSTRAINT bureau_pkey PRIMARY KEY (bureau_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE division\n",
    "    (\n",
    "      division_code text,\n",
    "      division_desc text,\n",
    "      CONSTRAINT division_pkey PRIMARY KEY (division_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE investigation_status\n",
    "    (\n",
    "      investigation_status_code text,\n",
    "      investigation_status_desc text,\n",
    "      CONSTRAINT investigation_status_pkey PRIMARY KEY (investigation_status_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE case_status\n",
    "    (\n",
    "      case_status_code text,\n",
    "      case_status_desc text,\n",
    "      CONSTRAINT case_status_pkey PRIMARY KEY (case_status_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE unit\n",
    "    (\n",
    "      unit_code text,\n",
    "      unit_desc text,\n",
    "      CONSTRAINT unit_pkey PRIMARY KEY (unit_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE weapon_group\n",
    "    (\n",
    "      weapon_group text,\n",
    "      weapon_desc text,\n",
    "      CONSTRAINT weapon_group_pkey PRIMARY KEY (weapon_desc)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE premise_group\n",
    "    (\n",
    "      premise_group text,\n",
    "      premise_desc text,\n",
    "      CONSTRAINT premise_group_pkey PRIMARY KEY (premise_desc)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE weapon\n",
    "    (\n",
    "      weapon_code text,\n",
    "      weapon_desc text,\n",
    "      CONSTRAINT weapon_pkey PRIMARY KEY (weapon_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE premise\n",
    "    (\n",
    "      premise_code text,\n",
    "      premise_desc text,\n",
    "      CONSTRAINT premise_pkey PRIMARY KEY (premise_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE incident\n",
    "    (\n",
    "      incident_id bigint NOT NULL,\n",
    "      call_id bigint,\n",
    "      time_filed timestamp without time zone,\n",
    "      street_num text,\n",
    "      street_name text,\n",
    "      city text,\n",
    "      zip text,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      beat text,\n",
    "      district text,\n",
    "      sector text,\n",
    "      premise_code text,\n",
    "      weapon_code text,\n",
    "      domestic text,\n",
    "      juvenile text,\n",
    "      gang_related text,\n",
    "      emp_bureau_code text,\n",
    "      emp_division_code text,\n",
    "      emp_unit_code text,\n",
    "      num_officers integer,\n",
    "      investigation_status_code text,\n",
    "      investigator_unit_code text,\n",
    "      case_status_code text,\n",
    "      lwchrgid bigint,\n",
    "      charge_seq bigint,\n",
    "      ucr_code bigint,\n",
    "      ucr_short_desc text,\n",
    "      attempted_or_committed text,\n",
    "      CONSTRAINT incident_pkey PRIMARY KEY (incident_id)\n",
    "      \n",
    "      --EVERYTHING IS NULLABLE AAAAAAGH\n",
    "      --CONSTRAINT incident_case_status_code_fkey --nullable\n",
    "      --  FOREIGN KEY (case_status_code) REFERENCES case_status (case_status_code),\n",
    "      --CONSTRAINT incident_emp_bureau_code_fkey\n",
    "      --  FOREIGN KEY (emp_bureau_code) REFERENCES bureau (bureau_code),\n",
    "      --CONSTRAINT incident_emp_division_code_fkey\n",
    "      --  FOREIGN KEY (emp_division_code) REFERENCES division (division_code),\n",
    "      --CONSTRAINT incident_emp_unit_code_fkey\n",
    "      --  FOREIGN KEY (emp_unit_code) REFERENCES unit (unit_code),\n",
    "      --CONSTRAINT incident_investigator_unit_code_fkey -- nullable\n",
    "      --  FOREIGN KEY (investigator_unit_code) REFERENCES unit (unit_code),\n",
    "      --CONSTRAINT incident_investigation_status_code_fkey\n",
    "      --  FOREIGN KEY (investigation_status_code) REFERENCES investigation_status (investigation_status_code),\n",
    "      --CONSTRAINT incident_premise_code_fkey\n",
    "      --  FOREIGN KEY (premise_code) REFERENCES premise (premise_code),\n",
    "      --CONSTRAINT incident_weapon_code_fkey\n",
    "      --  FOREIGN KEY (weapon_code) REFERENCES weapon (weapon_code),\n",
    "      --CONSTRAINT incident_call_id_fkey\n",
    "      --  FOREIGN KEY (call_id) REFERENCES call (call_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE mo_item\n",
    "    (\n",
    "      mo_item_code text,\n",
    "      mo_item_desc text,\n",
    "      mo_group_code text,\n",
    "      mo_group_desc text,\n",
    "      CONSTRAINT mo_item_pkey PRIMARY KEY (mo_item_code, mo_group_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE modus_operandi\n",
    "    (\n",
    "      incident_id bigint,\n",
    "      mo_id bigint,\n",
    "      mo_group_code text,\n",
    "      mo_item_code text,\n",
    "      CONSTRAINT mo_pkey PRIMARY KEY (mo_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "reset_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##cfs_2014_inmain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "timestamp_expr = re.compile(\"\\[(\\d{2}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}) (.+?)\\]\")\n",
    "\n",
    "def split_notes_dict(notes,call_id):\n",
    "    \"\"\"\n",
    "    Return a list of dicts.  Each dict represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    dicts = []\n",
    "    regex_split = timestamp_expr.split(notes)[:-1]  # get rid of the last empty string created by the split\n",
    "    for i in range(0,len(regex_split),3):\n",
    "        text = regex_split[i].strip()\n",
    "        timestamp = dt.datetime.strptime(regex_split[i+1], \"%m/%d/%y %H:%M:%S\")\n",
    "        author = regex_split[i+2]\n",
    "        dicts.append({\"text\": text, \"timestamp\": timestamp, \"author\": author, \"call_id\": call_id})\n",
    "    return dicts\n",
    "\n",
    "def split_notes(notes):\n",
    "    \"\"\"\n",
    "    Return a list of tuples.  Each tuple represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    notes = str(notes)\n",
    "    tuples = []\n",
    "    regex_split = timestamp_expr.split(notes)[:-1]  # get rid of the last empty string created by the split\n",
    "    for i in range(0,len(regex_split),3):\n",
    "        text = regex_split[i].strip()\n",
    "        timestamp = dt.datetime.strptime(regex_split[i+1], \"%m/%d/%y %H:%M:%S\")\n",
    "        author = regex_split[i+2]\n",
    "        tuples.append((text, timestamp, author))\n",
    "    return tuples\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "chunksize = 20000\n",
    "j = 0\n",
    "\n",
    "# We need to map the inmain columns to the renamed columns in the call table\n",
    "# if an inmain column isn't in this dict, it means we need to drop it\n",
    "call_mappings = {\n",
    "    \"inci_id\": \"call_id\",\n",
    "    \"calltime\": \"call_time\",\n",
    "    \"calldow\": \"call_dow\",\n",
    "    \"case_id\": \"case_id\",\n",
    "    \"callsource\": \"call_source\",\n",
    "    \"primeunit\": \"primary_unit\",\n",
    "    \"firstdisp\": \"first_dispatched\",\n",
    "    \"streetno\": \"street_num\",\n",
    "    \"streetonly\": \"street_name\",\n",
    "    \"citydesc\": \"city_desc\",\n",
    "    \"zip\": \"zip\",\n",
    "    \"crossroad1\": \"crossroad1\",\n",
    "    \"crossroad2\": \"crossroad2\",\n",
    "    \"geox\": \"geox\",\n",
    "    \"geoy\": \"geoy\",\n",
    "    \"service\": \"service\",\n",
    "    \"agency\": \"agency\",\n",
    "    \"statbeat\": \"beat\",\n",
    "    \"district\": \"district\",\n",
    "    \"ra\": \"sector\",\n",
    "    \"business\": \"business\",\n",
    "    \"naturecode\": \"nature_code\",\n",
    "    \"nature\": \"nature_desc\",\n",
    "    \"priority\": \"priority\",\n",
    "    \"rptonly\": \"report_only\",\n",
    "    \"cancelled\": \"cancelled\",\n",
    "    \"timeroute\": \"time_enroute\",\n",
    "    \"timefini\": \"time_finished\",\n",
    "    \"firstdtm\": \"first_unit_dispatch\",\n",
    "    \"firstenr\": \"first_unit_enroute\",\n",
    "    \"firstarrv\": \"first_unit_arrive\",\n",
    "    \"firsttran\": \"first_unit_transport\",\n",
    "    \"lastclr\": \"last_unit_clear\",\n",
    "    \"timeclose\": \"time_closed\",\n",
    "    \"reptaken\": \"reporting_unit\",\n",
    "    \"closecode\": \"close_code\",\n",
    "    \"closecomm\": \"close_comm\"\n",
    "}\n",
    "\n",
    "keep_columns = set(call_mappings.keys())\n",
    "\n",
    "for call in pd.read_csv('../csv_data/cfs_2014_inmain.csv', chunksize=chunksize, iterator=True, encoding='ISO-8859-1',\n",
    "                       low_memory=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    nice, clean iterative algorithm for separating out the notes data -- unfortunately, it's prohibitively slow\n",
    "    (~3 mins per 25k record or thereabouts)\n",
    "    \"\"\"\n",
    "    #for index, row in call.iterrows():\n",
    "    #    note = note.append(pd.DataFrame(split_notes_dict(str(row['notes']), row['inci_id'])))\n",
    "        #if call.iloc[i]['naturecode'] not in nature_set:\n",
    "        #    nature_set.add(call.iloc[i]['naturecode'])\n",
    "        #    nature = nature.append(pd.DataFrame({\"nature_code\": [call.iloc[i]['naturecode']],\n",
    "        #                                \"nature_desc\": [call.iloc[i]['nature']]}))\n",
    "   \n",
    "    \"\"\"\n",
    "    Horrid ugly algorithm for separating out the notes data -- it's faster by about 10x though\n",
    "    Pandas is really slow when iterating on rows, so we have to do all the transformations to a whole series/list\n",
    "    at a time\n",
    "    \"\"\"\n",
    "    # Create a new series, which is (for each call) a list of tuples containing the text, author, and timestamp\n",
    "    # of that call:\n",
    "    # ex. Series([\"one long string with text, author, timestamp for all remarks\"]) -> \n",
    "    #     Series([(text, author, timestamp), (text2, author2, timestamp2)])\n",
    "    call['collected_notes'] = call['notes'].apply(split_notes)\n",
    "    \n",
    "    # Combine the previous series with the inci_id of each row, preserving the relationship between inci_id\n",
    "    # and each individual remark, then convert it to a list so we can reduce and map\n",
    "    # ex. Series([(text, author, timestamp), (text2, author2, timestamp2)]) ->\n",
    "    #     [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)]\n",
    "    combined_notes = call['collected_notes'].combine(call['inci_id'],\n",
    "                                                          lambda x,y: [(e,y) for e in x]).tolist()\n",
    "    \n",
    "    # Reduce the list of lists using extend; instead of a list of lists of tuples, we have one long list of\n",
    "    # nested tuples\n",
    "    # ex. [[((text, author, timestamp), inci_id)], [((text2, author2, timestamp2), inci_id2)]] ->\n",
    "    #     [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)]\n",
    "    extended_notes = []\n",
    "    for l in combined_notes:\n",
    "        extended_notes.extend(l)\n",
    "    \n",
    "    # Flatten the tuples, so we have a list of non-nested tuples\n",
    "    # ex. [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)] ->\n",
    "    #     [(text, author, timestamp, inci_id), (text2, author2, timestamp2, inci_id2)]\n",
    "    extended_notes = map(lambda x: (x[0][0],x[0][1],x[0][2],x[1]), extended_notes)\n",
    "    \n",
    "    # Create a dataframe from the list of tuples (whew)\n",
    "    note = pd.DataFrame.from_records(extended_notes, columns=['text','timestamp','author','call_id'])\n",
    "    \n",
    "    # drop unnecessary columns\n",
    "    for c in call.columns:\n",
    "        if c not in keep_columns:\n",
    "            call = call.drop(c, axis=1)   \n",
    "    \n",
    "    # rename to the CFS Analytics column names\n",
    "    call.rename(columns=call_mappings, inplace=True)\n",
    "    \n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    # Perform datetime conversions\n",
    "    call['call_time'] = pd.to_datetime(call['call_time'])\n",
    "    call['time_enroute'] = pd.to_datetime(call['time_enroute'])\n",
    "    call['time_finished'] = pd.to_datetime(call['time_finished'])\n",
    "    call['first_unit_dispatch'] = pd.to_datetime(call['first_unit_dispatch'])\n",
    "    call['first_unit_enroute'] = pd.to_datetime(call['first_unit_enroute'])\n",
    "    call['first_unit_arrive'] = pd.to_datetime(call['first_unit_arrive'])\n",
    "    call['first_unit_transport'] = pd.to_datetime(call['first_unit_transport'])\n",
    "    call['last_unit_clear'] = pd.to_datetime(call['last_unit_clear'])\n",
    "    call['time_closed'] = pd.to_datetime(call['time_closed'])\n",
    "\n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "    \n",
    "    # get rid of excess whitespace\n",
    "    call = call.applymap(safe_strip)\n",
    "    note = note.applymap(safe_strip)\n",
    "    \n",
    "    # store in the database\n",
    "    call.to_sql('call', engine, index=False, if_exists='append')\n",
    "    note.to_sql('note', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_xxx2014_incilog.csv\n",
    "There is one of these for each month, so we have to load them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "months = (\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\")\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "for month in months:\n",
    "    start = dt.datetime.now()\n",
    "    print(\"Starting load for month: %s\" % (month))\n",
    "    # load the data in chunks so we don't use too much memory\n",
    "    chunksize = 20000\n",
    "    j = 0\n",
    "\n",
    "    # We need to map the incilog columns to the renamed columns in the call_log table\n",
    "    # if an incilog column isn't in this dict, it means we need to drop it\n",
    "    call_log_mappings = {\n",
    "        \"incilogid\": \"call_log_id\",\n",
    "        \"transtype\": \"transaction_code\",\n",
    "        \"descript\": \"transaction_desc\",\n",
    "        \"timestamp\": \"timestamp\",\n",
    "        \"inci_id\": \"call_id\",\n",
    "        \"unitcode\": \"unit_code\",\n",
    "        \"radorev\": \"radio_or_event\",\n",
    "        \"unitperid\": \"unitper_id\",\n",
    "        \"closecode\": \"close_code\"\n",
    "    }\n",
    "    \n",
    "    keep_columns = set(call_log_mappings.keys())\n",
    "\n",
    "    for call_log in pd.read_csv('../csv_data/cfs_%s2014_incilog.csv' % (month), chunksize=chunksize, \n",
    "                           iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "        for c in call_log.columns:\n",
    "            if c not in keep_columns:\n",
    "                call_log = call_log.drop(c, axis=1)\n",
    "\n",
    "        # rename to the CFS Analytics column names\n",
    "        call_log.rename(columns=call_log_mappings, inplace=True)\n",
    "\n",
    "        ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "        ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "            \n",
    "        # Perform datetime conversions\n",
    "        call_log['timestamp'] = pd.to_datetime(call_log['timestamp'])\n",
    "        \n",
    "        # progress update\n",
    "        j+=1\n",
    "        print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "        # strip excess whitespace\n",
    "        call_log = call_log.applymap(safe_strip)\n",
    "        \n",
    "        # store in the database\n",
    "        call_log.to_sql('call_log', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assorted small lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are a million of these, so let's make life easier and reuse all that code\n",
    "lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.CSSTATUS.csv\",\n",
    "        \"table\": \"case_status\",\n",
    "        \"mapping\": {\"code_agcy\": \"case_status_code\", \"descriptn\": \"case_status_desc\"}\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMDIVISION.csv\",\n",
    "        \"table\": \"division\",\n",
    "        \"mapping\": {\"code_agcy\": \"division_code\", \"descriptn\": \"division_desc\"}\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMSECTION.csv\",\n",
    "        \"table\": \"unit\",\n",
    "        \"mapping\": {\"code_agcy\": \"unit_code\", \"descriptn\": \"unit_desc\"}\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMUNIT.csv\",\n",
    "        \"table\": \"bureau\",\n",
    "        \"mapping\": {\"code_agcy\": \"bureau_code\", \"descriptn\": \"bureau_desc\"}\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.INVSTSTATS.csv\",\n",
    "        \"table\": \"investigation_status\",\n",
    "        \"mapping\": {\"code_agcy\": \"investigation_status_code\", \"descriptn\": \"investigation_status_desc\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in lookup_jobs:\n",
    "    print(\"loading %s into %s\" % (job['file'], job['table']))\n",
    "    data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    \n",
    "    keep_columns = set(job['mapping'].keys())\n",
    "    for c in data.columns:\n",
    "        if c not in keep_columns:\n",
    "            data = data.drop(c, axis=1)\n",
    "            \n",
    "    data.rename(columns=job['mapping'], inplace=True)\n",
    "    data.to_sql(job['table'], engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These have to create \"nested\" tables and are a little tougher, but we can still reuse the code\n",
    "\n",
    "nested_lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.PREMISE.csv\",\n",
    "        \"outer_table\": \"premise\",\n",
    "        \"inner_table\": \"premise_group\",\n",
    "        \"outer_cols\": [\"premise_code\", \"premise_desc\"],\n",
    "        \"inner_cols\": [\"premise_group\", \"premise_desc\"]\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.WEAPON.csv\",\n",
    "        \"outer_table\": \"weapon\",\n",
    "        \"inner_table\": \"weapon_group\",\n",
    "        \"outer_cols\": [\"weapon_code\", \"weapon_desc\"],\n",
    "        \"inner_cols\": [\"weapon_group\", \"weapon_desc\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in nested_lookup_jobs:\n",
    "    print(\"loading %s into %s and %s\" % (job['file'], job['outer_table'], job['inner_table']))\n",
    "    data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    \n",
    "    outer_data = pd.concat([data['code_agcy'], data['descriptn_b']], axis=1, keys=job['outer_cols'])\n",
    "    inner_data = pd.concat([data['descriptn_a'], data['descriptn_b']], axis=1, keys=job['inner_cols'])\n",
    "    \n",
    "    outer_data = outer_data.drop_duplicates()\n",
    "    \n",
    "    outer_data.to_sql(job['outer_table'], engine, index=False, if_exists='append')\n",
    "    inner_data.to_sql(job['inner_table'], engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combine_date_time(str_date, str_time):\n",
    "    date = dt.datetime.strptime(str_date, \"%m/%d/%y\")\n",
    "    time = dt.datetime.strptime(str_time, \"%I:%M %p\")\n",
    "    return dt.datetime(date.year, date.month, date.day, time.hour, time.minute)\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "chunksize = 20000\n",
    "j = 0\n",
    "\n",
    "# We need to map the incilog columns to the renamed columns in the call_log table\n",
    "# if an incilog column isn't in this dict, it means we need to drop it\n",
    "incident_mappings = {\n",
    "    \"lwmainid\": \"incident_id\",\n",
    "    \"inci_id\": \"call_id\",\n",
    "    \"time\": \"time_filed\",\n",
    "    \"streetnbr\": \"street_num\",\n",
    "    \"street\": \"street_name\",\n",
    "    \"city\": \"city\",\n",
    "    \"zip\": \"zip\",\n",
    "    \"geox\": \"geox\",\n",
    "    \"geoy\": \"geoy\",\n",
    "    \"tract\": \"beat\",\n",
    "    \"district\": \"district\",\n",
    "    \"reportarea\": \"sector\",\n",
    "    \"premise\": \"premise_code\",\n",
    "    \"weapon\": \"weapon_code\",\n",
    "    \"domestic\": \"domestic\",\n",
    "    \"juvenile\": \"juvenile\",\n",
    "    \"gangrelat\": \"gang_related\",\n",
    "    \"emunit\": \"emp_bureau_code\",\n",
    "    \"emdivision\": \"emp_division_code\",\n",
    "    \"emsection\": \"emp_unit_code\",\n",
    "    \"asst_offcr\": \"num_officers\",\n",
    "    \"invststats\": \"investigation_status_code\",\n",
    "    \"investunit\": \"investigator_unit_code\",\n",
    "    \"csstatus\": \"case_status_code\",\n",
    "    \"lwchrgid\": \"lwchrgid\",\n",
    "    \"chrgcnt\": \"charge_seq\",\n",
    "    \"ucr_code\": \"ucr_code\",\n",
    "    \"arr_chrg\": \"ucr_short_desc\",\n",
    "    \"attm_comp\": \"attempted_or_committed\"\n",
    "}\n",
    "\n",
    "keep_columns = set(incident_mappings.keys())\n",
    "\n",
    "ucr_desc = pd.DataFrame({\"ucr_short_desc\": [], \"ucr_long_desc\": []})\n",
    "\n",
    "for incident in pd.read_csv('../csv_data/cfs_2014_lwmain.csv', chunksize=chunksize, \n",
    "                       iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "    \n",
    "    ucr_desc = ucr_desc.append(pd.concat([ incident['arr_chrg'],\n",
    "                                           incident['chrgdesc'] ],\n",
    "                                        axis=1, keys=['ucr_short_desc', 'ucr_long_desc']))\n",
    "    \n",
    "    # Perform datetime conversions\n",
    "    incident['time'] = incident['date_rept'].combine(incident['time'], combine_date_time)\n",
    "    \n",
    "    for c in incident.columns:\n",
    "        if c not in keep_columns:\n",
    "            incident = incident.drop(c, axis=1)\n",
    "\n",
    "    # rename to the CFS Analytics column names\n",
    "    incident.rename(columns=incident_mappings, inplace=True)\n",
    "\n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    # strip whitespace\n",
    "    incident = incident.applymap(safe_strip)\n",
    "    ucr_desc = ucr_desc.applymap(safe_strip)\n",
    "    \n",
    "    # convert empty strings in num_officers to nulls so we can insert as an int column\n",
    "    incident['num_officers'] = incident['num_officers'].map(lambda x: None if x == '' else x)\n",
    "    \n",
    "    # These \"primary key\" values have two records and I don't want to deal with it\n",
    "    incident = incident[~(incident.incident_id.isin((498659, 503578, 521324)))]\n",
    "    \n",
    "    # incident call_ids don't have the same '20' prefix that the others do, so here we add it\n",
    "    # also get rid of anything pre-2014 because we don't have those in the calls table\n",
    "    incident['call_id'] = incident['call_id'].map(lambda x: x + 2000000000)\n",
    "    incident = incident[incident.call_id > 2014000001]\n",
    "    \n",
    "    # Drop duplicate ucr_descs\n",
    "    ucr_desc = ucr_desc.drop_duplicates()\n",
    "    \n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "    incident = incident.applymap(safe_strip)\n",
    "    \n",
    "    # store in the database\n",
    "    incident.to_sql('incident', engine, index=False, if_exists='append')\n",
    "\n",
    "ucr_desc.to_sql('ucr_desc', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmodop.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "# strange unexplainable crash using the usual 20k chunk size (and 10k sometimes? and 5k sometimes? this makes no sense)\n",
    "# so go with 20k (no) 10k (no) 5k (no)\n",
    "# actually just put your favorite number here and hope it doesn't crash\n",
    "chunksize = 2500\n",
    "j = 0\n",
    "\n",
    "# We need to map the incilog columns to the renamed columns in the call_log table\n",
    "# if an incilog column isn't in this dict, it means we need to drop it\n",
    "modop_mappings = {\n",
    "    \"lwmainid\": \"incident_id\",\n",
    "    \"lwmodopid\": \"mo_id\",\n",
    "    \"mogroup\": \"mo_group_code\",\n",
    "    \"moitem\": \"mo_item_code\"\n",
    "}\n",
    "\n",
    "keep_columns = set(modop_mappings.keys())\n",
    "\n",
    "mo_item = pd.DataFrame({\"mo_item_code\": [], \"mo_item_desc\": [], \"mo_group_code\": [], \"mo_group_desc\": []})\n",
    "\n",
    "for modop in pd.read_csv('../csv_data/cfs_2014_lwmodop.csv', chunksize=chunksize, \n",
    "                       iterator=True, low_memory=False):\n",
    "    \n",
    "    mo_item = mo_item.append(pd.concat([ modop['moitem'],\n",
    "                                         modop['itemdesc'],\n",
    "                                         modop['mogroup'],\n",
    "                                         modop['groupdesc'] ],\n",
    "                                        axis=1, keys=['mo_item_code', 'mo_item_desc',\n",
    "                                                      'mo_group_code', 'mo_group_desc']))\n",
    "\n",
    "    for c in modop.columns:\n",
    "        if c not in keep_columns:\n",
    "            modop = modop.drop(c, axis=1)\n",
    "\n",
    "    # rename to the CFS Analytics column names\n",
    "    modop.rename(columns=modop_mappings, inplace=True)\n",
    "\n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    modop = modop.applymap(safe_strip)\n",
    "    mo_item = mo_item.applymap(safe_strip)\n",
    "    \n",
    "    # The group codes are getting a decimal place for some reason.  convert them to ints\n",
    "    mo_item['mo_group_code'] = mo_item['mo_group_code'].map(lambda x: str(int(x)))\n",
    "    \n",
    "    # Drop duplicate mo_items\n",
    "    mo_item = mo_item.drop_duplicates()\n",
    "    \n",
    "    # Gotta get rid of any of the incident records we had to drop due to duplicate \"primary keys\"\n",
    "    modop = modop[~(modop.incident_id.isin((498659, 503578, 521324)))]\n",
    "    \n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "    \n",
    "    # store in the database\n",
    "    modop.to_sql('modus_operandi', engine, index=False, if_exists='append')\n",
    "\n",
    "# Fix weird exception row causing a key error)\n",
    "mo_item['mo_item_desc'] = mo_item['mo_item_desc'].map(lambda x: \"Discharged\" if x == \"Discharged34\" else x)\n",
    "mo_item.to_sql('mo_item', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Adding foreign key constraints\n",
    "We can't add some of the foreign key constraints until all the data is in there, so we'll do that down here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine.execute(\"\"\"\n",
    "ALTER TABLE incident\n",
    "ADD CONSTRAINT incident_ucr_short_desc_fkey FOREIGN KEY (ucr_short_desc) REFERENCES ucr_desc (ucr_short_desc);\n",
    "\"\"\")\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "ALTER TABLE modus_operandi\n",
    "ADD CONSTRAINT mo_mo_item_code_fkey\n",
    "FOREIGN KEY (mo_item_code, mo_group_code) REFERENCES mo_item (mo_item_code, mo_group_code);\n",
    "\"\"\")\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "ALTER TABLE weapon\n",
    "ADD CONSTRAINT weapon_weapon_desc_fk FOREIGN KEY (weapon_desc) REFERENCES weapon_group (weapon_desc);\n",
    "\"\"\")\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "ALTER TABLE premise\n",
    "ADD CONSTRAINT premise_premise_desc_fk FOREIGN KEY (premise_desc) REFERENCES premise_group (premise_desc);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
