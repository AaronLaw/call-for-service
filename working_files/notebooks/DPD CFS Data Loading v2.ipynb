{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Loading, Cleaning, and Normalization\n",
    "Now that we have a better idea of what the data contains, we're going to load it in a format that will be more efficient for analysis.  Changes to make:\n",
    "\n",
    " - account for 4 digit years in the note-splitting regex\n",
    " - account for null author in the note-splitting regex (ex. see caller at hq at the desk  [08/01/14 15:58:37 WEAVERM]  [EPD] Aborted by Law Priority with code: 1. Caller Hung Up  [08/01/14 16:11:23 ] ) and null text (ex.  old oxford  dearborn and roxboro needs salt and sand trucks  [03/03/14 22:33:07 HOLLANDJ]  [03/03/14 22:34:30 HOLLANDJ])\n",
    " - plural table names for ORM\n",
    " - turn all blanks into null\n",
    " \n",
    "##call\n",
    " - split call.call_source into new table\n",
    " - split call.primary_unit, call.reporting_unit into new table (get list of all units from call_log + '2091')\n",
    " - filter out call_time < 2014 (that one weird row from 2007)\n",
    " - call.first_dispatched refers to new unit table\n",
    " - can create list of streets from call; split call.street_name, call.crossroad1, call.crossroad2, incident.street_name into new table (get list of all streets from call.street_name)\n",
    " - split city_desc into new table\n",
    " - drop service, agency\n",
    " - ditch nature code, split nature_desc into own table\n",
    "\n",
    "##call_log\n",
    " - get full transaction_descs, split them out?  too many contain unit name now, code<->desc is not 1:1\n",
    " - ditch unitper_id?\n",
    " \n",
    "##note\n",
    " - split author out into own table\n",
    " \n",
    "##incident\n",
    " - link city to new city_desc table\n",
    " - convert premise_code, weapon_code, bureau_code, division_code, unit_code, investigation_status_code, case_status_code, to int\n",
    " - drop lwchrgid, charge_seq\n",
    " \n",
    "We'll load each table as a two-step process.  First, we scan each table and accumulate a set for each lookup table associated.  We'll then load these lookup tables.  Second, we'll load the main table.  This should be less complicated than trying to accumulate the lookup tables during the chunked-out load of the main table.\n",
    "\n",
    "Main table: call\n",
    "Lookup tables: call_unit, city, nature\n",
    "Lookup tables loaded separately: call_source\n",
    "\n",
    "Main table: note\n",
    "Lookup tables: note_author\n",
    "\n",
    "Main table: call_log\n",
    "Lookup tables: transaction\n",
    "Lookup tables loaded separately: close_code\n",
    "\n",
    "Main table: incident\n",
    "Lookup tables: city (should already have everything from call), ucr_descr\n",
    "Lookup tables loaded separately: premise, weapon, bureau, division, unit, investigation_status, case_status\n",
    "\n",
    "Main table: modus_operandi\n",
    "Lookup tables: mo_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use dataset to stuff the data into a local instance of postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the tables before touching the data so they have all the proper constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Database DDL\n",
    "\n",
    "Code to create the database schema is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CHANGE CREDENTIALS AS APPROPRIATE\n",
    "db = dataset.connect('postgresql://jnance@localhost:5432/cfs')\n",
    "engine = create_engine('postgresql://jnance@localhost:5432/cfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_db():\n",
    "    \"\"\"\n",
    "    Remove and recreate tables to prepare for reloading the db\n",
    "    \"\"\"\n",
    "    db.query(\"DROP TABLE IF EXISTS note CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS note_author CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call_source CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call_unit CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS city CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call_log CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS transaction CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS close_code CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS ucr_descr CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS incident CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS modus_operandi CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS mo_item CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS bureau CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS case_status CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS division CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS unit CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS investigation_status CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS weapon CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS weapon_group CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS premise CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS premise_group CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS NATURE CASCADE;\")\n",
    "\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE ucr_descr\n",
    "    (\n",
    "      ucr_descr_id serial NOT NULL,\n",
    "      short_descr text,\n",
    "      long_descr text,\n",
    "      CONSTRAINT ucr_descr_pk PRIMARY KEY (ucr_descr_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE bureau\n",
    "    (\n",
    "      bureau_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT bureau_pk PRIMARY KEY (bureau_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE division\n",
    "    (\n",
    "      division_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT division_pk PRIMARY KEY (division_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE investigation_status\n",
    "    (\n",
    "      investigation_status_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT investigation_status_pk PRIMARY KEY (investigation_status_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE case_status\n",
    "    (\n",
    "      case_status_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT case_status_pk PRIMARY KEY (case_status_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE unit\n",
    "    (\n",
    "      unit_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT unit_pk PRIMARY KEY (unit_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE weapon_group\n",
    "    (\n",
    "      weapon_group_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT weapon_group_pk PRIMARY KEY (weapon_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE premise_group\n",
    "    (\n",
    "      premise_group_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT premise_group_pk PRIMARY KEY (premise_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE weapon\n",
    "    (\n",
    "      weapon_id serial NOT NULL,\n",
    "      descr text,\n",
    "      weapon_group_id int,\n",
    "      CONSTRAINT weapon_pk PRIMARY KEY (weapon_id),\n",
    "      CONSTRAINT weapon_group_weapon_fk FOREIGN KEY (weapon_group_id) REFERENCES weapon_group (weapon_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE premise\n",
    "    (\n",
    "      premise_id serial NOT NULL,\n",
    "      descr text,\n",
    "      premise_group_id int,\n",
    "      CONSTRAINT premise_pk PRIMARY KEY (premise_id),\n",
    "      CONSTRAINT premise_group_premise_fk FOREIGN KEY (premise_group_id) REFERENCES premise_group (premise_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE city\n",
    "    (\n",
    "      city_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT city_pk PRIMARY KEY (city_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE incident\n",
    "    (\n",
    "      incident_id bigint NOT NULL,\n",
    "      case_id bigint UNIQUE,\n",
    "      time_filed timestamp without time zone,\n",
    "      month_filed int,\n",
    "      week_filed int,\n",
    "      dow_filed int,\n",
    "      street_num int,\n",
    "      street_name text,\n",
    "      city_id int,\n",
    "      zip int,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      beat text,\n",
    "      district text,\n",
    "      sector text,\n",
    "      premise_id int,\n",
    "      weapon_id int,\n",
    "      domestic text,\n",
    "      juvenile text,\n",
    "      gang_related text,\n",
    "      emp_bureau_id int,\n",
    "      emp_division_id int,\n",
    "      emp_unit_id int,\n",
    "      num_officers int,\n",
    "      investigation_status_id int,\n",
    "      investigator_unit_id int,\n",
    "      case_status_id int,\n",
    "      ucr_code int,\n",
    "      ucr_descr_id int,\n",
    "      attempted_or_committed boolean,\n",
    "      \n",
    "      CONSTRAINT incident_pk PRIMARY KEY (incident_id),\n",
    "      \n",
    "      CONSTRAINT case_status_incident_fk\n",
    "        FOREIGN KEY (case_status_id) REFERENCES case_status (case_status_id),\n",
    "      CONSTRAINT bureau_incident_fk\n",
    "        FOREIGN KEY (emp_bureau_id) REFERENCES bureau (bureau_id),\n",
    "      CONSTRAINT division_incident_fk\n",
    "        FOREIGN KEY (emp_division_id) REFERENCES division (division_id),\n",
    "      CONSTRAINT unit_incident_emp_fk\n",
    "        FOREIGN KEY (emp_unit_id) REFERENCES unit (unit_id),\n",
    "      CONSTRAINT unit_incident_investigator_fk\n",
    "        FOREIGN KEY (investigator_unit_id) REFERENCES unit (unit_id),\n",
    "      CONSTRAINT investigation_status_incident_fk\n",
    "        FOREIGN KEY (investigation_status_id) REFERENCES investigation_status (investigation_status_id),\n",
    "      CONSTRAINT premise_incident_fk\n",
    "        FOREIGN KEY (premise_id) REFERENCES premise (premise_id),\n",
    "      CONSTRAINT weapon_incident_fk\n",
    "        FOREIGN KEY (weapon_id) REFERENCES weapon (weapon_id),\n",
    "      CONSTRAINT city_incident_fk\n",
    "        FOREIGN KEY (city_id) REFERENCES city (city_id),\n",
    "      CONSTRAINT ucr_descr_incident_fk\n",
    "        FOREIGN KEY (ucr_descr_id) REFERENCES ucr_descr (ucr_descr_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE mo_item\n",
    "    (\n",
    "      mo_item_id int NOT NULL,\n",
    "      item_descr text,\n",
    "      mo_group_id int NOT NULL,\n",
    "      group_descr text,\n",
    "      CONSTRAINT mo_item_pk PRIMARY KEY (mo_item_id, mo_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE modus_operandi\n",
    "    (\n",
    "      incident_id bigint,\n",
    "      mo_id bigint,\n",
    "      mo_group_id int,\n",
    "      mo_item_id int,\n",
    "      \n",
    "      CONSTRAINT mo_pkey PRIMARY KEY (mo_id),\n",
    "      \n",
    "      CONSTRAINT incident_modus_operandi_fk FOREIGN KEY (incident_id) REFERENCES incident (incident_id),\n",
    "      CONSTRAINT mo_item_modus_operandi_fk FOREIGN KEY (mo_item_id, mo_group_id) \n",
    "        REFERENCES mo_item (mo_item_id, mo_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call_source\n",
    "    (\n",
    "      call_source_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT call_source_pk PRIMARY KEY (call_source_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call_unit\n",
    "    (\n",
    "      call_unit_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT call_unit_pk PRIMARY KEY (call_unit_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE close_code\n",
    "    (\n",
    "      close_code_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT close_code_pk PRIMARY KEY (close_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE nature\n",
    "    (\n",
    "      nature_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT nature_pk PRIMARY KEY (nature_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call\n",
    "    (\n",
    "      call_id bigint NOT NULL,\n",
    "      month int,\n",
    "      week int,\n",
    "      day_of_week int,\n",
    "      hour int,\n",
    "      case_id bigint,\n",
    "      call_source_id int,\n",
    "      primary_unit_id int,\n",
    "      first_dispatched_id int,\n",
    "      reporting_unit_id int,\n",
    "      street_num int,\n",
    "      street_name text,\n",
    "      city_id int,\n",
    "      zip int,\n",
    "      crossroad1 text,\n",
    "      crossroad2 text,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      beat text,\n",
    "      district text,\n",
    "      sector text,\n",
    "      business text,\n",
    "      nature_id int,\n",
    "      priority text,\n",
    "      report_only boolean,\n",
    "      cancelled boolean,\n",
    "      time_received timestamp without time zone,\n",
    "      time_routed timestamp without time zone,\n",
    "      time_finished timestamp without time zone,\n",
    "      first_unit_dispatch timestamp without time zone,\n",
    "      first_unit_enroute timestamp without time zone,\n",
    "      first_unit_arrive timestamp without time zone,\n",
    "      first_unit_transport timestamp without time zone,\n",
    "      last_unit_clear timestamp without time zone,\n",
    "      time_closed timestamp without time zone,\n",
    "      close_code_id int,\n",
    "      close_comm text,\n",
    "      \n",
    "      CONSTRAINT call_pk PRIMARY KEY (call_id),\n",
    "      \n",
    "      CONSTRAINT call_source_call_fk\n",
    "        FOREIGN KEY (call_source_id) REFERENCES call_source (call_source_id),\n",
    "      CONSTRAINT call_unit_call_primary_unit_fk\n",
    "        FOREIGN KEY (primary_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT call_unit_call_first_dispatched_fk\n",
    "        FOREIGN KEY (first_dispatched_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT call_unit_call_reporting_unit_fk\n",
    "        FOREIGN KEY (reporting_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT city_call_fk\n",
    "        FOREIGN KEY (city_id) REFERENCES city (city_id),\n",
    "      CONSTRAINT close_code_call_fk\n",
    "        FOREIGN KEY (close_code_id) REFERENCES close_code (close_code_id),\n",
    "      CONSTRAINT incident_call_fk\n",
    "        FOREIGN KEY (case_id) REFERENCES incident (case_id),\n",
    "      CONSTRAINT nature_call_fk\n",
    "        FOREIGN KEY (nature_id) REFERENCES nature (nature_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE note_author\n",
    "    (\n",
    "      note_author_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT note_author_pk PRIMARY KEY (note_author_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE note\n",
    "    (\n",
    "      note_id serial NOT NULL,\n",
    "      body text,\n",
    "      time_recorded timestamp without time zone,\n",
    "      note_author_id int,\n",
    "      call_id bigint,\n",
    "      CONSTRAINT note_pk PRIMARY KEY (note_id),\n",
    "      \n",
    "      CONSTRAINT call_note_fk FOREIGN KEY (call_id) REFERENCES call (call_id),\n",
    "      CONSTRAINT note_author_note_fk FOREIGN KEY (note_author_id) REFERENCES note_author (note_author_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE transaction\n",
    "    (\n",
    "      transaction_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT transaction_pk PRIMARY KEY (transaction_id)\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call_log\n",
    "    (\n",
    "      call_log_id bigint NOT NULL,\n",
    "      transaction_id int,\n",
    "      time_recorded timestamp without time zone,\n",
    "      call_id bigint,\n",
    "      call_unit_id int,\n",
    "      close_code_id int,\n",
    "      \n",
    "      CONSTRAINT call_log_pk PRIMARY KEY (call_log_id),\n",
    "      \n",
    "      CONSTRAINT call_unit_call_log_fk FOREIGN KEY (call_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT call_call_log_fk FOREIGN KEY (call_id) REFERENCES call (call_id),\n",
    "      CONSTRAINT close_code_call_log_fk FOREIGN KEY (close_code_id) REFERENCES close_code (close_code_id),\n",
    "      CONSTRAINT transaction_call_log_fk FOREIGN KEY (transaction_id) REFERENCES transaction (transaction_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "    \n",
    "reset_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Small lookup tables v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading LWMAIN.CSSTATUS.csv into case_status\n",
      "loading LWMAIN.EMDIVISION.csv into division\n",
      "loading LWMAIN.EMSECTION.csv into unit\n",
      "loading LWMAIN.EMUNIT.csv into bureau\n",
      "loading LWMAIN.INVSTSTATS.csv into investigation_status\n",
      "loading inmain.callsource.tsv into call_source\n",
      "loading inmain.closecode.tsv into close_code\n"
     ]
    }
   ],
   "source": [
    "# There are a million of these, so let's make life easier and reuse all that code\n",
    "\n",
    "# We need to save the mapping between DPD's short codes and our database ids so we can apply it to the records\n",
    "# in the main tables\n",
    "#\n",
    "# These have the DPD's codes as keys and our internal database PKs as values\n",
    "case_status_code_mapping = {}\n",
    "division_code_mapping = {}\n",
    "unit_code_mapping = {}\n",
    "bureau_code_mapping = {}\n",
    "investigation_status_code_mapping = {}\n",
    "call_source_code_mapping = {}\n",
    "close_code_code_mapping = {}\n",
    "\n",
    "lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.CSSTATUS.csv\",\n",
    "        \"table\": \"case_status\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_mapping\": case_status_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMDIVISION.csv\",\n",
    "        \"table\": \"division\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_mapping\": division_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMSECTION.csv\",\n",
    "        \"table\": \"unit\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_mapping\": unit_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMUNIT.csv\",\n",
    "        \"table\": \"bureau\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_mapping\": bureau_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.INVSTSTATS.csv\",\n",
    "        \"table\": \"investigation_status\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_mapping\": investigation_status_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"inmain.callsource.tsv\",\n",
    "        \"table\": \"call_source\",\n",
    "        \"mapping\": {\"Description\": \"descr\"},\n",
    "        \"code_mapping\": call_source_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"inmain.closecode.tsv\",\n",
    "        \"table\": \"close_code\",\n",
    "        \"mapping\": {\"Description\": \"descr\"},\n",
    "        \"code_mapping\": close_code_code_mapping\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in lookup_jobs:\n",
    "    print(\"loading %s into %s\" % (job['file'], job['table']))\n",
    "    \n",
    "    if job['file'].endswith(\".csv\"):\n",
    "        data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    elif job['file'].endswith(\".tsv\"):\n",
    "        data = pd.read_csv(\"../csv_data/%s\" % (job['file']), sep='\\t')\n",
    "    \n",
    "    # Keep track of the ids, as the data is ordered, so these will be the same assigned by the incrementing\n",
    "    # primary key in the database.\n",
    "    id_ = 1    \n",
    "    for (i,row) in data.iterrows():\n",
    "        job['code_mapping'][row['code_agcy']] = id_\n",
    "        id_ += 1\n",
    "\n",
    "    # Keep only the desired columns\n",
    "    keep_columns = set(job['mapping'].keys())\n",
    "    for c in data.columns:\n",
    "        if c not in keep_columns:\n",
    "            data = data.drop(c, axis=1)\n",
    "            \n",
    "    # Change the column names to the ones we want and insert the data\n",
    "    data.rename(columns=job['mapping'], inplace=True)\n",
    "    data.to_sql(job['table'], engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading LWMAIN.PREMISE.csv into premise and premise_group\n",
      "loading LWMAIN.WEAPON.csv into weapon and weapon_group\n"
     ]
    }
   ],
   "source": [
    "#These have to create \"nested\" tables and are a little tougher, but we can still reuse the code\n",
    "\n",
    "# Still need to keep track of the mappings\n",
    "weapon_code_mapping = {}\n",
    "premise_code_mapping = {}\n",
    "\n",
    "nested_lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.PREMISE.csv\",\n",
    "        \"outer_table\": \"premise\",\n",
    "        \"inner_table\": \"premise_group\",\n",
    "        \"outer_cols\": [\"premise_group_id\",\"descr\"],\n",
    "        \"inner_col\": \"descr\",\n",
    "        \"inner_id\": \"premise_group_id\",\n",
    "        \"code_mapping\": premise_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.WEAPON.csv\",\n",
    "        \"outer_table\": \"weapon\",\n",
    "        \"inner_table\": \"weapon_group\",\n",
    "        \"outer_cols\": [\"weapon_group_id\",\"descr\"],\n",
    "        \"inner_col\": \"descr\",\n",
    "        \"inner_id\": \"weapon_group_id\",\n",
    "        \"code_mapping\": weapon_code_mapping\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in nested_lookup_jobs:\n",
    "    print(\"loading %s into %s and %s\" % (job['file'], job['outer_table'], job['inner_table']))\n",
    "    data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    \n",
    "    # load the group table by getting all the unique groups\n",
    "    inner_data = data['descriptn_a'].drop_duplicates()\n",
    "    inner_data.name = job['inner_col']\n",
    "    inner_data.to_sql(job['inner_table'], engine, index=False, if_exists='append')\n",
    "    \n",
    "    # Learn the mapping between groups and group_ids in the database so we can insert the proper\n",
    "    # group_ids with the outer tables\n",
    "    groups = {}\n",
    "    for row in db.query(\"SELECT * FROM %s\" % (job['inner_table'])):\n",
    "        groups[row[job['inner_col']]] = row[job['inner_id']]\n",
    "       \n",
    "    # Figure out what the database ids will be, so we can convert DPD's columns to the database ids in the\n",
    "    # main table load\n",
    "    id_ = 1\n",
    "    for (i,row) in data.iterrows():\n",
    "        job['code_mapping'][row['code_agcy']] = id_\n",
    "        id_ += 1\n",
    "    \n",
    "    # Concatenate and rename the series we want\n",
    "    outer_data = pd.concat([data['descriptn_a'], data['descriptn_b']], axis=1, keys=job['outer_cols'])\n",
    "    \n",
    "    # use the groups mapping to turn group names into ids from our database\n",
    "    outer_data[job['inner_id']] = outer_data[job['inner_id']].map(lambda x: groups[x])\n",
    "    \n",
    "    # Store the records\n",
    "    outer_data.to_sql(job['outer_table'], engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "chunksize = 20000\n",
    "\n",
    "ucr_descr_code_mapping = {}\n",
    "\n",
    "city_code_mapping = {}\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "city = pd.DataFrame()\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "# We'll start out by doing a pass through the file and loading the lookup tables we need (ucr_descr, city)\n",
    "for incident in pd.read_csv('../csv_data/cfs_2014_lwmain.csv', chunksize=chunksize, \n",
    "                       iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "    \n",
    "    #Strip extraneous white space and turn resulting blanks into NULLs\n",
    "    incident = incident.applymap(safe_strip).applymap(lambda x: None if x == '' or pd.isnull(x) else x)\n",
    "    \n",
    "    # Turn the ucr_descrs into pairs, since it's the pairs that are unique\n",
    "    ucr_descr_pairs = pd.concat([incident['arr_chrg'], incident['chrgdesc']], axis=1)\n",
    "    ucr_descr_pairs = ucr_descr_pairs.drop_duplicates()\n",
    "    \n",
    "    # Add the cities in the current chunk to the dataframe\n",
    "    city = pd.concat([city, incident['city']], axis=0)\n",
    "    city = city.drop_duplicates()\n",
    "\n",
    "# switch to our column names\n",
    "city.rename(columns={0:'descr'}, inplace=True)\n",
    "ucr_descr_pairs.rename(columns={'arr_chrg': 'short_descr', 'chrgdesc': 'long_descr'}, inplace=True)\n",
    "\n",
    "# we don't need nulls in a lookup table\n",
    "city = city[~city.descr.isnull()]\n",
    "\n",
    "#store the records\n",
    "city.to_sql('city', engine, index=False, if_exists='append')\n",
    "ucr_descr_pairs.to_sql('ucr_descr', engine, index=False, if_exists='append')\n",
    "\n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#OLD CODE TO BE REWRITTEN BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##cfs_2014_inmain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 seconds: completed 20000 rows\n",
      "80 seconds: completed 40000 rows\n",
      "150 seconds: completed 60000 rows\n",
      "220 seconds: completed 80000 rows\n",
      "293 seconds: completed 100000 rows\n",
      "372 seconds: completed 120000 rows\n",
      "449 seconds: completed 140000 rows\n",
      "526 seconds: completed 160000 rows\n",
      "606 seconds: completed 180000 rows\n",
      "688 seconds: completed 200000 rows\n",
      "769 seconds: completed 220000 rows\n",
      "849 seconds: completed 240000 rows\n",
      "929 seconds: completed 260000 rows\n",
      "1012 seconds: completed 280000 rows\n",
      "1093 seconds: completed 300000 rows\n",
      "1179 seconds: completed 320000 rows\n",
      "1261 seconds: completed 340000 rows\n",
      "1345 seconds: completed 360000 rows\n",
      "1427 seconds: completed 380000 rows\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "timestamp_expr = re.compile(\"\\[(\\d{2}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}) (.+?)\\]\")\n",
    "\n",
    "def split_notes_dict(notes,call_id):\n",
    "    \"\"\"\n",
    "    Return a list of dicts.  Each dict represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    dicts = []\n",
    "    regex_split = timestamp_expr.split(notes)[:-1]  # get rid of the last empty string created by the split\n",
    "    for i in range(0,len(regex_split),3):\n",
    "        text = regex_split[i].strip()\n",
    "        timestamp = dt.datetime.strptime(regex_split[i+1], \"%m/%d/%y %H:%M:%S\")\n",
    "        author = regex_split[i+2]\n",
    "        dicts.append({\"text\": text, \"timestamp\": timestamp, \"author\": author, \"call_id\": call_id})\n",
    "    return dicts\n",
    "\n",
    "def split_notes(notes):\n",
    "    \"\"\"\n",
    "    Return a list of tuples.  Each tuple represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    notes = str(notes)\n",
    "    tuples = []\n",
    "    regex_split = timestamp_expr.split(notes)[:-1]  # get rid of the last empty string created by the split\n",
    "    for i in range(0,len(regex_split),3):\n",
    "        text = regex_split[i].strip()\n",
    "        timestamp = dt.datetime.strptime(regex_split[i+1], \"%m/%d/%y %H:%M:%S\")\n",
    "        author = regex_split[i+2]\n",
    "        tuples.append((text, timestamp, author))\n",
    "    return tuples\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "def clean_caseid(c):\n",
    "    c = str(c).replace('nan','').replace('-','').replace(' ','')\n",
    "    return None if c == '' else int(c)\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "chunksize = 20000\n",
    "j = 0\n",
    "\n",
    "# We need to map the inmain columns to the renamed columns in the call table\n",
    "# if an inmain column isn't in this dict, it means we need to drop it\n",
    "call_mappings = {\n",
    "    \"inci_id\": \"call_id\",\n",
    "    \"calltime\": \"call_time\",\n",
    "    \"calldow\": \"call_dow\",\n",
    "    \"case_id\": \"case_id\",\n",
    "    \"callsource\": \"call_source\",\n",
    "    \"primeunit\": \"primary_unit\",\n",
    "    \"firstdisp\": \"first_dispatched\",\n",
    "    \"streetno\": \"street_num\",\n",
    "    \"streetonly\": \"street_name\",\n",
    "    \"citydesc\": \"city_desc\",\n",
    "    \"zip\": \"zip\",\n",
    "    \"crossroad1\": \"crossroad1\",\n",
    "    \"crossroad2\": \"crossroad2\",\n",
    "    \"geox\": \"geox\",\n",
    "    \"geoy\": \"geoy\",\n",
    "    \"service\": \"service\",\n",
    "    \"agency\": \"agency\",\n",
    "    \"statbeat\": \"beat\",\n",
    "    \"district\": \"district\",\n",
    "    \"ra\": \"sector\",\n",
    "    \"business\": \"business\",\n",
    "    \"naturecode\": \"nature_code\",\n",
    "    \"nature\": \"nature_desc\",\n",
    "    \"priority\": \"priority\",\n",
    "    \"rptonly\": \"report_only\",\n",
    "    \"cancelled\": \"cancelled\",\n",
    "    \"timeroute\": \"time_enroute\",\n",
    "    \"timefini\": \"time_finished\",\n",
    "    \"firstdtm\": \"first_unit_dispatch\",\n",
    "    \"firstenr\": \"first_unit_enroute\",\n",
    "    \"firstarrv\": \"first_unit_arrive\",\n",
    "    \"firsttran\": \"first_unit_transport\",\n",
    "    \"lastclr\": \"last_unit_clear\",\n",
    "    \"timeclose\": \"time_closed\",\n",
    "    \"reptaken\": \"reporting_unit\",\n",
    "    \"closecode\": \"close_code\",\n",
    "    \"closecomm\": \"close_comm\"\n",
    "}\n",
    "\n",
    "keep_columns = set(call_mappings.keys())\n",
    "\n",
    "for call in pd.read_csv('../csv_data/cfs_2014_inmain.csv', chunksize=chunksize, iterator=True, encoding='ISO-8859-1',\n",
    "                       low_memory=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    nice, clean iterative algorithm for separating out the notes data -- unfortunately, it's prohibitively slow\n",
    "    (~3 mins per 25k record or thereabouts)\n",
    "    \"\"\"\n",
    "    #for index, row in call.iterrows():\n",
    "    #    note = note.append(pd.DataFrame(split_notes_dict(str(row['notes']), row['inci_id'])))\n",
    "        #if call.iloc[i]['naturecode'] not in nature_set:\n",
    "        #    nature_set.add(call.iloc[i]['naturecode'])\n",
    "        #    nature = nature.append(pd.DataFrame({\"nature_code\": [call.iloc[i]['naturecode']],\n",
    "        #                                \"nature_desc\": [call.iloc[i]['nature']]}))\n",
    "   \n",
    "    \"\"\"\n",
    "    Horrid ugly algorithm for separating out the notes data -- it's faster by about 10x though\n",
    "    Pandas is really slow when iterating on rows, so we have to do all the transformations to a whole series/list\n",
    "    at a time\n",
    "    \"\"\"\n",
    "    # Create a new series, which is (for each call) a list of tuples containing the text, author, and timestamp\n",
    "    # of that call:\n",
    "    # ex. Series([\"one long string with text, author, timestamp for all remarks\"]) -> \n",
    "    #     Series([(text, author, timestamp), (text2, author2, timestamp2)])\n",
    "    call['collected_notes'] = call['notes'].apply(split_notes)\n",
    "    \n",
    "    # Combine the previous series with the inci_id of each row, preserving the relationship between inci_id\n",
    "    # and each individual remark, then convert it to a list so we can reduce and map\n",
    "    # ex. Series([(text, author, timestamp), (text2, author2, timestamp2)]) ->\n",
    "    #     [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)]\n",
    "    combined_notes = call['collected_notes'].combine(call['inci_id'],\n",
    "                                                          lambda x,y: [(e,y) for e in x]).tolist()\n",
    "    \n",
    "    # Reduce the list of lists using extend; instead of a list of lists of tuples, we have one long list of\n",
    "    # nested tuples\n",
    "    # ex. [[((text, author, timestamp), inci_id)], [((text2, author2, timestamp2), inci_id2)]] ->\n",
    "    #     [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)]\n",
    "    extended_notes = []\n",
    "    for l in combined_notes:\n",
    "        extended_notes.extend(l)\n",
    "    \n",
    "    # Flatten the tuples, so we have a list of non-nested tuples\n",
    "    # ex. [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)] ->\n",
    "    #     [(text, author, timestamp, inci_id), (text2, author2, timestamp2, inci_id2)]\n",
    "    extended_notes = map(lambda x: (x[0][0],x[0][1],x[0][2],x[1]), extended_notes)\n",
    "    \n",
    "    # Create a dataframe from the list of tuples (whew)\n",
    "    note = pd.DataFrame.from_records(extended_notes, columns=['text','timestamp','author','call_id'])\n",
    "    \n",
    "    # drop unnecessary columns\n",
    "    for c in call.columns:\n",
    "        if c not in keep_columns:\n",
    "            call = call.drop(c, axis=1)   \n",
    "    \n",
    "    # rename to the CFS Analytics column names\n",
    "    call.rename(columns=call_mappings, inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    # get rid of some weird records that break the case_id cleanup\n",
    "    call = call[~(call.call_id.isin((2014055521,2014269353)))]\n",
    "    note = note[~(note.call_id.isin((2014055521,2014269353)))]\n",
    "    \n",
    "    # clean up the case_id column\n",
    "    call['case_id'] = call['case_id'].map(clean_caseid)\n",
    "    \n",
    "    # Perform datetime conversions\n",
    "    call['call_time'] = pd.to_datetime(call['call_time'])\n",
    "    call['time_enroute'] = pd.to_datetime(call['time_enroute'])\n",
    "    call['time_finished'] = pd.to_datetime(call['time_finished'])\n",
    "    call['first_unit_dispatch'] = pd.to_datetime(call['first_unit_dispatch'])\n",
    "    call['first_unit_enroute'] = pd.to_datetime(call['first_unit_enroute'])\n",
    "    call['first_unit_arrive'] = pd.to_datetime(call['first_unit_arrive'])\n",
    "    call['first_unit_transport'] = pd.to_datetime(call['first_unit_transport'])\n",
    "    call['last_unit_clear'] = pd.to_datetime(call['last_unit_clear'])\n",
    "    call['time_closed'] = pd.to_datetime(call['time_closed'])\n",
    "\n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "    \n",
    "    # get rid of excess whitespace\n",
    "    call = call.applymap(safe_strip)\n",
    "    note = note.applymap(safe_strip)\n",
    "    \n",
    "    # store in the database\n",
    "    call.to_sql('call', engine, index=False, if_exists='append')\n",
    "    note.to_sql('note', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_xxx2014_incilog.csv\n",
    "There is one of these for each month, so we have to load them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting load for month: jan\n",
      "1 seconds: completed 20000 rows\n",
      "17 seconds: completed 40000 rows\n",
      "32 seconds: completed 60000 rows\n",
      "48 seconds: completed 80000 rows\n",
      "64 seconds: completed 100000 rows\n",
      "81 seconds: completed 120000 rows\n",
      "97 seconds: completed 140000 rows\n",
      "113 seconds: completed 160000 rows\n",
      "129 seconds: completed 180000 rows\n",
      "145 seconds: completed 200000 rows\n",
      "161 seconds: completed 220000 rows\n",
      "Starting load for month: feb\n",
      "1 seconds: completed 20000 rows\n",
      "18 seconds: completed 40000 rows\n",
      "34 seconds: completed 60000 rows\n",
      "50 seconds: completed 80000 rows\n",
      "65 seconds: completed 100000 rows\n",
      "81 seconds: completed 120000 rows\n",
      "97 seconds: completed 140000 rows\n",
      "113 seconds: completed 160000 rows\n",
      "130 seconds: completed 180000 rows\n",
      "145 seconds: completed 200000 rows\n",
      "Starting load for month: mar\n",
      "1 seconds: completed 20000 rows\n",
      "17 seconds: completed 40000 rows\n",
      "33 seconds: completed 60000 rows\n",
      "50 seconds: completed 80000 rows\n",
      "66 seconds: completed 100000 rows\n",
      "82 seconds: completed 120000 rows\n",
      "98 seconds: completed 140000 rows\n",
      "115 seconds: completed 160000 rows\n",
      "131 seconds: completed 180000 rows\n",
      "148 seconds: completed 200000 rows\n",
      "164 seconds: completed 220000 rows\n",
      "Starting load for month: apr\n",
      "1 seconds: completed 20000 rows\n",
      "16 seconds: completed 40000 rows\n",
      "33 seconds: completed 60000 rows\n",
      "49 seconds: completed 80000 rows\n",
      "66 seconds: completed 100000 rows\n",
      "83 seconds: completed 120000 rows\n",
      "99 seconds: completed 140000 rows\n",
      "116 seconds: completed 160000 rows\n",
      "132 seconds: completed 180000 rows\n",
      "148 seconds: completed 200000 rows\n",
      "165 seconds: completed 220000 rows\n",
      "Starting load for month: may\n",
      "1 seconds: completed 20000 rows\n",
      "17 seconds: completed 40000 rows\n",
      "33 seconds: completed 60000 rows\n",
      "50 seconds: completed 80000 rows\n",
      "66 seconds: completed 100000 rows\n",
      "82 seconds: completed 120000 rows\n",
      "98 seconds: completed 140000 rows\n",
      "114 seconds: completed 160000 rows\n",
      "131 seconds: completed 180000 rows\n",
      "147 seconds: completed 200000 rows\n",
      "163 seconds: completed 220000 rows\n",
      "Starting load for month: jun\n",
      "1 seconds: completed 20000 rows\n",
      "17 seconds: completed 40000 rows\n",
      "33 seconds: completed 60000 rows\n",
      "49 seconds: completed 80000 rows\n",
      "65 seconds: completed 100000 rows\n",
      "82 seconds: completed 120000 rows\n",
      "98 seconds: completed 140000 rows\n",
      "114 seconds: completed 160000 rows\n",
      "130 seconds: completed 180000 rows\n",
      "146 seconds: completed 200000 rows\n",
      "Starting load for month: jul\n",
      "1 seconds: completed 20000 rows\n",
      "17 seconds: completed 40000 rows\n",
      "33 seconds: completed 60000 rows\n",
      "50 seconds: completed 80000 rows\n",
      "66 seconds: completed 100000 rows\n",
      "83 seconds: completed 120000 rows\n",
      "100 seconds: completed 140000 rows\n",
      "117 seconds: completed 160000 rows\n",
      "133 seconds: completed 180000 rows\n",
      "Starting load for month: aug\n",
      "1 seconds: completed 20000 rows\n",
      "17 seconds: completed 40000 rows\n",
      "34 seconds: completed 60000 rows\n",
      "51 seconds: completed 80000 rows\n",
      "67 seconds: completed 100000 rows\n",
      "84 seconds: completed 120000 rows\n",
      "101 seconds: completed 140000 rows\n",
      "118 seconds: completed 160000 rows\n",
      "133 seconds: completed 180000 rows\n",
      "149 seconds: completed 200000 rows\n",
      "Starting load for month: sep\n",
      "1 seconds: completed 20000 rows\n",
      "16 seconds: completed 40000 rows\n",
      "32 seconds: completed 60000 rows\n",
      "48 seconds: completed 80000 rows\n",
      "64 seconds: completed 100000 rows\n",
      "81 seconds: completed 120000 rows\n",
      "97 seconds: completed 140000 rows\n",
      "113 seconds: completed 160000 rows\n",
      "129 seconds: completed 180000 rows\n",
      "Starting load for month: oct\n",
      "1 seconds: completed 20000 rows\n",
      "17 seconds: completed 40000 rows\n",
      "33 seconds: completed 60000 rows\n",
      "50 seconds: completed 80000 rows\n",
      "66 seconds: completed 100000 rows\n",
      "83 seconds: completed 120000 rows\n",
      "98 seconds: completed 140000 rows\n",
      "114 seconds: completed 160000 rows\n",
      "131 seconds: completed 180000 rows\n",
      "147 seconds: completed 200000 rows\n",
      "Starting load for month: nov\n",
      "1 seconds: completed 20000 rows\n",
      "17 seconds: completed 40000 rows\n",
      "34 seconds: completed 60000 rows\n",
      "50 seconds: completed 80000 rows\n",
      "66 seconds: completed 100000 rows\n",
      "83 seconds: completed 120000 rows\n",
      "100 seconds: completed 140000 rows\n",
      "116 seconds: completed 160000 rows\n",
      "132 seconds: completed 180000 rows\n",
      "Starting load for month: dec\n",
      "1 seconds: completed 20000 rows\n",
      "17 seconds: completed 40000 rows\n",
      "34 seconds: completed 60000 rows\n",
      "50 seconds: completed 80000 rows\n",
      "67 seconds: completed 100000 rows\n",
      "83 seconds: completed 120000 rows\n",
      "100 seconds: completed 140000 rows\n",
      "116 seconds: completed 160000 rows\n",
      "132 seconds: completed 180000 rows\n"
     ]
    }
   ],
   "source": [
    "months = (\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\")\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "for month in months:\n",
    "    start = dt.datetime.now()\n",
    "    print(\"Starting load for month: %s\" % (month))\n",
    "    # load the data in chunks so we don't use too much memory\n",
    "    chunksize = 20000\n",
    "    j = 0\n",
    "\n",
    "    # We need to map the incilog columns to the renamed columns in the call_log table\n",
    "    # if an incilog column isn't in this dict, it means we need to drop it\n",
    "    call_log_mappings = {\n",
    "        \"incilogid\": \"call_log_id\",\n",
    "        \"transtype\": \"transaction_code\",\n",
    "        \"descript\": \"transaction_desc\",\n",
    "        \"timestamp\": \"timestamp\",\n",
    "        \"inci_id\": \"call_id\",\n",
    "        \"unitcode\": \"unit_code\",\n",
    "        \"radorev\": \"radio_or_event\",\n",
    "        \"unitperid\": \"unitper_id\",\n",
    "        \"closecode\": \"close_code\"\n",
    "    }\n",
    "    \n",
    "    keep_columns = set(call_log_mappings.keys())\n",
    "\n",
    "    for call_log in pd.read_csv('../csv_data/cfs_%s2014_incilog.csv' % (month), chunksize=chunksize, \n",
    "                           iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "        for c in call_log.columns:\n",
    "            if c not in keep_columns:\n",
    "                call_log = call_log.drop(c, axis=1)\n",
    "\n",
    "        # rename to the CFS Analytics column names\n",
    "        call_log.rename(columns=call_log_mappings, inplace=True)\n",
    "\n",
    "        ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "        ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "            \n",
    "        # Perform datetime conversions\n",
    "        call_log['timestamp'] = pd.to_datetime(call_log['timestamp'])\n",
    "        \n",
    "        # progress update\n",
    "        j+=1\n",
    "        print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "        # strip excess whitespace\n",
    "        call_log = call_log.applymap(safe_strip)\n",
    "        \n",
    "        # store in the database\n",
    "        call_log.to_sql('call_log', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 seconds: completed 20000 rows\n",
      "21 seconds: completed 40000 rows\n"
     ]
    }
   ],
   "source": [
    "def combine_date_time(str_date, str_time):\n",
    "    date = dt.datetime.strptime(str_date, \"%m/%d/%y\")\n",
    "    time = dt.datetime.strptime(str_time, \"%I:%M %p\")\n",
    "    return dt.datetime(date.year, date.month, date.day, time.hour, time.minute)\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "chunksize = 20000\n",
    "j = 0\n",
    "\n",
    "# We need to map the incilog columns to the renamed columns in the call_log table\n",
    "# if an incilog column isn't in this dict, it means we need to drop it\n",
    "incident_mappings = {\n",
    "    \"lwmainid\": \"incident_id\",\n",
    "    \"inci_id\": \"case_id\",\n",
    "    \"time\": \"time_filed\",\n",
    "    \"streetnbr\": \"street_num\",\n",
    "    \"street\": \"street_name\",\n",
    "    \"city\": \"city\",\n",
    "    \"zip\": \"zip\",\n",
    "    \"geox\": \"geox\",\n",
    "    \"geoy\": \"geoy\",\n",
    "    \"tract\": \"beat\",\n",
    "    \"district\": \"district\",\n",
    "    \"reportarea\": \"sector\",\n",
    "    \"premise\": \"premise_code\",\n",
    "    \"weapon\": \"weapon_code\",\n",
    "    \"domestic\": \"domestic\",\n",
    "    \"juvenile\": \"juvenile\",\n",
    "    \"gangrelat\": \"gang_related\",\n",
    "    \"emunit\": \"emp_bureau_code\",\n",
    "    \"emdivision\": \"emp_division_code\",\n",
    "    \"emsection\": \"emp_unit_code\",\n",
    "    \"asst_offcr\": \"num_officers\",\n",
    "    \"invststats\": \"investigation_status_code\",\n",
    "    \"investunit\": \"investigator_unit_code\",\n",
    "    \"csstatus\": \"case_status_code\",\n",
    "    \"lwchrgid\": \"lwchrgid\",\n",
    "    \"chrgcnt\": \"charge_seq\",\n",
    "    \"ucr_code\": \"ucr_code\",\n",
    "    \"arr_chrg\": \"ucr_short_desc\",\n",
    "    \"attm_comp\": \"attempted_or_committed\"\n",
    "}\n",
    "\n",
    "keep_columns = set(incident_mappings.keys())\n",
    "\n",
    "ucr_desc = pd.DataFrame({\"ucr_short_desc\": [], \"ucr_long_desc\": []})\n",
    "\n",
    "for incident in pd.read_csv('../csv_data/cfs_2014_lwmain.csv', chunksize=chunksize, \n",
    "                       iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "    \n",
    "    ucr_desc = ucr_desc.append(pd.concat([ incident['arr_chrg'],\n",
    "                                           incident['chrgdesc'] ],\n",
    "                                        axis=1, keys=['ucr_short_desc', 'ucr_long_desc']))\n",
    "    \n",
    "    # Perform datetime conversions\n",
    "    incident['time'] = incident['date_rept'].combine(incident['time'], combine_date_time)\n",
    "    \n",
    "    for c in incident.columns:\n",
    "        if c not in keep_columns:\n",
    "            incident = incident.drop(c, axis=1)\n",
    "\n",
    "    # rename to the CFS Analytics column names\n",
    "    incident.rename(columns=incident_mappings, inplace=True)\n",
    "\n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    # strip whitespace\n",
    "    incident = incident.applymap(safe_strip)\n",
    "    ucr_desc = ucr_desc.applymap(safe_strip)\n",
    "    \n",
    "    # convert empty strings in num_officers to nulls so we can insert as an int column\n",
    "    incident['num_officers'] = incident['num_officers'].map(lambda x: None if x == '' else x)\n",
    "    \n",
    "    # These \"primary key\" values have two records and I don't want to deal with it\n",
    "    incident = incident[~(incident.incident_id.isin((498659, 503578, 521324)))]\n",
    "    \n",
    "    # Drop duplicate ucr_descs\n",
    "    ucr_desc = ucr_desc.drop_duplicates()\n",
    "    \n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "    incident = incident.applymap(safe_strip)\n",
    "    \n",
    "    # store in the database\n",
    "    incident.to_sql('incident', engine, index=False, if_exists='append')\n",
    "\n",
    "ucr_desc.to_sql('ucr_desc', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmodop.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 seconds: completed 2500 rows\n",
      "1 seconds: completed 5000 rows\n",
      "3 seconds: completed 7500 rows\n",
      "4 seconds: completed 10000 rows\n",
      "6 seconds: completed 12500 rows\n",
      "8 seconds: completed 15000 rows\n",
      "9 seconds: completed 17500 rows\n",
      "11 seconds: completed 20000 rows\n",
      "13 seconds: completed 22500 rows\n",
      "14 seconds: completed 25000 rows\n",
      "16 seconds: completed 27500 rows\n",
      "17 seconds: completed 30000 rows\n",
      "19 seconds: completed 32500 rows\n",
      "21 seconds: completed 35000 rows\n",
      "22 seconds: completed 37500 rows\n",
      "24 seconds: completed 40000 rows\n",
      "26 seconds: completed 42500 rows\n"
     ]
    }
   ],
   "source": [
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "# strange unexplainable crash using the usual 20k chunk size (and 10k sometimes? and 5k sometimes? this makes no sense)\n",
    "# so go with 20k (no) 10k (no) 5k (no)\n",
    "# actually just put your favorite number here and hope it doesn't crash\n",
    "chunksize = 2500\n",
    "j = 0\n",
    "\n",
    "# We need to map the incilog columns to the renamed columns in the call_log table\n",
    "# if an incilog column isn't in this dict, it means we need to drop it\n",
    "modop_mappings = {\n",
    "    \"lwmainid\": \"incident_id\",\n",
    "    \"lwmodopid\": \"mo_id\",\n",
    "    \"mogroup\": \"mo_group_code\",\n",
    "    \"moitem\": \"mo_item_code\"\n",
    "}\n",
    "\n",
    "keep_columns = set(modop_mappings.keys())\n",
    "\n",
    "mo_item = pd.DataFrame({\"mo_item_code\": [], \"mo_item_desc\": [], \"mo_group_code\": [], \"mo_group_desc\": []})\n",
    "\n",
    "for modop in pd.read_csv('../csv_data/cfs_2014_lwmodop.csv', chunksize=chunksize, \n",
    "                       iterator=True, low_memory=False):\n",
    "    \n",
    "    mo_item = mo_item.append(pd.concat([ modop['moitem'],\n",
    "                                         modop['itemdesc'],\n",
    "                                         modop['mogroup'],\n",
    "                                         modop['groupdesc'] ],\n",
    "                                        axis=1, keys=['mo_item_code', 'mo_item_desc',\n",
    "                                                      'mo_group_code', 'mo_group_desc']))\n",
    "\n",
    "    for c in modop.columns:\n",
    "        if c not in keep_columns:\n",
    "            modop = modop.drop(c, axis=1)\n",
    "\n",
    "    # rename to the CFS Analytics column names\n",
    "    modop.rename(columns=modop_mappings, inplace=True)\n",
    "\n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    modop = modop.applymap(safe_strip)\n",
    "    mo_item = mo_item.applymap(safe_strip)\n",
    "    \n",
    "    # The group codes are getting a decimal place for some reason.  convert them to ints\n",
    "    mo_item['mo_group_code'] = mo_item['mo_group_code'].map(lambda x: str(int(x)))\n",
    "    \n",
    "    # Drop duplicate mo_items\n",
    "    mo_item = mo_item.drop_duplicates()\n",
    "    \n",
    "    # Gotta get rid of any of the incident records we had to drop due to duplicate \"primary keys\"\n",
    "    modop = modop[~(modop.incident_id.isin((498659, 503578, 521324)))]\n",
    "    \n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "    \n",
    "    # store in the database\n",
    "    modop.to_sql('modus_operandi', engine, index=False, if_exists='append')\n",
    "\n",
    "# Fix weird exception row causing a key error)\n",
    "mo_item['mo_item_desc'] = mo_item['mo_item_desc'].map(lambda x: \"Discharged\" if x == \"Discharged34\" else x)\n",
    "mo_item.to_sql('mo_item', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Adding foreign key constraints\n",
    "We can't add some of the foreign key constraints until all the data is in there, so we'll do that down here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x10b9727b8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"\"\"\n",
    "ALTER TABLE incident\n",
    "ADD CONSTRAINT incident_ucr_short_desc_fkey FOREIGN KEY (ucr_short_desc) REFERENCES ucr_desc (ucr_short_desc);\n",
    "\"\"\")\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "ALTER TABLE modus_operandi\n",
    "ADD CONSTRAINT mo_mo_item_code_fkey\n",
    "FOREIGN KEY (mo_item_code, mo_group_code) REFERENCES mo_item (mo_item_code, mo_group_code);\n",
    "\"\"\")\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "ALTER TABLE weapon\n",
    "ADD CONSTRAINT weapon_weapon_desc_fk FOREIGN KEY (weapon_desc) REFERENCES weapon_group (weapon_desc);\n",
    "\"\"\")\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "ALTER TABLE premise\n",
    "ADD CONSTRAINT premise_premise_desc_fk FOREIGN KEY (premise_desc) REFERENCES premise_group (premise_desc);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
