{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Loading, Cleaning, and Normalization\n",
    "We need to load the data from .csv into Postgres.  We also need to normalize the data to make analysis easy.  We'll use Pandas to deal with the .csv loading and data storage.\n",
    "\n",
    "Files we need to load:\n",
    "- cfs_2014_inmain.csv (CFS data)\n",
    "- cfs_xxx2014_incilog.csv (CFS event data -- one for each month)\n",
    "- cfs_2014_lwmain.csv (incident data)\n",
    "- cfs_2014_lwmodop.csv (incident modus operandi data)\n",
    "- LWMAIN.THING.csv (incident lookup tables, where THING is one of the following: CSSTATUS, EMDIVISION, EMSECTION, EMUNIT, INSTSTATS, PREMISE, or WEAPON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use pandas and sqlalchemy to stuff the data into a local instance of postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import datetime as dt\n",
    "from IPython.display import display\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inci_id</th>\n",
       "      <th>calltime</th>\n",
       "      <th>calldow</th>\n",
       "      <th>case_id</th>\n",
       "      <th>callsource</th>\n",
       "      <th>primeunit</th>\n",
       "      <th>firstdisp</th>\n",
       "      <th>streetno</th>\n",
       "      <th>streetonly</th>\n",
       "      <th>street</th>\n",
       "      <th>...</th>\n",
       "      <th>secs2tr</th>\n",
       "      <th>secsar2tr</th>\n",
       "      <th>lastclr</th>\n",
       "      <th>secs2lc</th>\n",
       "      <th>secsar2lc</th>\n",
       "      <th>secstr2lc</th>\n",
       "      <th>timeclose</th>\n",
       "      <th>reptaken</th>\n",
       "      <th>closecode</th>\n",
       "      <th>closecomm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014000002</td>\n",
       "      <td>1/1/14 0:00:22</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PHONE</td>\n",
       "      <td>BK2</td>\n",
       "      <td>BK2</td>\n",
       "      <td>301</td>\n",
       "      <td>S ELM ST</td>\n",
       "      <td>301 S ELM ST</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1/1/14 0:04:20</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1/1/14 0:04:22</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014000003</td>\n",
       "      <td>1/1/14 0:00:40</td>\n",
       "      <td>4</td>\n",
       "      <td>14000001</td>\n",
       "      <td>SELF</td>\n",
       "      <td>B200</td>\n",
       "      <td>B200</td>\n",
       "      <td>1610</td>\n",
       "      <td>GUESS RD</td>\n",
       "      <td>1610 GUESS RD</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1/1/14 0:15:57</td>\n",
       "      <td>918</td>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "      <td>1/1/14 0:15:59</td>\n",
       "      <td>B200</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      inci_id        calltime  calldow   case_id callsource primeunit  \\\n",
       "0  2014000002  1/1/14 0:00:22        4       NaN      PHONE    BK2      \n",
       "1  2014000003  1/1/14 0:00:40        4  14000001      SELF     B200     \n",
       "\n",
       "    firstdisp  streetno streetonly         street    ...    secs2tr  \\\n",
       "0  BK2              301   S ELM ST   301 S ELM ST    ...          0   \n",
       "1  B200            1610   GUESS RD  1610 GUESS RD    ...          0   \n",
       "\n",
       "   secsar2tr         lastclr secs2lc  secsar2lc  secstr2lc       timeclose  \\\n",
       "0          0  1/1/14 0:04:20     238          0          0  1/1/14 0:04:22   \n",
       "1          0  1/1/14 0:15:57     918        917          0  1/1/14 0:15:59   \n",
       "\n",
       "  reptaken  closecode closecomm  \n",
       "0                  10       NaN  \n",
       "1   B200            1       NaN  \n",
       "\n",
       "[2 rows x 53 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.read_csv('../csv_data/cfs_2014_inmain.csv', nrows=2).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the tables before touching the data so they have all the proper constraints.  Pandas' to_sql method, while helpful, won't handle the constraints automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Database DDL\n",
    "\n",
    "Code to create the database schema is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://localhost/cfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_db():\n",
    "    \"\"\"\n",
    "    Remove and recreate tables to prepare for reloading the db\n",
    "    \"\"\"\n",
    "    engine.execute(\"DROP TABLE IF EXISTS note CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS call CASCADE;\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE call\n",
    "    (\n",
    "      call_id bigint NOT NULL,\n",
    "      call_time timestamp without time zone,\n",
    "      call_dow bigint,\n",
    "      case_id text,\n",
    "      call_source text,\n",
    "      primary_unit text,\n",
    "      first_dispatched text,\n",
    "      street_num text,\n",
    "      street_name text,\n",
    "      city_desc text,\n",
    "      zip text,\n",
    "      crossroad1 text,\n",
    "      crossroad2 text,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      service text,\n",
    "      agency text,\n",
    "      beat text,\n",
    "      district text,\n",
    "      sector text,\n",
    "      business text,\n",
    "      nature_code text,\n",
    "      nature_desc text,\n",
    "      priority text,\n",
    "      report_only bigint,\n",
    "      cancelled bigint,\n",
    "      time_enroute timestamp without time zone,\n",
    "      time_finished timestamp without time zone,\n",
    "      first_unit_dispatch timestamp without time zone,\n",
    "      first_unit_enroute timestamp without time zone,\n",
    "      first_unit_arrive timestamp without time zone,\n",
    "      first_unit_transport timestamp without time zone,\n",
    "      last_unit_clear timestamp without time zone,\n",
    "      time_closed timestamp without time zone,\n",
    "      reporting_unit text,\n",
    "      close_code text,\n",
    "      close_comm text,\n",
    "      CONSTRAINT call_id_pkey PRIMARY KEY (call_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE note\n",
    "    (\n",
    "      note_id serial NOT NULL,\n",
    "      text text,\n",
    "      \"timestamp\" timestamp without time zone,\n",
    "      author text,\n",
    "      call_id integer,\n",
    "      CONSTRAINT note_pkey PRIMARY KEY (note_id),\n",
    "      CONSTRAINT note_call_id_fkey FOREIGN KEY (call_id) REFERENCES call (call_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE call_log\n",
    "    (\n",
    "      call_log_id bigint NOT NULL,\n",
    "      transaction_code text,\n",
    "      transaction_desc text,\n",
    "      \"timestamp\" timestamp without time zone,\n",
    "      call_id bigint,\n",
    "      unit_code text,\n",
    "      radio_or_event text,\n",
    "      unitper_id bigint,\n",
    "      close_code text,\n",
    "      CONSTRAINT call_log_pkey PRIMARY KEY (call_log_id),\n",
    "      CONSTRAINT call_log_call_id_fkey FOREIGN KEY (call_id) REFERENCES call (call_id)\n",
    "    );\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##cfs_2014_inmain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 seconds: completed 20000 rows\n",
      "34 seconds: completed 40000 rows\n",
      "60 seconds: completed 60000 rows\n",
      "86 seconds: completed 80000 rows\n",
      "112 seconds: completed 100000 rows\n",
      "139 seconds: completed 120000 rows\n",
      "165 seconds: completed 140000 rows\n",
      "192 seconds: completed 160000 rows\n",
      "220 seconds: completed 180000 rows\n",
      "247 seconds: completed 200000 rows\n",
      "275 seconds: completed 220000 rows\n",
      "301 seconds: completed 240000 rows\n",
      "328 seconds: completed 260000 rows\n",
      "355 seconds: completed 280000 rows\n",
      "383 seconds: completed 300000 rows\n",
      "410 seconds: completed 320000 rows\n",
      "437 seconds: completed 340000 rows\n",
      "465 seconds: completed 360000 rows\n",
      "489 seconds: completed 380000 rows\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "timestamp_expr = re.compile(\"\\[(\\d{2}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}) (.+?)\\]\")\n",
    "\n",
    "def split_notes_dict(notes,call_id):\n",
    "    \"\"\"\n",
    "    Return a list of dicts.  Each dict represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    dicts = []\n",
    "    regex_split = timestamp_expr.split(notes)[:-1]  # get rid of the last empty string created by the split\n",
    "    for i in range(0,len(regex_split),3):\n",
    "        text = regex_split[i].strip()\n",
    "        timestamp = dt.datetime.strptime(regex_split[i+1], \"%m/%d/%y %H:%M:%S\")\n",
    "        author = regex_split[i+2]\n",
    "        dicts.append({\"text\": text, \"timestamp\": timestamp, \"author\": author, \"call_id\": call_id})\n",
    "    return dicts\n",
    "\n",
    "def split_notes(notes):\n",
    "    \"\"\"\n",
    "    Return a list of tuples.  Each tuple represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    notes = str(notes)\n",
    "    tuples = []\n",
    "    regex_split = timestamp_expr.split(notes)[:-1]  # get rid of the last empty string created by the split\n",
    "    for i in range(0,len(regex_split),3):\n",
    "        text = regex_split[i].strip()\n",
    "        timestamp = dt.datetime.strptime(regex_split[i+1], \"%m/%d/%y %H:%M:%S\")\n",
    "        author = regex_split[i+2]\n",
    "        tuples.append((text, timestamp, author))\n",
    "    return tuples\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "chunksize = 20000\n",
    "j = 0\n",
    "\n",
    "# We need to map the inmain columns to the renamed columns in the call table\n",
    "# if an inmain column isn't in this dict, it means we need to drop it\n",
    "call_mappings = {\n",
    "    \"inci_id\": \"call_id\",\n",
    "    \"calltime\": \"call_time\",\n",
    "    \"calldow\": \"call_dow\",\n",
    "    \"case_id\": \"case_id\",\n",
    "    \"callsource\": \"call_source\",\n",
    "    \"primeunit\": \"primary_unit\",\n",
    "    \"firstdisp\": \"first_dispatched\",\n",
    "    \"streetno\": \"street_num\",\n",
    "    \"streetonly\": \"street_name\",\n",
    "    \"citydesc\": \"city_desc\",\n",
    "    \"zip\": \"zip\",\n",
    "    \"crossroad1\": \"crossroad1\",\n",
    "    \"crossroad2\": \"crossroad2\",\n",
    "    \"geox\": \"geox\",\n",
    "    \"geoy\": \"geoy\",\n",
    "    \"service\": \"service\",\n",
    "    \"agency\": \"agency\",\n",
    "    \"statbeat\": \"beat\",\n",
    "    \"district\": \"district\",\n",
    "    \"ra\": \"sector\",\n",
    "    \"business\": \"business\",\n",
    "    \"naturecode\": \"nature_code\",\n",
    "    \"nature\": \"nature_desc\",\n",
    "    \"priority\": \"priority\",\n",
    "    \"rptonly\": \"report_only\",\n",
    "    \"cancelled\": \"cancelled\",\n",
    "    \"timeroute\": \"time_enroute\",\n",
    "    \"timefini\": \"time_finished\",\n",
    "    \"firstdtm\": \"first_unit_dispatch\",\n",
    "    \"firstenr\": \"first_unit_enroute\",\n",
    "    \"firstarrv\": \"first_unit_arrive\",\n",
    "    \"firsttran\": \"first_unit_transport\",\n",
    "    \"lastclr\": \"last_unit_clear\",\n",
    "    \"timeclose\": \"time_closed\",\n",
    "    \"reptaken\": \"reporting_unit\",\n",
    "    \"closecode\": \"close_code\",\n",
    "    \"closecomm\": \"close_comm\"\n",
    "}\n",
    "\n",
    "keep_columns = set(call_mappings.keys())\n",
    "\n",
    "for call in pd.read_csv('../csv_data/cfs_2014_inmain.csv', chunksize=chunksize, iterator=True, encoding='ISO-8859-1',\n",
    "                       low_memory=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    nice, clean iterative algorithm for separating out the notes data -- unfortunately, it's prohibitively slow\n",
    "    (~3 mins per 25k record or thereabouts)\n",
    "    \"\"\"\n",
    "    #for index, row in call.iterrows():\n",
    "    #    note = note.append(pd.DataFrame(split_notes_dict(str(row['notes']), row['inci_id'])))\n",
    "        #if call.iloc[i]['naturecode'] not in nature_set:\n",
    "        #    nature_set.add(call.iloc[i]['naturecode'])\n",
    "        #    nature = nature.append(pd.DataFrame({\"nature_code\": [call.iloc[i]['naturecode']],\n",
    "        #                                \"nature_desc\": [call.iloc[i]['nature']]}))\n",
    "   \n",
    "    \"\"\"\n",
    "    Horrid ugly algorithm for separating out the notes data -- it's faster by about 10x though\n",
    "    Pandas is really slow when iterating on rows, so we have to do all the transformations to a whole series/list\n",
    "    at a time\n",
    "    \"\"\"\n",
    "    # Create a new series, which is (for each call) a list of tuples containing the text, author, and timestamp\n",
    "    # of that call:\n",
    "    # ex. Series([\"one long string with text, author, timestamp for all remarks\"]) -> \n",
    "    #     Series([(text, author, timestamp), (text2, author2, timestamp2)])\n",
    "    call['collected_notes'] = call['notes'].apply(split_notes)\n",
    "    \n",
    "    # Combine the previous series with the inci_id of each row, preserving the relationship between inci_id\n",
    "    # and each individual remark, then convert it to a list so we can reduce and map\n",
    "    # ex. Series([(text, author, timestamp), (text2, author2, timestamp2)]) ->\n",
    "    #     [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)]\n",
    "    combined_notes = call['collected_notes'].combine(call['inci_id'],\n",
    "                                                          lambda x,y: [(e,y) for e in x]).tolist()\n",
    "    \n",
    "    # Reduce the list of lists using extend; instead of a list of lists of tuples, we have one long list of\n",
    "    # nested tuples\n",
    "    # ex. [[((text, author, timestamp), inci_id)], [((text2, author2, timestamp2), inci_id2)]] ->\n",
    "    #     [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)]\n",
    "    extended_notes = []\n",
    "    for l in combined_notes:\n",
    "        extended_notes.extend(l)\n",
    "    \n",
    "    # Flatten the tuples, so we have a list of non-nested tuples\n",
    "    # ex. [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)] ->\n",
    "    #     [(text, author, timestamp, inci_id), (text2, author2, timestamp2, inci_id2)]\n",
    "    extended_notes = map(lambda x: (x[0][0],x[0][1],x[0][2],x[1]), extended_notes)\n",
    "    \n",
    "    # Create a dataframe from the list of tuples (whew)\n",
    "    note = pd.DataFrame.from_records(extended_notes, columns=['text','timestamp','author','call_id'])\n",
    "    \n",
    "    # drop unnecessary columns\n",
    "    for c in call.columns:\n",
    "        if c not in keep_columns:\n",
    "            call = call.drop(c, axis=1)   \n",
    "    \n",
    "    # rename to the CFS Analytics column names\n",
    "    call.rename(columns=call_mappings, inplace=True)\n",
    "    \n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    # Perform datetime conversions\n",
    "    call['call_time'] = pd.to_datetime(call['call_time'])\n",
    "    call['time_enroute'] = pd.to_datetime(call['time_enroute'])\n",
    "    call['time_finished'] = pd.to_datetime(call['time_finished'])\n",
    "    call['first_unit_dispatch'] = pd.to_datetime(call['first_unit_dispatch'])\n",
    "    call['first_unit_enroute'] = pd.to_datetime(call['first_unit_enroute'])\n",
    "    call['first_unit_arrive'] = pd.to_datetime(call['first_unit_arrive'])\n",
    "    call['first_unit_transport'] = pd.to_datetime(call['first_unit_transport'])\n",
    "    call['last_unit_clear'] = pd.to_datetime(call['last_unit_clear'])\n",
    "    call['time_closed'] = pd.to_datetime(call['time_closed'])\n",
    "\n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "    \n",
    "\n",
    "    # store in the database\n",
    "    call.to_sql('call', engine, index=False, if_exists='append')\n",
    "    note.to_sql('note', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_xxx2014_incilog.csv\n",
    "There is one of these for each month, so we have to load them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months = (\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\")\n",
    "\n",
    "for month in months:\n",
    "    start = dt.datetime.now()\n",
    "    print(\"Starting load for month: %s\" % (month))\n",
    "    # load the data in chunks so we don't use too much memory\n",
    "    chunksize = 20000\n",
    "    j = 0\n",
    "\n",
    "    # We need to map the incilog columns to the renamed columns in the call_log table\n",
    "    # if an incilog column isn't in this dict, it means we need to drop it\n",
    "    call_log_mappings = {\n",
    "        \"incilogid\": \"call_log_id\",\n",
    "        \"transtype\": \"transaction_code\",\n",
    "        \"descript\": \"transaction_desc\",\n",
    "        \"timestamp\": \"timestamp\",\n",
    "        \"inci_id\": \"call_id\",\n",
    "        \"unitcode\": \"unit_code\",\n",
    "        \"radorev\": \"radio_or_event\",\n",
    "        \"unitperid\": \"unitper_id\",\n",
    "        \"closecode\": \"close_code\"\n",
    "    }\n",
    "    \n",
    "    keep_columns = set(call_log_mappings.keys())\n",
    "\n",
    "    for call_log in pd.read_csv('../csv_data/cfs_%s2014_incilog.csv' % (month), chunksize=chunksize, \n",
    "                           iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "        for c in call_log.columns:\n",
    "            if c not in keep_columns:\n",
    "                call_log = call_log.drop(c, axis=1)\n",
    "\n",
    "        # rename to the CFS Analytics column names\n",
    "        call_log.rename(columns=call_log_mappings, inplace=True)\n",
    "\n",
    "        ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "        ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "        \n",
    "        # january has some bogus call_ids, so we have to filter them\n",
    "        if month == \"jan\":\n",
    "            call_log = call_log[call_log.call_id > 2014000000]\n",
    "            \n",
    "        # Perform datetime conversions\n",
    "        call_log['timestamp'] = pd.to_datetime(call_log['timestamp'])\n",
    "        \n",
    "        # progress update\n",
    "        j+=1\n",
    "        print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "        # store in the database\n",
    "        call_log.to_sql('call_log', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmain.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6ac341518ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                                         axis=1, keys=['ucr_short_desc', 'ucr_long_desc']))\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mdate_filed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_rept'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%m/%d/%y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mtime_filed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%I:%M %p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not Series"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "chunksize = 20000\n",
    "j = 0\n",
    "\n",
    "# We need to map the incilog columns to the renamed columns in the call_log table\n",
    "# if an incilog column isn't in this dict, it means we need to drop it\n",
    "incident_mappings = {\n",
    "    \"lwmainid\": \"incident_id\",\n",
    "    \"inci_id\": \"call_id\",\n",
    "    \"time\": \"time_filed\",\n",
    "    \"streetnbr\": \"street_num\",\n",
    "    \"street\": \"street_name\",\n",
    "    \"city\": \"city\",\n",
    "    \"zip\": \"zip\",\n",
    "    \"geox\": \"geox\",\n",
    "    \"geoy\": \"geoy\",\n",
    "    \"tract\": \"beat\",\n",
    "    \"district\": \"district\",\n",
    "    \"reportarea\": \"sector\",\n",
    "    \"premise\": \"premise_code\",\n",
    "    \"weapon\": \"weapon_code\",\n",
    "    \"domestic\": \"domestic\",\n",
    "    \"juvenile\": \"juvenile\",\n",
    "    \"gangrelat\": \"gang_related\",\n",
    "    \"emunit\": \"emp_bureau_code\",\n",
    "    \"emdivision\": \"emp_division_code\",\n",
    "    \"emsection\": \"emp_unit_code\",\n",
    "    \"asst_offcr\": \"num_officers\",\n",
    "    \"invststats\": \"investigation_status_code\",\n",
    "    \"investunit\": \"investigator_unit_code\",\n",
    "    \"csstatus\": \"case_status_code\",\n",
    "    \"lwchrgid\": \"lwchrgid\",\n",
    "    \"chrgcnt\": \"charge_seq\",\n",
    "    \"ucr_code\": \"ucr_code\",\n",
    "    \"arr_chrg\": \"ucr_short_desc\",\n",
    "    \"attm_comp\": \"attempted_or_committed\"\n",
    "}\n",
    "\n",
    "keep_columns = set(incident_mappings.keys())\n",
    "\n",
    "ucr_desc = pd.DataFrame({\"ucr_short_desc\": [], \"ucr_long_desc\": []})\n",
    "\n",
    "for incident in pd.read_csv('../csv_data/cfs_2014_lwmain.csv', chunksize=chunksize, \n",
    "                       iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "    \n",
    "    ucr_desc = ucr_desc.append(pd.concat([ incident['arr_chrg'],\n",
    "                                           incident['chrgdesc'] ],\n",
    "                                        axis=1, keys=['ucr_short_desc', 'ucr_long_desc']))\n",
    "    \n",
    "    date_filed = dt.datetime.strptime(incident['date_rept'], \"%m/%d/%y\")\n",
    "    time_filed = dt.datetime.strptime(incident['time'], \"%I:%M %p\")\n",
    "    \n",
    "    # Perform datetime conversions\n",
    "    incident['time_filed'] = dt.datetime(date_filed.year, date_filed.month, date_filed.day,\n",
    "                                         time_filed.hour, time_filed.minute)\n",
    "    \n",
    "    for c in incident.columns:\n",
    "        if c not in keep_columns:\n",
    "            incident = incident.drop(c, axis=1)\n",
    "\n",
    "    # rename to the CFS Analytics column names\n",
    "    incident.rename(columns=incident_mappings, inplace=True)\n",
    "\n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    # Drop duplicate ucr_descs\n",
    "    ucr_desc = ucr_desc.drop_duplicates()\n",
    "    \n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "    # store in the database\n",
    "    incident.to_sql('incident', engine, index=False, if_exists='append')\n",
    "\n",
    "ucr_desc.to_sql('ucr_desc', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial exploration of the Durham PD CFS data using non-robust .csv reading code.  Has windows line endings, so have to open the file in universal mode to account for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "first = True\n",
    "incilog_header = \"\"\n",
    "incilog = []\n",
    "\n",
    "with open(\"cfs_mar2015_incilog.csv\",\"rU\") as f:\n",
    "    for line in f.readlines():\n",
    "        if first:\n",
    "            incilog_header = line\n",
    "            first = False\n",
    "        else:\n",
    "            incilog.append([datum.strip() for datum in line.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['63260886',\n",
      " 'RPTO',\n",
      " 'Report Only',\n",
      " '3/27/15 15:22:41',\n",
      " '55361',\n",
      " '2014412231',\n",
      " 'B125',\n",
      " 'R',\n",
      " '997150',\n",
      " '']\n"
     ]
    }
   ],
   "source": [
    "pprint(incilog[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = True\n",
    "inmain_header = \"\"\n",
    "inmain = []\n",
    "\n",
    "with open(\"cfs_mar2015_inmain.csv\",\"rU\") as f:\n",
    "    for line in f.readlines():\n",
    "        if first:\n",
    "            inmain_header = line\n",
    "            first = False\n",
    "        else:\n",
    "            inmain.append([datum.strip() for datum in line.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015087068',\n",
      " '3/1/15 0:00:32',\n",
      " '1',\n",
      " '',\n",
      " 'E911',\n",
      " 'C413',\n",
      " 'C424',\n",
      " '617',\n",
      " 'HOPE AVE',\n",
      " '617 HOPE AVE',\n",
      " 'DURHAM',\n",
      " '27707',\n",
      " 'ANACOSTA ST',\n",
      " 'LINCOLN ST',\n",
      " '2030390.25',\n",
      " '807470.19',\n",
      " 'LAW',\n",
      " 'DPD',\n",
      " '412',\n",
      " 'D4',\n",
      " 'STH',\n",
      " '',\n",
      " 'ASSIST',\n",
      " 'ASSIST PERSON',\n",
      " '4',\n",
      " '0',\n",
      " '0',\n",
      " 'actve dist...child advised mom and aunt aruging  [03/01/15 00:01:14 SMITHK]  WRLS  [03/01/15 00:01:19 SMITHK]  NO PHASE 2.....EHX SHOWS 500 MAHONE POSS APT1  [03/01/15 00:04:09 SMITHK]  [EPD] Aborted by Law Priority with code: 1. Caller hung up  [03/01/15 00:07:42 SMITHK]  {C413} NEED BETTER LOCATION  [03/01/15 00:09:50 ROSSA]',\n",
      " '3/1/15 0:04:11',\n",
      " '219',\n",
      " '3/1/15 0:08:11',\n",
      " '459',\n",
      " '3/1/15 0:04:53',\n",
      " '261',\n",
      " '42',\n",
      " '0',\n",
      " '3/1/15 0:04:53',\n",
      " '261',\n",
      " '0',\n",
      " '3/1/15 0:09:32',\n",
      " '540',\n",
      " '279',\n",
      " 'NULL',\n",
      " '0',\n",
      " '0',\n",
      " '3/1/15 0:34:42',\n",
      " '2050',\n",
      " '1510',\n",
      " '0',\n",
      " '3/1/15 0:34:43',\n",
      " '',\n",
      " '10',\n",
      " '']\n"
     ]
    }
   ],
   "source": [
    "pprint(inmain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dispatcher's remarks are all concatenated together, separated by brackets containing what appear to be timestamps and names.  We'll use regexes to pull these apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actve dist...child advised mom and aunt aruging  ',\n",
      " '03/01/15 00:01:14',\n",
      " 'SMITHK',\n",
      " '  WRLS  ',\n",
      " '03/01/15 00:01:19',\n",
      " 'SMITHK',\n",
      " '  NO PHASE 2.....EHX SHOWS 500 MAHONE POSS APT1  ',\n",
      " '03/01/15 00:04:09',\n",
      " 'SMITHK',\n",
      " '  [EPD] Aborted by Law Priority with code: 1. Caller hung up  ',\n",
      " '03/01/15 00:07:42',\n",
      " 'SMITHK',\n",
      " '  {C413} NEED BETTER LOCATION  ',\n",
      " '03/01/15 00:09:50',\n",
      " 'ROSSA',\n",
      " '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "timestamp_expr = re.compile(\"\\[(\\d{2}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}) (.+?)\\]\")\n",
    "\n",
    "test_str = \"actve dist...child advised mom and aunt aruging  [03/01/15 00:01:14 SMITHK]  \\\n",
    "WRLS  [03/01/15 00:01:19 SMITHK]  \\\n",
    "NO PHASE 2.....EHX SHOWS 500 MAHONE POSS APT1  [03/01/15 00:04:09 SMITHK]  \\\n",
    "[EPD] Aborted by Law Priority with code: 1. Caller hung up  [03/01/15 00:07:42 SMITHK]  \\\n",
    "{C413} NEED BETTER LOCATION  [03/01/15 00:09:50 ROSSA]\"\n",
    "\n",
    "pprint(timestamp_expr.split(test_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function we can use to get the data for each individual note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('actve dist...child advised mom and aunt aruging',\n",
      "  datetime.datetime(2015, 3, 1, 0, 1, 14),\n",
      "  'SMITHK'),\n",
      " ('WRLS', datetime.datetime(2015, 3, 1, 0, 1, 19), 'SMITHK'),\n",
      " ('NO PHASE 2.....EHX SHOWS 500 MAHONE POSS APT1',\n",
      "  datetime.datetime(2015, 3, 1, 0, 4, 9),\n",
      "  'SMITHK'),\n",
      " ('[EPD] Aborted by Law Priority with code: 1. Caller hung up',\n",
      "  datetime.datetime(2015, 3, 1, 0, 7, 42),\n",
      "  'SMITHK'),\n",
      " ('{C413} NEED BETTER LOCATION',\n",
      "  datetime.datetime(2015, 3, 1, 0, 9, 50),\n",
      "  'ROSSA')]\n"
     ]
    }
   ],
   "source": [
    "def split_notes(notes):\n",
    "    \"\"\"\n",
    "    Return a list of 3-tuples.  Each tuple represents a single note and contains the timestamp, the note-taker, and\n",
    "    the text of the note.\n",
    "    \"\"\"\n",
    "    tuples = []\n",
    "    regex_split = timestamp_expr.split(notes)[:-1]  # get rid of the last empty string created by the split\n",
    "    for i in range(0,len(regex_split),3):\n",
    "        note = regex_split[i].strip()\n",
    "        timestamp = dt.datetime.strptime(regex_split[i+1], \"%m/%d/%y %H:%M:%S\")\n",
    "        notetaker = regex_split[i+2]\n",
    "        tuples.append((note,timestamp,notetaker))\n",
    "    return tuples\n",
    "\n",
    "pprint(split_notes(test_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions we need answered about some of the fields:\n",
    "\n",
    "inmain\n",
    "- can we get any more info about the cases from the case_id? (case_id: case number, if a report is generated from the call)\n",
    "- callsources: E911, ALARM self-explanatory, but SELF, PHONE and RADIO?\n",
    "- primeunit: what are the responsibilities of the prime unit?\n",
    "- service is always LAW, agency is always DPD\n",
    "- nature/naturecode: differences between HANG UP, HANG UP WIRELESS PHASE 1, and HANG UP WIRELESS PHASE 2?\n",
    "- notes: need abbreviations used, can maybe get some of them from the nature codes\n",
    "- meanings of closecodes?\n",
    "\n",
    "incilog\n",
    "- each unit = one officer? any additional info we can get from unitper table, such as officer pay to more accurately estimate cost?\n",
    "\n",
    "assuming \"code_agcy\" for all since that matches up best with the data\n",
    "lwmain.csstatus\n",
    "- which code (code_fbi, code_sbi, code_agcy) is the one corresponding to the csstatus foreign key? (assuming code_agcy) are any columns other than descriptn informative?\n",
    "\n",
    "same questions for lwmain.emdivision, emsection, emunit, invststats, premise, weapon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (eventually) Here we'll create the database schema to store the CFS data in a more structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# I think we're actually going to use postgres -- maybe not worry about the specific db implementation for now\n",
    "\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('dpd_cfs.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "CREATE_INCIDENT = \\\n",
    "\\\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS incident (\n",
    "    inci_id INTEGER PRIMARY KEY,\n",
    "    calltime TIMESTAMP,\n",
    "    calldow INTEGER,\n",
    "    case_id INTEGER,\n",
    "    callsource VARCHAR,\n",
    "    primeunit VARCHAR,\n",
    "    firstdisp VARCHAR,\n",
    "    streetno INTEGER,\n",
    "    streetonly VARCHAR,\n",
    "    street VARCHAR,\n",
    "    citydesc VARCHAR,\n",
    "    zip INTEGER,\n",
    "    crossroad1 VARCHAR,\n",
    "    crossroad2 VARCHAR,\n",
    "    geox DOUBLE,\n",
    "    geoy DOUBLE,\n",
    "    service VARCHAR,\n",
    "    agency VARCHAR,\n",
    "    \n",
    "    )\n",
    "\\\"\"\"\n",
    "\n",
    "c.execute('')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
