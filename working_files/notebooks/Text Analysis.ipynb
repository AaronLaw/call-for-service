{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be performing exploratory text analysis on the notes from Durham's CFS data.  Some ideas for places to take this:\n",
    "\n",
    " - Word frequencies for an initial look\n",
    " - Figure out the pattern (if any) behind the dispatcher scripts, including priority, chief complaint, questions/answers, and subject/vehicle descriptions\n",
    "\n",
    "Some observations from reading remarks:\n",
    " - \"x ft from STREET NAME\" indicates directed patrol at that location\n",
    " - there is a structure for transports that includes the distance traveled with a transport, but the officers don't appear to fill it out consistently\n",
    " - 10 codes are seen as \"10-72\" or just \"72\"\n",
    " - CAD-generated messages are prefixed by a bracketed descriptor -- usually \"[EPD]\".  Messages left by the units are preceded by (UNIT NUMBER).\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at word frequencies, we have to combine all the words into a single corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/dialects/postgresql/base.py:2421: SAWarning: Did not recognize type 'geometry' of column 'point'\n",
      "  (attype, name))\n",
      "/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/dialects/postgresql/base.py:2421: SAWarning: Did not recognize type 'geometry' of column 'geom'\n",
      "  (attype, name))\n",
      "/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/dialects/postgresql/base.py:2421: SAWarning: Did not recognize type 'geometry' of column 'extent'\n",
      "  (attype, name))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import dataset\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "db_uri = \"postgresql://jnance:@localhost:5432/cfs\"\n",
    "\n",
    "engine = create_engine(db_uri)\n",
    "db = dataset.connect(db_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/dialects/postgresql/base.py:2421: SAWarning: Did not recognize type 'geometry' of column 'geom'\n",
      "  (attype, name))\n",
      "/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/dialects/postgresql/base.py:2421: SAWarning: Did not recognize type 'geometry' of column 'extent'\n",
      "  (attype, name))\n"
     ]
    }
   ],
   "source": [
    "notes = []\n",
    "\n",
    "for row in db.query(\"SELECT body FROM note;\"):\n",
    "    if row['body']:\n",
    "        notes += row['body'].lower().split()\n",
    "    \n",
    "txt = nltk.Text(notes)\n",
    "fd = nltk.FreqDist(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the most frequent words in the corpus.  Many of these will probably become domain stopwords. (note: must download the nltk stopwords corpus using `nltk.download()` before this will work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= TOP 100 WORDS =========\n",
      "1. caller\n",
      "2. [epd]\n",
      "3. -\n",
      "4. questions:\n",
      "5. ***\n",
      "6. scene.\n",
      "7. is:\n",
      "8. incident\n",
      "9. vehicle\n",
      "10. response:\n",
      "11. chief\n",
      "12. complaint:\n",
      "13. statement:\n",
      "14. 1.\n",
      "15. 2.\n",
      "16. 3.\n",
      "17. 4.\n",
      "18. 5.\n",
      "19. 6.\n",
      "20. info\n",
      "21. /\n",
      "22. 7.\n",
      "23. one\n",
      "24. code:\n",
      "25. person\n",
      "26. .\n",
      "27. description\n",
      "28. known\n",
      "29. involved.\n",
      "30. involves\n",
      "31. dispatch\n",
      "32. male\n",
      "33. 8.\n",
      "34. suspect\n",
      "35. call\n",
      "36. 9.\n",
      "37. weapons\n",
      "38. adv\n",
      "39. 1.suspect:\n",
      "40. back\n",
      "41. 10.\n",
      "42. location\n",
      "43. race:\n",
      "44. priority\n",
      "45. gender:\n",
      "46. 11.\n",
      "47. color:\n",
      "48. age:\n",
      "49. danger.\n",
      "50. clothing:\n",
      "51. alarm\n",
      "52. party\n",
      "53. cb\n",
      "54. happened\n",
      "55. blk\n",
      "56. business/resident/owner\n",
      "57. suspect`s\n",
      "58. reported\n",
      "59. known.\n",
      "60. 12.\n",
      "61. phone\n",
      "62. number\n",
      "63. traffic\n",
      "64. left\n",
      "65. suspect/person\n",
      "66. 2\n",
      "67. progress.\n",
      "68. area.\n",
      "69. involved\n",
      "70. female\n",
      "71. victim\n",
      "72. calling\n",
      "73. suspicious\n",
      "74. black\n",
      "75. pd\n",
      "76. name\n",
      "77. responsible\n",
      "78. n/a\n",
      "79. make:\n",
      "80. medical\n",
      "81. mentioned.\n",
      "82. area\n",
      "83. needs\n",
      "84. door\n",
      "85. 13.\n",
      "86. s/he\n",
      "87. body:\n",
      "88. [fire]\n",
      "89. 2nd\n",
      "90. victim.\n",
      "91. disturbance\n",
      "92. see\n",
      "93. ft\n",
      "94. disturbance.\n",
      "95. white\n",
      "96. veh\n",
      "97. line\n",
      "98. open\n",
      "99. model:\n",
      "100. theft\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"========= TOP 100 WORDS =========\")\n",
    "l = fd.most_common()\n",
    "ndx = 0\n",
    "num = 1\n",
    "ignore = nltk.corpus.stopwords.words('english')\n",
    "while (num <= 100 and ndx < len(l)):\n",
    "    if l[ndx][0] not in ignore:\n",
    "        print(str(num) + \". \" + l[ndx][0])\n",
    "        num += 1\n",
    "    ndx += 1\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these words are part of the call scripts transcribed by the CAD system, describing the caller's answers to specific questions asked by the dispatcher.  We'll probably need to add the script words to a stopword list, but we can keep the descriptive words that go along with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dpd_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "domain_stopwords = set(['caller', '[epd]', '-', 'questions:', 'scene.', '***', 'is:', 'incident', 'response:', 'chief',\n",
    "                       'complaint:', 'statement:', 'info', 'description', 'known', 'involved', 'involves', 'dispatch',\n",
    "                       '1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '/', 'code:',\n",
    "                       '.', 'call', 'suspect', 'adv', '1.suspect:', 'location', 'race:', 'priority', 'gender:',\n",
    "                       'color:', 'age:', 'clothing:', 'cb', 'suspect`s', 'reported', 'known.', 'suspect/person',\n",
    "                       'progress.', 'area.', 'pd', 'name', 'n/a', 'make:', 'area', 's/he',\n",
    "                       '[fire]', 'model:'])\n",
    "\n",
    "dpd_stopwords.update(domain_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying again with domain stopwords added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= TOP 100 WORDS =========\n",
      "1. vehicle\n",
      "2. one\n",
      "3. person\n",
      "4. involved.\n",
      "5. male\n",
      "6. weapons\n",
      "7. back\n",
      "8. danger.\n",
      "9. alarm\n",
      "10. party\n",
      "11. happened\n",
      "12. blk\n",
      "13. business/resident/owner\n",
      "14. phone\n",
      "15. number\n",
      "16. traffic\n",
      "17. left\n",
      "18. 2\n",
      "19. female\n",
      "20. victim\n",
      "21. calling\n",
      "22. suspicious\n",
      "23. black\n",
      "24. responsible\n",
      "25. medical\n",
      "26. mentioned.\n",
      "27. needs\n",
      "28. door\n",
      "29. body:\n",
      "30. 2nd\n",
      "31. victim.\n",
      "32. disturbance\n",
      "33. see\n",
      "34. ft\n",
      "35. disturbance.\n",
      "36. white\n",
      "37. veh\n",
      "38. line\n",
      "39. open\n",
      "40. theft\n",
      "41. attention.\n",
      "42. past:\n",
      "43. aborted\n",
      "44. leave\n",
      "45. alarms\n",
      "46. reportedly\n",
      "47. caller.\n",
      "48. 14.\n",
      "49. drugs\n",
      "50. monitoring\n",
      "51. company.\n",
      "52. two\n",
      "53. activation\n",
      "54. still\n",
      "55. keyholder/owner\n",
      "56. blocking\n",
      "57. emergency\n",
      "58. car\n",
      "59. alarm.\n",
      "60. law\n",
      "61. incident.\n",
      "62. contacted.\n",
      "63. property\n",
      "64. st\n",
      "65. someone\n",
      "66. front\n",
      "67. going\n",
      "68. advised\n",
      "69. house\n",
      "70. vm\n",
      "71. suspect/person/vehicle\n",
      "72. 1\n",
      "73. 4\n",
      "74. vehicles\n",
      "75. blue\n",
      "76. burglary/intrusion\n",
      "77. tty\n",
      "78. unk\n",
      "79. no:\n",
      "80. alcohol\n",
      "81. 3\n",
      "82. officer\n",
      "83. vehicle.\n",
      "84. people\n",
      "85. 3rd\n",
      "86. might\n",
      "87. in.\n",
      "88. wants\n",
      "89. ago):\n",
      "90. (minutes\n",
      "91. name:\n",
      "92. wanted\n",
      "93. slowing\n",
      "94. apt\n",
      "95. stated\n",
      "96. small\n",
      "97. contact\n",
      "98. shirt\n",
      "99. needed\n",
      "100. susp\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"========= TOP 100 WORDS =========\")\n",
    "l = fd.most_common()\n",
    "ndx = 0\n",
    "num = 1\n",
    "ignore = dpd_stopwords\n",
    "while (num <= 100 and ndx < len(l)):\n",
    "    if l[ndx][0] not in ignore:\n",
    "        print(str(num) + \". \" + l[ndx][0])\n",
    "        num += 1\n",
    "    ndx += 1\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gang-related\n",
    "##Data gathering\n",
    "We did a segment for Tampa on gang-related calls.  In this case, we have the benefit of a \"curated\" data set, as the incidents have a flag indicating whether a given call is gang-related.  Let's see if we can classify calls as gang-related or not based on the call notes.\n",
    "\n",
    "First, we'll get the info for each call.  Some of this may be useful in the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23699, 40)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_info = pd.read_sql(\"SELECT call.*, gang_related FROM call,\"\n",
    "                        \"incident WHERE call.incident_id = incident.incident_id AND EXISTS \"\n",
    "                        \"(SELECT 1 FROM note where call_id = call.call_id) ORDER BY call_id;\",\n",
    "                        engine)\n",
    "call_info.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the columns to see which appear to be the most informative -- we'll drop any ones that are likely unnecessary or won't be accessible at the start of a call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['call_id', 'month_received', 'week_received', 'dow_received', 'hour_received', 'case_id', 'call_source_id', 'primary_unit_id', 'first_dispatched_id', 'reporting_unit_id', 'street_num', 'street_name', 'city_id', 'zip', 'crossroad1', 'crossroad2', 'geox', 'geoy', 'beat', 'district', 'sector', 'business', 'nature_id', 'priority', 'report_only', 'cancelled', 'time_received', 'time_routed', 'time_finished', 'first_unit_dispatch', 'first_unit_enroute', 'first_unit_arrive', 'first_unit_transport', 'last_unit_clear', 'time_closed', 'close_code_id', 'close_comments', 'incident_id', 'year_received', 'gang_related'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(call_info.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_info['street'] = call_info['street_num'] + ' ' + call_info['street_name']\n",
    "\n",
    "drop_cols = ['case_id', 'reporting_unit_id', 'geox', 'geoy', 'report_only', 'cancelled', 'time_received',\n",
    "             'time_routed', 'time_finished', 'first_unit_dispatch', 'first_unit_enroute', 'first_unit_arrive',\n",
    "             'first_unit_transport', 'last_unit_clear', 'time_closed', 'close_code_id', 'close_comments',\n",
    "             'street_num', 'street_name']\n",
    "\n",
    "for col in drop_cols:\n",
    "    call_info = call_info.drop(col, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23699, 23)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23699, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"separate_notes = defaultdict(list)\n",
    "\n",
    "for row in db.query(\"SELECT * from note;\"):\n",
    "    separate_notes[row['call_id']].append(row['body'])\n",
    "\n",
    "note_bodies = []\n",
    "call_ids = []\n",
    "    \n",
    "for call_id in separate_notes.keys():\n",
    "    call_ids.append(call_id)\n",
    "    note_bodies.append('\\n'.join(separate_notes[call_id]))\n",
    "    \n",
    "notes = pd.DataFrame({'note': note_bodies}, index=call_ids)\n",
    "notes.index.name = 'call_id'\n",
    "notes.head()\"\"\"\n",
    "\n",
    "notes = pd.read_sql(\"SELECT note.call_id, body FROM note INNER JOIN call ON note.call_id=call.call_id \"\n",
    "                    \"WHERE call.incident_id IS NOT NULL ORDER BY call_id;\", con=engine)\n",
    "notes['body'] = notes['body'].map(lambda x: '' if x is None else x)\n",
    "note_grp = notes.groupby('call_id', sort=False).agg(lambda x: '\\n'.join(x.tolist()))\n",
    "note_grp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23699, 1064)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#Get a vector of counts to use for synsetting and stemming\n",
    "\n",
    "vec = CountVectorizer(stop_words=stopwords, min_df=0.005, max_df=0.99)\n",
    "data = vec.fit_transform(note_grp['body'].tolist()).toarray()\n",
    "data_names = vec.get_feature_names()\n",
    "\n",
    "group = pd.DataFrame(data, columns=data_names)\n",
    "group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23699, 4212)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Make DataFrame with synsets occuring >= 150 times\n",
    "for column in group.columns:\n",
    "    for synset in wn.synsets(column):\n",
    "        try:\n",
    "            synset_df[synset.name()] += group[column]\n",
    "        except KeyError:\n",
    "            synset_df[synset.name()] = group[column]\n",
    "\n",
    "for column in synset_df.columns:\n",
    "    if sum(synset_df[column]) < 250:\n",
    "        synset_df.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "synset_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23699, 660)\n"
     ]
    }
   ],
   "source": [
    "# Make DataFrame with most common stems\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stemmer_df = pd.DataFrame()\n",
    "for column in group.columns:\n",
    "    try:\n",
    "        stemmer_df[stemmer.stem(column)] += group[column]\n",
    "    except KeyError:\n",
    "        stemmer_df[stemmer.stem(column)] = group[column]\n",
    "        \n",
    "for column in stemmer_df.columns:\n",
    "    if sum(stemmer_df[column]) < 250:\n",
    "        stemmer_df.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "for column in stemmer_df.columns:\n",
    "    new_column = column + '_stem'\n",
    "    stemmer_df.rename(columns={column: new_column}, inplace=True)\n",
    "    \n",
    "print(stemmer_df.shape)\n",
    "    \n",
    "synset_and_stems = pd.concat([synset_df, stemmer_df], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23699, 166)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get another vector of counts to use by itself as a feature\n",
    "\n",
    "vec = CountVectorizer(stop_words=stopwords, min_df=0.05, max_df=0.99)\n",
    "data = vec.fit_transform(note_grp['body'].tolist()).toarray()\n",
    "data_names = vec.get_feature_names()\n",
    "\n",
    "group = pd.DataFrame(data, columns=data_names)\n",
    "group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_data = pd.concat([call_info, group, synset_and_stems], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_data.to_csv('../csv_data/gang_related.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23699, 5061)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnance/anaconda/lib/python3.4/site-packages/pandas/io/parsers.py:1159: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../csv_data/gang_related.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    21003\n",
       "NaN       2103\n",
       "True       593\n",
       "dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gang_related'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21596, 5061)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[pd.notnull(data['gang_related'])]\n",
    "data.replace([np.inf, -np.inf], np.nan)\n",
    "data = data.fillna(value=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['gang_related'] = data['gang_related'].astype(int)\n",
    "\n",
    "\n",
    "for str_col in ('street_name', 'crossroad1', 'crossroad2', 'beat', 'district', 'sector', 'business', 'priority'):\n",
    "    le = LabelEncoder()\n",
    "    data[str_col] = pd.Series(le.fit_transform(data[str_col].tolist()))\n",
    "data_features = data.drop('gang_related', 1)\n",
    "data_features_values = data_features.values\n",
    "feature_labels = list(data_features.columns.values)\n",
    "\n",
    "data = data.fillna(value=0)\n",
    "\n",
    "#Subset training\n",
    "data_train = data_features.loc[4250:]\n",
    "data_train_features_values = data_train.values\n",
    "\n",
    "#Subset target\n",
    "data_train_target = data.loc[4250:]['gang_related']\n",
    "data_train_target_values = data_train_target.values #.astype('<U32')\n",
    "data_train_features_values = data_train_features_values #.astype('<U32')\n",
    "\n",
    "#Subset validation\n",
    "data_valid = data_features.loc[0:4250]\n",
    "data_valid_features_values = data_valid.values\n",
    "\n",
    "data_valid_features_values = data_valid_features_values #.astype('float32')\n",
    "data_valid_target = data.loc[0:4250]['gang_related']\n",
    "data_valid_target_values = data_valid_target.values #.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-11642363d5a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train_features_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train_target_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1318\u001b[0m                                  \"the primal form.\")\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[1;32m    350\u001b[0m                              array.ndim)\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     50\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     51\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 52\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(penalty='l1', cv=5, solver='liblinear')\n",
    "clf.fit(data_train_features_values, data_train_target_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing=clf.predict(data_valid_features_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.score(data_valid_features_values, data_valid_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
