{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELOAD WEAPON, CASE_STATUS, REATTEMPT INCIDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Loading, Cleaning, and Normalization\n",
    "We need to load the data from .csv into Postgres.  We also need to normalize the data to make analysis easy.  We'll use Pandas to deal with the .csv loading and data storage.\n",
    "\n",
    "Files we need to load:\n",
    "- cfs_2014_inmain.csv (CFS data)\n",
    "- cfs_xxx2014_incilog.csv (CFS event data -- one for each month)\n",
    "- cfs_2014_lwmain.csv (incident data)\n",
    "- cfs_2014_lwmodop.csv (incident modus operandi data)\n",
    "- LWMAIN.THING.csv (incident lookup tables, where THING is one of the following: CSSTATUS, EMDIVISION, EMSECTION, EMUNIT, INSTSTATS, PREMISE, or WEAPON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use pandas and sqlalchemy to stuff the data into a local instance of postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import datetime as dt\n",
    "from IPython.display import display\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the tables before touching the data so they have all the proper constraints.  Pandas' to_sql method, while helpful, won't handle the constraints automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Database DDL\n",
    "\n",
    "Code to create the database schema is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://localhost/cfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_db():\n",
    "    \"\"\"\n",
    "    Remove and recreate tables to prepare for reloading the db\n",
    "    \"\"\"\n",
    "    engine.execute(\"DROP TABLE IF EXISTS note CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS call CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS call_log CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS ucr_desc CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS incident CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS modus_operandi CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS mo_group CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS mo_item CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS bureau CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS case_status CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS division CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS unit CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS investigation_status CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS weapon CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS weapon_group CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS premise CASCADE;\")\n",
    "    engine.execute(\"DROP TABLE IF EXISTS premise_group CASCADE;\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE call\n",
    "    (\n",
    "      call_id bigint NOT NULL,\n",
    "      call_time timestamp without time zone,\n",
    "      call_dow bigint,\n",
    "      case_id text,\n",
    "      call_source text,\n",
    "      primary_unit text,\n",
    "      first_dispatched text,\n",
    "      street_num text,\n",
    "      street_name text,\n",
    "      city_desc text,\n",
    "      zip text,\n",
    "      crossroad1 text,\n",
    "      crossroad2 text,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      service text,\n",
    "      agency text,\n",
    "      beat text,\n",
    "      district text,\n",
    "      sector text,\n",
    "      business text,\n",
    "      nature_code text,\n",
    "      nature_desc text,\n",
    "      priority text,\n",
    "      report_only bigint,\n",
    "      cancelled bigint,\n",
    "      time_enroute timestamp without time zone,\n",
    "      time_finished timestamp without time zone,\n",
    "      first_unit_dispatch timestamp without time zone,\n",
    "      first_unit_enroute timestamp without time zone,\n",
    "      first_unit_arrive timestamp without time zone,\n",
    "      first_unit_transport timestamp without time zone,\n",
    "      last_unit_clear timestamp without time zone,\n",
    "      time_closed timestamp without time zone,\n",
    "      reporting_unit text,\n",
    "      close_code text,\n",
    "      close_comm text,\n",
    "      CONSTRAINT call_id_pkey PRIMARY KEY (call_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE note\n",
    "    (\n",
    "      note_id serial NOT NULL,\n",
    "      text text,\n",
    "      \"timestamp\" timestamp without time zone,\n",
    "      author text,\n",
    "      call_id bigint,\n",
    "      CONSTRAINT note_pkey PRIMARY KEY (note_id),\n",
    "      CONSTRAINT note_call_id_fkey FOREIGN KEY (call_id) REFERENCES call (call_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE call_log\n",
    "    (\n",
    "      call_log_id bigint NOT NULL,\n",
    "      transaction_code text,\n",
    "      transaction_desc text,\n",
    "      \"timestamp\" timestamp without time zone,\n",
    "      call_id bigint,\n",
    "      unit_code text,\n",
    "      radio_or_event text,\n",
    "      unitper_id bigint,\n",
    "      close_code text,\n",
    "      --CONSTRAINT call_log_call_id_fkey FOREIGN KEY (call_id) REFERENCES call (call_id) --nullable\n",
    "      CONSTRAINT call_log_pkey PRIMARY KEY (call_log_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE ucr_desc\n",
    "    (\n",
    "      ucr_long_desc text,\n",
    "      ucr_short_desc text NOT NULL,\n",
    "      CONSTRAINT ucr_desc_pkey PRIMARY KEY (ucr_short_desc)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE bureau\n",
    "    (\n",
    "      bureau_code text,\n",
    "      bureau_desc text,\n",
    "      CONSTRAINT bureau_pkey PRIMARY KEY (bureau_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE division\n",
    "    (\n",
    "      division_code text,\n",
    "      division_desc text,\n",
    "      CONSTRAINT division_pkey PRIMARY KEY (division_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE investigation_status\n",
    "    (\n",
    "      investigation_status_code text,\n",
    "      investigation_status_desc text,\n",
    "      CONSTRAINT investigation_status_pkey PRIMARY KEY (investigation_status_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE case_status\n",
    "    (\n",
    "      case_status_code text,\n",
    "      case_status_desc text,\n",
    "      CONSTRAINT case_status_pkey PRIMARY KEY (case_status_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE unit\n",
    "    (\n",
    "      unit_code text,\n",
    "      unit_desc text,\n",
    "      CONSTRAINT unit_pkey PRIMARY KEY (unit_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE weapon_group\n",
    "    (\n",
    "      weapon_group text,\n",
    "      weapon_desc text,\n",
    "      CONSTRAINT weapon_group_pkey PRIMARY KEY (weapon_desc)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE premise_group\n",
    "    (\n",
    "      premise_group text,\n",
    "      premise_desc text,\n",
    "      CONSTRAINT premise_group_pkey PRIMARY KEY (premise_desc)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE weapon\n",
    "    (\n",
    "      weapon_code text,\n",
    "      weapon_desc text,\n",
    "      CONSTRAINT weapon_pkey PRIMARY KEY (weapon_code),\n",
    "      CONSTRAINT weapon_weapon_desc_fk FOREIGN KEY (weapon_desc) REFERENCES weapon_group (weapon_desc)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE premise\n",
    "    (\n",
    "      premise_code text,\n",
    "      premise_desc text,\n",
    "      CONSTRAINT premise_pkey PRIMARY KEY (premise_code),\n",
    "      CONSTRAINT premise_premise_desc_fk FOREIGN KEY (premise_desc) REFERENCES premise_group (premise_desc)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE incident\n",
    "    (\n",
    "      incident_id bigint NOT NULL,\n",
    "      call_id bigint,\n",
    "      time_filed timestamp without time zone,\n",
    "      street_num text,\n",
    "      street_name text,\n",
    "      city text,\n",
    "      zip text,\n",
    "      geox bigint,\n",
    "      geoy bigint,\n",
    "      beat text,\n",
    "      district text,\n",
    "      sector text,\n",
    "      premise_code bigint,\n",
    "      weapon_code bigint,\n",
    "      domestic text,\n",
    "      juvenile text,\n",
    "      gang_related text,\n",
    "      emp_bureau_code text,\n",
    "      emp_division_code text,\n",
    "      emp_unit_code text,\n",
    "      num_officers integer,\n",
    "      investigation_status_code text,\n",
    "      investigator_unit_code text,\n",
    "      case_status_code bigint,\n",
    "      lwchrgid bigint,\n",
    "      charge_seq bigint,\n",
    "      ucr_code bigint,\n",
    "      ucr_short_desc text,\n",
    "      attempted_or_committed text,\n",
    "      CONSTRAINT incident_pkey PRIMARY KEY (incident_id)\n",
    "      \n",
    "      --EVERYTHING IS NULLABLE AAAAAAGH\n",
    "      --CONSTRAINT incident_case_status_code_fkey --nullable\n",
    "      --  FOREIGN KEY (case_status_code) REFERENCES case_status (case_status_code),\n",
    "      --CONSTRAINT incident_emp_bureau_code_fkey\n",
    "      --  FOREIGN KEY (emp_bureau_code) REFERENCES bureau (bureau_code),\n",
    "      --CONSTRAINT incident_emp_division_code_fkey\n",
    "      --  FOREIGN KEY (emp_division_code) REFERENCES division (division_code),\n",
    "      --CONSTRAINT incident_emp_unit_code_fkey\n",
    "      --  FOREIGN KEY (emp_unit_code) REFERENCES unit (unit_code),\n",
    "      --CONSTRAINT incident_investigator_unit_code_fkey -- nullable\n",
    "      --  FOREIGN KEY (investigator_unit_code) REFERENCES unit (unit_code),\n",
    "      --CONSTRAINT incident_investigation_status_code_fkey\n",
    "      --  FOREIGN KEY (investigation_status_code) REFERENCES investigation_status (investigation_status_code),\n",
    "      --CONSTRAINT incident_premise_code_fkey\n",
    "      --  FOREIGN KEY (premise_code) REFERENCES premise (premise_code),\n",
    "      --CONSTRAINT incident_weapon_code_fkey\n",
    "      --  FOREIGN KEY (weapon_code) REFERENCES weapon (weapon_code),\n",
    "      --CONSTRAINT incident_call_id_fkey\n",
    "      --  FOREIGN KEY (call_id) REFERENCES call (call_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE mo_item\n",
    "    (\n",
    "      mo_item_code text,\n",
    "      mo_item_desc text,\n",
    "      mo_group_code text,\n",
    "      mo_group_desc text,\n",
    "      CONSTRAINT mo_item_pkey PRIMARY KEY (mo_item_code, mo_group_code)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    engine.execute(\"\"\"\n",
    "    CREATE TABLE modus_operandi\n",
    "    (\n",
    "      incident_id bigint,\n",
    "      mo_id bigint,\n",
    "      mo_group_code bigint,\n",
    "      mo_item_code text,\n",
    "      CONSTRAINT mo_pkey PRIMARY KEY (mo_id),\n",
    "      CONSTRAINT mo_incident_id_fkey FOREIGN KEY (incident_id) REFERENCES incident (incident_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##cfs_2014_inmain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "timestamp_expr = re.compile(\"\\[(\\d{2}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}) (.+?)\\]\")\n",
    "\n",
    "def split_notes_dict(notes,call_id):\n",
    "    \"\"\"\n",
    "    Return a list of dicts.  Each dict represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    dicts = []\n",
    "    regex_split = timestamp_expr.split(notes)[:-1]  # get rid of the last empty string created by the split\n",
    "    for i in range(0,len(regex_split),3):\n",
    "        text = regex_split[i].strip()\n",
    "        timestamp = dt.datetime.strptime(regex_split[i+1], \"%m/%d/%y %H:%M:%S\")\n",
    "        author = regex_split[i+2]\n",
    "        dicts.append({\"text\": text, \"timestamp\": timestamp, \"author\": author, \"call_id\": call_id})\n",
    "    return dicts\n",
    "\n",
    "def split_notes(notes):\n",
    "    \"\"\"\n",
    "    Return a list of tuples.  Each tuple represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    notes = str(notes)\n",
    "    tuples = []\n",
    "    regex_split = timestamp_expr.split(notes)[:-1]  # get rid of the last empty string created by the split\n",
    "    for i in range(0,len(regex_split),3):\n",
    "        text = regex_split[i].strip()\n",
    "        timestamp = dt.datetime.strptime(regex_split[i+1], \"%m/%d/%y %H:%M:%S\")\n",
    "        author = regex_split[i+2]\n",
    "        tuples.append((text, timestamp, author))\n",
    "    return tuples\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "chunksize = 20000\n",
    "j = 0\n",
    "\n",
    "# We need to map the inmain columns to the renamed columns in the call table\n",
    "# if an inmain column isn't in this dict, it means we need to drop it\n",
    "call_mappings = {\n",
    "    \"inci_id\": \"call_id\",\n",
    "    \"calltime\": \"call_time\",\n",
    "    \"calldow\": \"call_dow\",\n",
    "    \"case_id\": \"case_id\",\n",
    "    \"callsource\": \"call_source\",\n",
    "    \"primeunit\": \"primary_unit\",\n",
    "    \"firstdisp\": \"first_dispatched\",\n",
    "    \"streetno\": \"street_num\",\n",
    "    \"streetonly\": \"street_name\",\n",
    "    \"citydesc\": \"city_desc\",\n",
    "    \"zip\": \"zip\",\n",
    "    \"crossroad1\": \"crossroad1\",\n",
    "    \"crossroad2\": \"crossroad2\",\n",
    "    \"geox\": \"geox\",\n",
    "    \"geoy\": \"geoy\",\n",
    "    \"service\": \"service\",\n",
    "    \"agency\": \"agency\",\n",
    "    \"statbeat\": \"beat\",\n",
    "    \"district\": \"district\",\n",
    "    \"ra\": \"sector\",\n",
    "    \"business\": \"business\",\n",
    "    \"naturecode\": \"nature_code\",\n",
    "    \"nature\": \"nature_desc\",\n",
    "    \"priority\": \"priority\",\n",
    "    \"rptonly\": \"report_only\",\n",
    "    \"cancelled\": \"cancelled\",\n",
    "    \"timeroute\": \"time_enroute\",\n",
    "    \"timefini\": \"time_finished\",\n",
    "    \"firstdtm\": \"first_unit_dispatch\",\n",
    "    \"firstenr\": \"first_unit_enroute\",\n",
    "    \"firstarrv\": \"first_unit_arrive\",\n",
    "    \"firsttran\": \"first_unit_transport\",\n",
    "    \"lastclr\": \"last_unit_clear\",\n",
    "    \"timeclose\": \"time_closed\",\n",
    "    \"reptaken\": \"reporting_unit\",\n",
    "    \"closecode\": \"close_code\",\n",
    "    \"closecomm\": \"close_comm\"\n",
    "}\n",
    "\n",
    "keep_columns = set(call_mappings.keys())\n",
    "\n",
    "for call in pd.read_csv('../csv_data/cfs_2014_inmain.csv', chunksize=chunksize, iterator=True, encoding='ISO-8859-1',\n",
    "                       low_memory=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    nice, clean iterative algorithm for separating out the notes data -- unfortunately, it's prohibitively slow\n",
    "    (~3 mins per 25k record or thereabouts)\n",
    "    \"\"\"\n",
    "    #for index, row in call.iterrows():\n",
    "    #    note = note.append(pd.DataFrame(split_notes_dict(str(row['notes']), row['inci_id'])))\n",
    "        #if call.iloc[i]['naturecode'] not in nature_set:\n",
    "        #    nature_set.add(call.iloc[i]['naturecode'])\n",
    "        #    nature = nature.append(pd.DataFrame({\"nature_code\": [call.iloc[i]['naturecode']],\n",
    "        #                                \"nature_desc\": [call.iloc[i]['nature']]}))\n",
    "   \n",
    "    \"\"\"\n",
    "    Horrid ugly algorithm for separating out the notes data -- it's faster by about 10x though\n",
    "    Pandas is really slow when iterating on rows, so we have to do all the transformations to a whole series/list\n",
    "    at a time\n",
    "    \"\"\"\n",
    "    # Create a new series, which is (for each call) a list of tuples containing the text, author, and timestamp\n",
    "    # of that call:\n",
    "    # ex. Series([\"one long string with text, author, timestamp for all remarks\"]) -> \n",
    "    #     Series([(text, author, timestamp), (text2, author2, timestamp2)])\n",
    "    call['collected_notes'] = call['notes'].apply(split_notes)\n",
    "    \n",
    "    # Combine the previous series with the inci_id of each row, preserving the relationship between inci_id\n",
    "    # and each individual remark, then convert it to a list so we can reduce and map\n",
    "    # ex. Series([(text, author, timestamp), (text2, author2, timestamp2)]) ->\n",
    "    #     [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)]\n",
    "    combined_notes = call['collected_notes'].combine(call['inci_id'],\n",
    "                                                          lambda x,y: [(e,y) for e in x]).tolist()\n",
    "    \n",
    "    # Reduce the list of lists using extend; instead of a list of lists of tuples, we have one long list of\n",
    "    # nested tuples\n",
    "    # ex. [[((text, author, timestamp), inci_id)], [((text2, author2, timestamp2), inci_id2)]] ->\n",
    "    #     [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)]\n",
    "    extended_notes = []\n",
    "    for l in combined_notes:\n",
    "        extended_notes.extend(l)\n",
    "    \n",
    "    # Flatten the tuples, so we have a list of non-nested tuples\n",
    "    # ex. [((text, author, timestamp), inci_id), ((text2, author2, timestamp2), inci_id2)] ->\n",
    "    #     [(text, author, timestamp, inci_id), (text2, author2, timestamp2, inci_id2)]\n",
    "    extended_notes = map(lambda x: (x[0][0],x[0][1],x[0][2],x[1]), extended_notes)\n",
    "    \n",
    "    # Create a dataframe from the list of tuples (whew)\n",
    "    note = pd.DataFrame.from_records(extended_notes, columns=['text','timestamp','author','call_id'])\n",
    "    \n",
    "    # drop unnecessary columns\n",
    "    for c in call.columns:\n",
    "        if c not in keep_columns:\n",
    "            call = call.drop(c, axis=1)   \n",
    "    \n",
    "    # rename to the CFS Analytics column names\n",
    "    call.rename(columns=call_mappings, inplace=True)\n",
    "    \n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    # Perform datetime conversions\n",
    "    call['call_time'] = pd.to_datetime(call['call_time'])\n",
    "    call['time_enroute'] = pd.to_datetime(call['time_enroute'])\n",
    "    call['time_finished'] = pd.to_datetime(call['time_finished'])\n",
    "    call['first_unit_dispatch'] = pd.to_datetime(call['first_unit_dispatch'])\n",
    "    call['first_unit_enroute'] = pd.to_datetime(call['first_unit_enroute'])\n",
    "    call['first_unit_arrive'] = pd.to_datetime(call['first_unit_arrive'])\n",
    "    call['first_unit_transport'] = pd.to_datetime(call['first_unit_transport'])\n",
    "    call['last_unit_clear'] = pd.to_datetime(call['last_unit_clear'])\n",
    "    call['time_closed'] = pd.to_datetime(call['time_closed'])\n",
    "\n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "    \n",
    "    # get rid of excess whitespace\n",
    "    call = call.applymap(safe_strip)\n",
    "    note = note.applymap(safe_strip)\n",
    "    \n",
    "    # store in the database\n",
    "    call.to_sql('call', engine, index=False, if_exists='append')\n",
    "    note.to_sql('note', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_xxx2014_incilog.csv\n",
    "There is one of these for each month, so we have to load them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting load for month: jan\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "19 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "33 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n",
      "43 seconds: completed 200000 rows\n",
      "47 seconds: completed 220000 rows\n",
      "Starting load for month: feb\n",
      "1 seconds: completed 20000 rows\n",
      "6 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "20 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "34 seconds: completed 160000 rows\n",
      "39 seconds: completed 180000 rows\n",
      "42 seconds: completed 200000 rows\n",
      "Starting load for month: mar\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "20 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "34 seconds: completed 160000 rows\n",
      "39 seconds: completed 180000 rows\n",
      "43 seconds: completed 200000 rows\n",
      "47 seconds: completed 220000 rows\n",
      "Starting load for month: apr\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "20 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "34 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n",
      "43 seconds: completed 200000 rows\n",
      "47 seconds: completed 220000 rows\n",
      "Starting load for month: may\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "20 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "34 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n",
      "43 seconds: completed 200000 rows\n",
      "47 seconds: completed 220000 rows\n",
      "Starting load for month: jun\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "19 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "33 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n",
      "42 seconds: completed 200000 rows\n",
      "Starting load for month: jul\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "14 seconds: completed 80000 rows\n",
      "19 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "28 seconds: completed 140000 rows\n",
      "33 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n",
      "Starting load for month: aug\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "14 seconds: completed 80000 rows\n",
      "19 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "28 seconds: completed 140000 rows\n",
      "33 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n",
      "41 seconds: completed 200000 rows\n",
      "Starting load for month: sep\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "20 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "34 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n",
      "Starting load for month: oct\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "20 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "34 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n",
      "42 seconds: completed 200000 rows\n",
      "Starting load for month: nov\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "19 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "34 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n",
      "Starting load for month: dec\n",
      "1 seconds: completed 20000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "10 seconds: completed 60000 rows\n",
      "15 seconds: completed 80000 rows\n",
      "20 seconds: completed 100000 rows\n",
      "24 seconds: completed 120000 rows\n",
      "29 seconds: completed 140000 rows\n",
      "34 seconds: completed 160000 rows\n",
      "38 seconds: completed 180000 rows\n"
     ]
    }
   ],
   "source": [
    "months = (\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\")\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "for month in months:\n",
    "    start = dt.datetime.now()\n",
    "    print(\"Starting load for month: %s\" % (month))\n",
    "    # load the data in chunks so we don't use too much memory\n",
    "    chunksize = 20000\n",
    "    j = 0\n",
    "\n",
    "    # We need to map the incilog columns to the renamed columns in the call_log table\n",
    "    # if an incilog column isn't in this dict, it means we need to drop it\n",
    "    call_log_mappings = {\n",
    "        \"incilogid\": \"call_log_id\",\n",
    "        \"transtype\": \"transaction_code\",\n",
    "        \"descript\": \"transaction_desc\",\n",
    "        \"timestamp\": \"timestamp\",\n",
    "        \"inci_id\": \"call_id\",\n",
    "        \"unitcode\": \"unit_code\",\n",
    "        \"radorev\": \"radio_or_event\",\n",
    "        \"unitperid\": \"unitper_id\",\n",
    "        \"closecode\": \"close_code\"\n",
    "    }\n",
    "    \n",
    "    keep_columns = set(call_log_mappings.keys())\n",
    "\n",
    "    for call_log in pd.read_csv('../csv_data/cfs_%s2014_incilog.csv' % (month), chunksize=chunksize, \n",
    "                           iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "        for c in call_log.columns:\n",
    "            if c not in keep_columns:\n",
    "                call_log = call_log.drop(c, axis=1)\n",
    "\n",
    "        # rename to the CFS Analytics column names\n",
    "        call_log.rename(columns=call_log_mappings, inplace=True)\n",
    "\n",
    "        ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "        ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "            \n",
    "        # Perform datetime conversions\n",
    "        call_log['timestamp'] = pd.to_datetime(call_log['timestamp'])\n",
    "        \n",
    "        # progress update\n",
    "        j+=1\n",
    "        print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "        # strip excess whitespace\n",
    "        call_log = call_log.applymap(safe_strip)\n",
    "        \n",
    "        # store in the database\n",
    "        call_log.to_sql('call_log', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assorted small lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading LWMAIN.CSSTATUS.csv into case_status\n",
      "loading LWMAIN.EMDIVISION.csv into division\n",
      "loading LWMAIN.EMSECTION.csv into unit\n",
      "loading LWMAIN.EMUNIT.csv into bureau\n",
      "loading LWMAIN.INVSTSTATS.csv into investigation_status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nnested_lookup_jobs = [\\n    {\\n        \"file\": \"LWMAIN.PREMISE.csv\",\\n        \"table_outer\": \"premise\",\\n        \"table_inner\": \"premise_group\",\\n        \"\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are a million of these, so let's make life easier and reuse all that code\n",
    "lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.CSSTATUS.csv\",\n",
    "        \"table\": \"case_status\",\n",
    "        \"mapping\": {\"code_agcy\": \"case_status_code\", \"descriptn\": \"case_status_desc\"}\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMDIVISION.csv\",\n",
    "        \"table\": \"division\",\n",
    "        \"mapping\": {\"code_agcy\": \"division_code\", \"descriptn\": \"division_desc\"}\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMSECTION.csv\",\n",
    "        \"table\": \"unit\",\n",
    "        \"mapping\": {\"code_agcy\": \"unit_code\", \"descriptn\": \"unit_desc\"}\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMUNIT.csv\",\n",
    "        \"table\": \"bureau\",\n",
    "        \"mapping\": {\"code_agcy\": \"bureau_code\", \"descriptn\": \"bureau_desc\"}\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.INVSTSTATS.csv\",\n",
    "        \"table\": \"investigation_status\",\n",
    "        \"mapping\": {\"code_agcy\": \"investigation_status_code\", \"descriptn\": \"investigation_status_desc\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in lookup_jobs:\n",
    "    print(\"loading %s into %s\" % (job['file'], job['table']))\n",
    "    data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    \n",
    "    keep_columns = set(job['mapping'].keys())\n",
    "    for c in data.columns:\n",
    "        if c not in keep_columns:\n",
    "            data = data.drop(c, axis=1)\n",
    "            \n",
    "    data.rename(columns=job['mapping'], inplace=True)\n",
    "    data.to_sql(job['table'], engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading LWMAIN.PREMISE.csv into premise and premise_group\n",
      "loading LWMAIN.WEAPON.csv into weapon and weapon_group\n"
     ]
    }
   ],
   "source": [
    "#These have to create \"nested\" tables and are a little tougher, but we can still reuse the code\n",
    "\n",
    "nested_lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.PREMISE.csv\",\n",
    "        \"outer_table\": \"premise\",\n",
    "        \"inner_table\": \"premise_group\",\n",
    "        \"outer_cols\": [\"premise_code\", \"premise_desc\"],\n",
    "        \"inner_cols\": [\"premise_group\", \"premise_desc\"]\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.WEAPON.csv\",\n",
    "        \"outer_table\": \"weapon\",\n",
    "        \"inner_table\": \"weapon_group\",\n",
    "        \"outer_cols\": [\"weapon_code\", \"weapon_desc\"],\n",
    "        \"inner_cols\": [\"weapon_group\", \"weapon_desc\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in nested_lookup_jobs:\n",
    "    print(\"loading %s into %s and %s\" % (job['file'], job['outer_table'], job['inner_table']))\n",
    "    data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    \n",
    "    outer_data = pd.concat([data['code_agcy'], data['descriptn_b']], axis=1, keys=job['outer_cols'])\n",
    "    inner_data = pd.concat([data['descriptn_a'], data['descriptn_b']], axis=1, keys=job['inner_cols'])\n",
    "    \n",
    "    outer_data = outer_data.drop_duplicates()\n",
    "    \n",
    "    outer_data.to_sql(job['outer_table'], engine, index=False, if_exists='append')\n",
    "    inner_data.to_sql(job['inner_table'], engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 seconds: completed 20000 rows\n"
     ]
    },
    {
     "ename": "DataError",
     "evalue": "(psycopg2.DataError) invalid input syntax for integer: \"\"\nLINE 1: ...', 202645613, 78503775, '422', 'D4', 'STH', '51', '', 'N', '...\n                                                             ^\n [SQL: 'INSERT INTO incident (incident_id, call_id, time_filed, street_num, street_name, city, zip, geox, geoy, beat, district, sector, premise_code, weapon_code, domestic, juvenile, gang_related, emp_bureau_code, emp_division_code, emp_unit_code, num_officers, investigation_status_code, investigator_unit_code, case_status_code, lwchrgid, charge_seq, ucr_code, ucr_short_desc, attempted_or_committed) VALUES (%(incident_id)s, %(call_id)s, %(time_filed)s, %(street_num)s, %(street_name)s, %(city)s, %(zip)s, %(geox)s, %(geoy)s, %(beat)s, %(district)s, %(sector)s, %(premise_code)s, %(weapon_code)s, %(domestic)s, %(juvenile)s, %(gang_related)s, %(emp_bureau_code)s, %(emp_division_code)s, %(emp_unit_code)s, %(num_officers)s, %(investigation_status_code)s, %(investigator_unit_code)s, %(case_status_code)s, %(lwchrgid)s, %(charge_seq)s, %(ucr_code)s, %(ucr_short_desc)s, %(attempted_or_committed)s)'] [parameters: ({'ucr_code': 1400, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 0, 14), 'emp_unit_code': 'C', 'premise_code': '32', 'attempted_or_committed': 'COM', 'call_id': 2014000006, 'investigator_unit_code': '', 'beat': '411', 'case_status_code': '1', 'incident_id': 488519, 'district': 'D4', 'juvenile': 'N', 'street_num': '531', 'ucr_short_desc': 'VANAUT', 'emp_bureau_code': 'UPB', 'geox': 202919475, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'S ROXBORO ST', 'geoy': 81465919, 'charge_seq': 1, 'num_officers': '0', 'gang_related': 'UNK', 'weapon_code': '9', 'zip': '27701', 'lwchrgid': 500790}, {'ucr_code': 1400, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 0, 44), 'emp_unit_code': 'B', 'premise_code': '9', 'attempted_or_committed': 'COM', 'call_id': 2014000009, 'investigator_unit_code': 'D3IN', 'beat': '323', 'case_status_code': '2', 'incident_id': 488524, 'district': 'D3', 'juvenile': 'U', 'street_num': '4600', 'ucr_short_desc': 'VANAUT', 'emp_bureau_code': 'UPB', 'geox': 200958350, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'UNIVERSITY DR', 'geoy': 80424525, 'charge_seq': 1, 'num_officers': '0', 'gang_related': 'NO', 'weapon_code': '3', 'zip': '27707', 'lwchrgid': 500761}, {'ucr_code': 510, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 1, 33), 'emp_unit_code': 'B', 'premise_code': '1', 'attempted_or_committed': 'COM', 'call_id': 2014000013, 'investigator_unit_code': 'D1IN', 'beat': '113', 'case_status_code': '2', 'incident_id': 488525, 'district': 'D1', 'juvenile': 'U', 'street_num': '1113', 'ucr_short_desc': 'BURGF', 'emp_bureau_code': 'UPB', 'geox': 203489100, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D2', 'street_name': 'N HYDE PARK AVE', 'geoy': 81795431, 'charge_seq': 1, 'num_officers': '3', 'gang_related': 'UNK', 'weapon_code': '97', 'zip': '27701', 'lwchrgid': 500775}, {'ucr_code': 510, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 1, 10), 'emp_unit_code': 'B', 'premise_code': '1', 'attempted_or_committed': 'COM', 'call_id': 2014000011, 'investigator_unit_code': 'B', 'beat': '125', 'case_status_code': '1', 'incident_id': 488526, 'district': 'D1', 'juvenile': 'N', 'street_num': '607', 'ucr_short_desc': 'BURGF', 'emp_bureau_code': 'UPB', 'geox': 205466038, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D4', 'street_name': 'SOUTHSHORE PKWY', 'geoy': 80867275, 'charge_seq': 1, 'num_officers': '5', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27703', 'lwchrgid': 501271}, {'ucr_code': 300, 'investigation_status_code': 'INAC', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 2, 51), 'emp_unit_code': 'C', 'premise_code': '62', 'attempted_or_committed': 'COM', 'call_id': 2014000018, 'investigator_unit_code': 'D4IN', 'beat': '413', 'case_status_code': '2', 'incident_id': 488527, 'district': 'D4', 'juvenile': 'N', 'street_num': '1099', 'ucr_short_desc': 'ROBIND', 'emp_bureau_code': 'UPB', 'geox': 203302913, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'RIDGEWAY AVE/WABASH ST', 'geoy': 80992138, 'charge_seq': 1, 'num_officers': '2', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27701', 'lwchrgid': 500770}, {'ucr_code': 9910, 'investigation_status_code': 'INAC', 'domestic': 'Y', 'time_filed': datetime.datetime(2014, 1, 1, 1, 17), 'emp_unit_code': 'C', 'premise_code': '1', 'attempted_or_committed': '', 'call_id': 2014000016, 'investigator_unit_code': 'DVU', 'beat': '221', 'case_status_code': '6', 'incident_id': 488528, 'district': 'D2', 'juvenile': 'N', 'street_num': '2206', 'ucr_short_desc': 'DVINFO', 'emp_bureau_code': 'UPB', 'geox': 200654063, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D2', 'street_name': 'ANTHONY DR', 'geoy': 83216331, 'charge_seq': 1, 'num_officers': '3', 'gang_related': 'NO', 'weapon_code': '12', 'zip': '27705', 'lwchrgid': 500854}, {'ucr_code': 810, 'investigation_status_code': 'CBAA', 'domestic': 'Y', 'time_filed': datetime.datetime(2014, 1, 1, 1, 44), 'emp_unit_code': 'B', 'premise_code': '2', 'attempted_or_committed': 'COM', 'call_id': 2014000014, 'investigator_unit_code': 'DVU', 'beat': '122', 'case_status_code': '3', 'incident_id': 488529, 'district': 'D1', 'juvenile': 'N', 'street_num': '1835', 'ucr_short_desc': 'SIMA', 'emp_bureau_code': 'UPB', 'geox': 203883963, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D4', 'street_name': 'CHEEK RD', 'geoy': 82106281, 'charge_seq': 1, 'num_officers': '2', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27704', 'lwchrgid': 500802}, {'ucr_code': 520, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 2, 19), 'emp_unit_code': 'B', 'premise_code': '2', 'attempted_or_committed': 'COM', 'call_id': 2014000017, 'investigator_unit_code': 'D3IN', 'beat': '314', 'case_status_code': '1', 'incident_id': 488530, 'district': 'D3', 'juvenile': 'U', 'street_num': '2920', 'ucr_short_desc': 'BURGNF', 'emp_bureau_code': 'UPB', 'geox': 201798075, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'CHAPEL HILL RD', 'geoy': 80818488, 'charge_seq': 1, 'num_officers': '0', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27707', 'lwchrgid': 500764}  ... displaying 10 of 19992 total bound parameter sets ...  {'ucr_code': 2670, 'investigation_status_code': 'CBAA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 10, 19, 0, 55), 'emp_unit_code': 'B', 'premise_code': '30', 'attempted_or_committed': 'COM', 'call_id': 2014033019, 'investigator_unit_code': 'B', 'beat': '522', 'case_status_code': '3', 'incident_id': 518976, 'district': 'D5', 'juvenile': 'N', 'street_num': '515', 'ucr_short_desc': 'OTHTRE', 'emp_bureau_code': 'UPB', 'geox': 202785400, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'W PETTIGREW ST', 'geoy': 81743213, 'charge_seq': 1, 'num_officers': '1', 'gang_related': 'NO', 'weapon_code': '97', 'zip': '27701', 'lwchrgid': 527253}, {'ucr_code': 510, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 10, 18, 22, 22), 'emp_unit_code': 'B', 'premise_code': '1', 'attempted_or_committed': 'COM', 'call_id': 2014033013, 'investigator_unit_code': 'D4IN', 'beat': '423', 'case_status_code': '2', 'incident_id': 518977, 'district': 'D4', 'juvenile': 'U', 'street_num': '3205', 'ucr_short_desc': 'BURGF', 'emp_bureau_code': 'UPB', 'geox': 204962563, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D4', 'street_name': 'SKYBROOK LN', 'geoy': 79259294, 'charge_seq': 1, 'num_officers': '0', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27703', 'lwchrgid': 527255})]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_executemany\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_executemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataError\u001b[0m: invalid input syntax for integer: \"\"\nLINE 1: ...', 202645613, 78503775, '422', 'D4', 'STH', '51', '', 'N', '...\n                                                             ^\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3f04f36e68c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# store in the database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mincident\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'incident'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mucr_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ucr_desc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflavor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mif_exists\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             dtype=dtype)\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\u001b[0m\n\u001b[1;32m    536\u001b[0m     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n\u001b[1;32m    537\u001b[0m                       \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m                       chunksize=chunksize, dtype=dtype)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1170\u001b[0m                          schema=schema, dtype=dtype)\n\u001b[1;32m   1171\u001b[0m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m         \u001b[0;31m# check for potentially case sensitivity issues (GH7815)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, chunksize)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mchunk_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     def _query_iterator(self, result, chunksize, columns, coerce_float=True,\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m_execute_insert\u001b[0;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, object, *multiparams, **params)\u001b[0m\n\u001b[1;32m    912\u001b[0m                 type(object))\n\u001b[1;32m    913\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, multiparams, params)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_clauseelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munique_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moptionaldict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, multiparams, params)\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mcompiled_sql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mdistilled_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mcompiled_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistilled_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         )\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m                 context)\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1339\u001b[0m                 util.raise_from_cause(\n\u001b[1;32m   1340\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m                     \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m                 )\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_from_cause\u001b[0;34m(exception, exc_info)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     exec(\"def reraise(tp, value, tb=None, cause=None):\\n\"\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb, cause)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1114\u001b[0m                         \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_executemany\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_executemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataError\u001b[0m: (psycopg2.DataError) invalid input syntax for integer: \"\"\nLINE 1: ...', 202645613, 78503775, '422', 'D4', 'STH', '51', '', 'N', '...\n                                                             ^\n [SQL: 'INSERT INTO incident (incident_id, call_id, time_filed, street_num, street_name, city, zip, geox, geoy, beat, district, sector, premise_code, weapon_code, domestic, juvenile, gang_related, emp_bureau_code, emp_division_code, emp_unit_code, num_officers, investigation_status_code, investigator_unit_code, case_status_code, lwchrgid, charge_seq, ucr_code, ucr_short_desc, attempted_or_committed) VALUES (%(incident_id)s, %(call_id)s, %(time_filed)s, %(street_num)s, %(street_name)s, %(city)s, %(zip)s, %(geox)s, %(geoy)s, %(beat)s, %(district)s, %(sector)s, %(premise_code)s, %(weapon_code)s, %(domestic)s, %(juvenile)s, %(gang_related)s, %(emp_bureau_code)s, %(emp_division_code)s, %(emp_unit_code)s, %(num_officers)s, %(investigation_status_code)s, %(investigator_unit_code)s, %(case_status_code)s, %(lwchrgid)s, %(charge_seq)s, %(ucr_code)s, %(ucr_short_desc)s, %(attempted_or_committed)s)'] [parameters: ({'ucr_code': 1400, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 0, 14), 'emp_unit_code': 'C', 'premise_code': '32', 'attempted_or_committed': 'COM', 'call_id': 2014000006, 'investigator_unit_code': '', 'beat': '411', 'case_status_code': '1', 'incident_id': 488519, 'district': 'D4', 'juvenile': 'N', 'street_num': '531', 'ucr_short_desc': 'VANAUT', 'emp_bureau_code': 'UPB', 'geox': 202919475, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'S ROXBORO ST', 'geoy': 81465919, 'charge_seq': 1, 'num_officers': '0', 'gang_related': 'UNK', 'weapon_code': '9', 'zip': '27701', 'lwchrgid': 500790}, {'ucr_code': 1400, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 0, 44), 'emp_unit_code': 'B', 'premise_code': '9', 'attempted_or_committed': 'COM', 'call_id': 2014000009, 'investigator_unit_code': 'D3IN', 'beat': '323', 'case_status_code': '2', 'incident_id': 488524, 'district': 'D3', 'juvenile': 'U', 'street_num': '4600', 'ucr_short_desc': 'VANAUT', 'emp_bureau_code': 'UPB', 'geox': 200958350, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'UNIVERSITY DR', 'geoy': 80424525, 'charge_seq': 1, 'num_officers': '0', 'gang_related': 'NO', 'weapon_code': '3', 'zip': '27707', 'lwchrgid': 500761}, {'ucr_code': 510, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 1, 33), 'emp_unit_code': 'B', 'premise_code': '1', 'attempted_or_committed': 'COM', 'call_id': 2014000013, 'investigator_unit_code': 'D1IN', 'beat': '113', 'case_status_code': '2', 'incident_id': 488525, 'district': 'D1', 'juvenile': 'U', 'street_num': '1113', 'ucr_short_desc': 'BURGF', 'emp_bureau_code': 'UPB', 'geox': 203489100, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D2', 'street_name': 'N HYDE PARK AVE', 'geoy': 81795431, 'charge_seq': 1, 'num_officers': '3', 'gang_related': 'UNK', 'weapon_code': '97', 'zip': '27701', 'lwchrgid': 500775}, {'ucr_code': 510, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 1, 10), 'emp_unit_code': 'B', 'premise_code': '1', 'attempted_or_committed': 'COM', 'call_id': 2014000011, 'investigator_unit_code': 'B', 'beat': '125', 'case_status_code': '1', 'incident_id': 488526, 'district': 'D1', 'juvenile': 'N', 'street_num': '607', 'ucr_short_desc': 'BURGF', 'emp_bureau_code': 'UPB', 'geox': 205466038, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D4', 'street_name': 'SOUTHSHORE PKWY', 'geoy': 80867275, 'charge_seq': 1, 'num_officers': '5', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27703', 'lwchrgid': 501271}, {'ucr_code': 300, 'investigation_status_code': 'INAC', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 2, 51), 'emp_unit_code': 'C', 'premise_code': '62', 'attempted_or_committed': 'COM', 'call_id': 2014000018, 'investigator_unit_code': 'D4IN', 'beat': '413', 'case_status_code': '2', 'incident_id': 488527, 'district': 'D4', 'juvenile': 'N', 'street_num': '1099', 'ucr_short_desc': 'ROBIND', 'emp_bureau_code': 'UPB', 'geox': 203302913, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'RIDGEWAY AVE/WABASH ST', 'geoy': 80992138, 'charge_seq': 1, 'num_officers': '2', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27701', 'lwchrgid': 500770}, {'ucr_code': 9910, 'investigation_status_code': 'INAC', 'domestic': 'Y', 'time_filed': datetime.datetime(2014, 1, 1, 1, 17), 'emp_unit_code': 'C', 'premise_code': '1', 'attempted_or_committed': '', 'call_id': 2014000016, 'investigator_unit_code': 'DVU', 'beat': '221', 'case_status_code': '6', 'incident_id': 488528, 'district': 'D2', 'juvenile': 'N', 'street_num': '2206', 'ucr_short_desc': 'DVINFO', 'emp_bureau_code': 'UPB', 'geox': 200654063, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D2', 'street_name': 'ANTHONY DR', 'geoy': 83216331, 'charge_seq': 1, 'num_officers': '3', 'gang_related': 'NO', 'weapon_code': '12', 'zip': '27705', 'lwchrgid': 500854}, {'ucr_code': 810, 'investigation_status_code': 'CBAA', 'domestic': 'Y', 'time_filed': datetime.datetime(2014, 1, 1, 1, 44), 'emp_unit_code': 'B', 'premise_code': '2', 'attempted_or_committed': 'COM', 'call_id': 2014000014, 'investigator_unit_code': 'DVU', 'beat': '122', 'case_status_code': '3', 'incident_id': 488529, 'district': 'D1', 'juvenile': 'N', 'street_num': '1835', 'ucr_short_desc': 'SIMA', 'emp_bureau_code': 'UPB', 'geox': 203883963, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D4', 'street_name': 'CHEEK RD', 'geoy': 82106281, 'charge_seq': 1, 'num_officers': '2', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27704', 'lwchrgid': 500802}, {'ucr_code': 520, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 1, 1, 2, 19), 'emp_unit_code': 'B', 'premise_code': '2', 'attempted_or_committed': 'COM', 'call_id': 2014000017, 'investigator_unit_code': 'D3IN', 'beat': '314', 'case_status_code': '1', 'incident_id': 488530, 'district': 'D3', 'juvenile': 'U', 'street_num': '2920', 'ucr_short_desc': 'BURGNF', 'emp_bureau_code': 'UPB', 'geox': 201798075, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'CHAPEL HILL RD', 'geoy': 80818488, 'charge_seq': 1, 'num_officers': '0', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27707', 'lwchrgid': 500764}  ... displaying 10 of 19992 total bound parameter sets ...  {'ucr_code': 2670, 'investigation_status_code': 'CBAA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 10, 19, 0, 55), 'emp_unit_code': 'B', 'premise_code': '30', 'attempted_or_committed': 'COM', 'call_id': 2014033019, 'investigator_unit_code': 'B', 'beat': '522', 'case_status_code': '3', 'incident_id': 518976, 'district': 'D5', 'juvenile': 'N', 'street_num': '515', 'ucr_short_desc': 'OTHTRE', 'emp_bureau_code': 'UPB', 'geox': 202785400, 'sector': 'NTH', 'city': 'DURHAM', 'emp_division_code': 'D3', 'street_name': 'W PETTIGREW ST', 'geoy': 81743213, 'charge_seq': 1, 'num_officers': '1', 'gang_related': 'NO', 'weapon_code': '97', 'zip': '27701', 'lwchrgid': 527253}, {'ucr_code': 510, 'investigation_status_code': 'INA', 'domestic': 'N', 'time_filed': datetime.datetime(2014, 10, 18, 22, 22), 'emp_unit_code': 'B', 'premise_code': '1', 'attempted_or_committed': 'COM', 'call_id': 2014033013, 'investigator_unit_code': 'D4IN', 'beat': '423', 'case_status_code': '2', 'incident_id': 518977, 'district': 'D4', 'juvenile': 'U', 'street_num': '3205', 'ucr_short_desc': 'BURGF', 'emp_bureau_code': 'UPB', 'geox': 204962563, 'sector': 'STH', 'city': 'DURHAM', 'emp_division_code': 'D4', 'street_name': 'SKYBROOK LN', 'geoy': 79259294, 'charge_seq': 1, 'num_officers': '0', 'gang_related': 'NO', 'weapon_code': '40', 'zip': '27703', 'lwchrgid': 527255})]"
     ]
    }
   ],
   "source": [
    "def combine_date_time(str_date, str_time):\n",
    "    date = dt.datetime.strptime(str_date, \"%m/%d/%y\")\n",
    "    time = dt.datetime.strptime(str_time, \"%I:%M %p\")\n",
    "    return dt.datetime(date.year, date.month, date.day, time.hour, time.minute)\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "chunksize = 20000\n",
    "j = 0\n",
    "\n",
    "# We need to map the incilog columns to the renamed columns in the call_log table\n",
    "# if an incilog column isn't in this dict, it means we need to drop it\n",
    "incident_mappings = {\n",
    "    \"lwmainid\": \"incident_id\",\n",
    "    \"inci_id\": \"call_id\",\n",
    "    \"time\": \"time_filed\",\n",
    "    \"streetnbr\": \"street_num\",\n",
    "    \"street\": \"street_name\",\n",
    "    \"city\": \"city\",\n",
    "    \"zip\": \"zip\",\n",
    "    \"geox\": \"geox\",\n",
    "    \"geoy\": \"geoy\",\n",
    "    \"tract\": \"beat\",\n",
    "    \"district\": \"district\",\n",
    "    \"reportarea\": \"sector\",\n",
    "    \"premise\": \"premise_code\",\n",
    "    \"weapon\": \"weapon_code\",\n",
    "    \"domestic\": \"domestic\",\n",
    "    \"juvenile\": \"juvenile\",\n",
    "    \"gangrelat\": \"gang_related\",\n",
    "    \"emunit\": \"emp_bureau_code\",\n",
    "    \"emdivision\": \"emp_division_code\",\n",
    "    \"emsection\": \"emp_unit_code\",\n",
    "    \"asst_offcr\": \"num_officers\",\n",
    "    \"invststats\": \"investigation_status_code\",\n",
    "    \"investunit\": \"investigator_unit_code\",\n",
    "    \"csstatus\": \"case_status_code\",\n",
    "    \"lwchrgid\": \"lwchrgid\",\n",
    "    \"chrgcnt\": \"charge_seq\",\n",
    "    \"ucr_code\": \"ucr_code\",\n",
    "    \"arr_chrg\": \"ucr_short_desc\",\n",
    "    \"attm_comp\": \"attempted_or_committed\"\n",
    "}\n",
    "\n",
    "keep_columns = set(incident_mappings.keys())\n",
    "\n",
    "ucr_desc = pd.DataFrame({\"ucr_short_desc\": [], \"ucr_long_desc\": []})\n",
    "\n",
    "for incident in pd.read_csv('../csv_data/cfs_2014_lwmain.csv', chunksize=chunksize, \n",
    "                       iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "    \n",
    "    ucr_desc = ucr_desc.append(pd.concat([ incident['arr_chrg'],\n",
    "                                           incident['chrgdesc'] ],\n",
    "                                        axis=1, keys=['ucr_short_desc', 'ucr_long_desc']))\n",
    "    \n",
    "    # Perform datetime conversions\n",
    "    incident['time'] = incident['date_rept'].combine(incident['time'], combine_date_time)\n",
    "    \n",
    "    for c in incident.columns:\n",
    "        if c not in keep_columns:\n",
    "            incident = incident.drop(c, axis=1)\n",
    "\n",
    "    # rename to the CFS Analytics column names\n",
    "    incident.rename(columns=incident_mappings, inplace=True)\n",
    "\n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    # strip whitespace\n",
    "    incident = incident.applymap(safe_strip)\n",
    "    ucr_desc = ucr_desc.applymap(safe_strip)\n",
    "    \n",
    "    # convert empty strings in num_officers to nulls so we can insert as an int column\n",
    "    incident['num_officers'] = incident['num_officers'].map(lambda x: None if x == '' else x)\n",
    "    \n",
    "    # These \"primary key\" values have two records and I don't want to deal with it\n",
    "    incident = incident[~(incident.incident_id.isin((498659, 503578, 521324)))]\n",
    "    \n",
    "    # incident call_ids don't have the same '20' prefix that the others do, so here we add it\n",
    "    # also get rid of anything pre-2014 because we don't have those in the calls table\n",
    "    incident['call_id'] = incident['call_id'].map(lambda x: x + 2000000000)\n",
    "    incident = incident[incident.call_id > 2014000001]\n",
    "    \n",
    "    # Drop duplicate ucr_descs\n",
    "    ucr_desc = ucr_desc.drop_duplicates()\n",
    "    \n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "    incident = incident.applymap(safe_strip)\n",
    "    \n",
    "    # store in the database\n",
    "    incident.to_sql('incident', engine, index=False, if_exists='append')\n",
    "\n",
    "ucr_desc.to_sql('ucr_desc', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmodop.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 seconds: completed 5000 rows\n",
      "0 seconds: completed 10000 rows\n",
      "1 seconds: completed 15000 rows\n",
      "2 seconds: completed 20000 rows\n",
      "3 seconds: completed 25000 rows\n",
      "3 seconds: completed 30000 rows\n",
      "4 seconds: completed 35000 rows\n",
      "5 seconds: completed 40000 rows\n",
      "6 seconds: completed 45000 rows\n"
     ]
    }
   ],
   "source": [
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "start = dt.datetime.now()\n",
    "# load the data in chunks so we don't use too much memory\n",
    "# strange unexplainable crash using the usual 20k chunk size (and 10k sometimes?), so go with 5k here\n",
    "chunksize = 5000\n",
    "j = 0\n",
    "\n",
    "# We need to map the incilog columns to the renamed columns in the call_log table\n",
    "# if an incilog column isn't in this dict, it means we need to drop it\n",
    "modop_mappings = {\n",
    "    \"lwmainid\": \"incident_id\",\n",
    "    \"lwmodopid\": \"mo_id\",\n",
    "    \"mogroup\": \"mo_group_code\",\n",
    "    \"moitem\": \"mo_item_code\"\n",
    "}\n",
    "\n",
    "keep_columns = set(modop_mappings.keys())\n",
    "\n",
    "mo_item = pd.DataFrame({\"mo_item_code\": [], \"mo_item_desc\": [], \"mo_group_code\": [], \"mo_group_desc\": []})\n",
    "\n",
    "for modop in pd.read_csv('../csv_data/cfs_2014_lwmodop.csv', chunksize=chunksize, \n",
    "                       iterator=True, low_memory=False):\n",
    "    \n",
    "    mo_item = mo_item.append(pd.concat([ modop['moitem'],\n",
    "                                         modop['itemdesc'],\n",
    "                                         modop['mogroup'],\n",
    "                                         modop['groupdesc'] ],\n",
    "                                        axis=1, keys=['mo_item_code', 'mo_item_desc',\n",
    "                                                      'mo_group_code', 'mo_group_desc']))\n",
    "\n",
    "    for c in modop.columns:\n",
    "        if c not in keep_columns:\n",
    "            modop = modop.drop(c, axis=1)\n",
    "\n",
    "    # rename to the CFS Analytics column names\n",
    "    modop.rename(columns=modop_mappings, inplace=True)\n",
    "\n",
    "    ##### USING DPD COLUMN NAMES ABOVE #########\n",
    "    ##### USING CFS ANALYTICS COLUMN NAMES BELOW ######\n",
    "    \n",
    "    modop = modop.applymap(safe_strip)\n",
    "    mo_item = mo_item.applymap(safe_strip)\n",
    "    \n",
    "    # Drop duplicate mo_items\n",
    "    mo_item = mo_item.drop_duplicates()\n",
    "    \n",
    "    # Gotta get rid of any of the incident records we had to drop due to duplicate \"primary keys\"\n",
    "    modop = modop[~(modop.incident_id.isin((498659, 503578, 521324)))]\n",
    "    \n",
    "    # progress update\n",
    "    j+=1\n",
    "    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "    \n",
    "    # store in the database\n",
    "    modop.to_sql('modus_operandi', engine, index=False, if_exists='append')\n",
    "\n",
    "# Fix weird exception row causing a key error)\n",
    "mo_item['mo_item_desc'] = mo_item['mo_item_desc'].map(lambda x: \"Discharged\" if x == \"Discharged34\" else x)\n",
    "mo_item.to_sql('mo_item', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Adding foreign key constraints\n",
    "We can't add some of the foreign key constraints until all the data is in there, so we'll do that down here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#engine.execute(\"\"\"\n",
    "#ALTER TABLE incident\n",
    "#ADD CONSTRAINT incident_call_id_fkey FOREIGN KEY (call_id) REFERENCES call (call_id);\n",
    "#\"\"\")\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "ALTER TABLE incident\n",
    "ADD CONSTRAINT incident_ucr_short_desc_fkey FOREIGN KEY (ucr_short_desc) REFERENCES ucr_desc (ucr_short_desc);\n",
    "\"\"\")\n",
    "\n",
    "engine.execute(\"\"\"\n",
    "ALTER TABLE modus_operandi\n",
    "ADD CONSTRAINT mo_mo_item_code_fkey\n",
    "FOREIGN KEY (mo_item_code, mo_group_code) REFERENCES mo_item (mo_item_code, mo_group_code);\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
