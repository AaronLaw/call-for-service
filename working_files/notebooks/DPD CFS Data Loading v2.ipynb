{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Loading, Cleaning, and Normalization\n",
    "Now that we have a better idea of what the data contains, we're going to load it in a format that will be more efficient for analysis.  Changes still needed:\n",
    " - note splitting regex still has issues.  `SELECT descr, COUNT(*) FROM note_author GROUP BY descr ORDER BY COUNT` will show the weird ones\n",
    " \n",
    "We'll load each table as a two-step process.  First, we scan each table and accumulate a set for each lookup table associated.  We'll then load these lookup tables.  Second, we'll load the main table.  This should be less complicated than trying to accumulate the lookup tables during the chunked-out load of the main table.\n",
    "\n",
    "Main table: call\n",
    "Lookup tables: call_unit, city, nature, beat, district, sector, zip, priority\n",
    "Lookup tables loaded separately: call_source\n",
    "\n",
    "Main table: note\n",
    "Lookup tables: note_author\n",
    "\n",
    "Main table: shift\n",
    "Lookup tables: officer, call_unit (update)\n",
    "\n",
    "Main table: call_log\n",
    "Lookup tables: transaction\n",
    "Lookup tables loaded separately: close_code\n",
    "\n",
    "Main table: incident\n",
    "Lookup tables: city, ucr_descr, ucr_code, beat, district, sector, zip\n",
    "Lookup tables loaded separately: premise, weapon, bureau, division, unit, investigation_status, case_status\n",
    "\n",
    "Main table: modus_operandi\n",
    "Lookup tables: mo_item\n",
    "\n",
    "Main table: out_of_service\n",
    "Lookup tables: call_unit (update), os_code\n",
    "\n",
    "Main table: shift\n",
    "Lookup tables: call_unit (update), officer_name\n",
    "\n",
    "Other tables: squad (referenced by call_unit, but since we're constantly updating call_unit, won't load this till the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use dataset to stuff the data into postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the tables before touching the data so they have all the proper constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Database DDL\n",
    "\n",
    "Code to create the database schema is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/dialects/postgresql/base.py:2421: SAWarning: Did not recognize type 'geometry' of column 'point'\n",
      "  (attype, name))\n",
      "/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/dialects/postgresql/base.py:2421: SAWarning: Did not recognize type 'geometry' of column 'geom'\n",
      "  (attype, name))\n",
      "/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/dialects/postgresql/base.py:2421: SAWarning: Did not recognize type 'geometry' of column 'extent'\n",
      "  (attype, name))\n"
     ]
    }
   ],
   "source": [
    "# CHANGE CREDENTIALS AS APPROPRIATE\n",
    "#sqlalchemy_uri = 'postgresql://datascientist:1234thumbwar@freyja.rtp.rti.org:5432/cfs_v3'\n",
    "sqlalchemy_uri = 'postgresql://jnance:@localhost:5432/cfs'\n",
    "\n",
    "db = dataset.connect(sqlalchemy_uri)\n",
    "engine = create_engine(sqlalchemy_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_db():\n",
    "    \"\"\"\n",
    "    Remove and recreate tables to prepare for reloading the db\n",
    "    \"\"\"\n",
    "    db.query(\"DROP TABLE IF EXISTS note CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS note_author CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call_source CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call_unit CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS priority CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS city CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS beat CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS zip_code CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS district CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS sector CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS officer CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS shift CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS shift_unit CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call_log CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS transaction CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS close_code CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS ucr_descr CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS ucr_code CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS incident CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS incident_mo_item CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS mo_item CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS mo_group CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS bureau CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS case_status CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS division CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS unit CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS investigation_status CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS weapon CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS weapon_group CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS premise CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS premise_group CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS nature CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS oos_code CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS out_of_service CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS squad CASCADE;\")\n",
    "\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE ucr_code\n",
    "    (\n",
    "      ucr_code_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT ucr_code_pk PRIMARY KEY (ucr_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE ucr_descr\n",
    "    (\n",
    "      ucr_descr_id serial NOT NULL,\n",
    "      short_descr text,\n",
    "      long_descr text,\n",
    "      ucr_code_id int,\n",
    "      CONSTRAINT ucr_descr_pk PRIMARY KEY (ucr_descr_id),\n",
    "      CONSTRAINT ucr_code_ucr_descr_fk FOREIGN KEY (ucr_code_id) REFERENCES ucr_code (ucr_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE bureau\n",
    "    (\n",
    "      bureau_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT bureau_pk PRIMARY KEY (bureau_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE division\n",
    "    (\n",
    "      division_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT division_pk PRIMARY KEY (division_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE investigation_status\n",
    "    (\n",
    "      investigation_status_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT investigation_status_pk PRIMARY KEY (investigation_status_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE case_status\n",
    "    (\n",
    "      case_status_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT case_status_pk PRIMARY KEY (case_status_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE unit\n",
    "    (\n",
    "      unit_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT unit_pk PRIMARY KEY (unit_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE weapon_group\n",
    "    (\n",
    "      weapon_group_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT weapon_group_pk PRIMARY KEY (weapon_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE premise_group\n",
    "    (\n",
    "      premise_group_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT premise_group_pk PRIMARY KEY (premise_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE weapon\n",
    "    (\n",
    "      weapon_id serial NOT NULL,\n",
    "      descr text,\n",
    "      weapon_group_id int,\n",
    "      CONSTRAINT weapon_pk PRIMARY KEY (weapon_id),\n",
    "      CONSTRAINT weapon_group_weapon_fk FOREIGN KEY (weapon_group_id) REFERENCES weapon_group (weapon_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE premise\n",
    "    (\n",
    "      premise_id serial NOT NULL,\n",
    "      descr text,\n",
    "      premise_group_id int,\n",
    "      CONSTRAINT premise_pk PRIMARY KEY (premise_id),\n",
    "      CONSTRAINT premise_group_premise_fk FOREIGN KEY (premise_group_id) REFERENCES premise_group (premise_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE city\n",
    "    (\n",
    "      city_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT city_pk PRIMARY KEY (city_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE sector\n",
    "    (\n",
    "      sector_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT sector_pk PRIMARY KEY (sector_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE district\n",
    "    (\n",
    "      district_id serial NOT NULL,\n",
    "      sector_id int,\n",
    "      descr text,\n",
    "      CONSTRAINT district_pk PRIMARY KEY (district_id),\n",
    "      CONSTRAINT sector_district_fk FOREIGN KEY (sector_id) REFERENCES sector (sector_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE beat\n",
    "    (\n",
    "      beat_id serial NOT NULL,\n",
    "      district_id int,\n",
    "      sector_id int,\n",
    "      descr text,\n",
    "      CONSTRAINT beat_pk PRIMARY KEY (beat_id),\n",
    "      CONSTRAINT district_beat_fk FOREIGN KEY (district_id) REFERENCES district (district_id),\n",
    "      CONSTRAINT sector_beat_fk FOREIGN KEY (sector_id) REFERENCES sector (sector_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE zip_code\n",
    "    (\n",
    "      zip_code_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT zip_code_pk PRIMARY KEY (zip_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE incident\n",
    "    (\n",
    "      incident_id bigint NOT NULL,\n",
    "      case_id bigint UNIQUE,\n",
    "      time_filed timestamp without time zone,\n",
    "      month_filed int,\n",
    "      week_filed int,\n",
    "      year_filed int,\n",
    "      dow_filed int,\n",
    "      street_num int,\n",
    "      street_name text,\n",
    "      city_id int,\n",
    "      zip_code_id int,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      beat_id int,\n",
    "      district_id int,\n",
    "      sector_id int,\n",
    "      premise_id int,\n",
    "      weapon_id int,\n",
    "      domestic boolean,\n",
    "      juvenile boolean,\n",
    "      gang_related boolean,\n",
    "      emp_bureau_id int,\n",
    "      emp_division_id int,\n",
    "      emp_unit_id int,\n",
    "      num_officers int,\n",
    "      investigation_status_id int,\n",
    "      investigator_unit_id int,\n",
    "      case_status_id int,\n",
    "      ucr_descr_id int,\n",
    "      committed boolean,\n",
    "      \n",
    "      CONSTRAINT incident_pk PRIMARY KEY (incident_id),\n",
    "      \n",
    "      CONSTRAINT case_status_incident_fk\n",
    "        FOREIGN KEY (case_status_id) REFERENCES case_status (case_status_id),\n",
    "      CONSTRAINT bureau_incident_fk\n",
    "        FOREIGN KEY (emp_bureau_id) REFERENCES bureau (bureau_id),\n",
    "      CONSTRAINT division_incident_fk\n",
    "        FOREIGN KEY (emp_division_id) REFERENCES division (division_id),\n",
    "      CONSTRAINT unit_incident_emp_fk\n",
    "        FOREIGN KEY (emp_unit_id) REFERENCES unit (unit_id),\n",
    "      CONSTRAINT unit_incident_investigator_fk\n",
    "        FOREIGN KEY (investigator_unit_id) REFERENCES unit (unit_id),\n",
    "      CONSTRAINT investigation_status_incident_fk\n",
    "        FOREIGN KEY (investigation_status_id) REFERENCES investigation_status (investigation_status_id),\n",
    "      CONSTRAINT premise_incident_fk\n",
    "        FOREIGN KEY (premise_id) REFERENCES premise (premise_id),\n",
    "      CONSTRAINT weapon_incident_fk\n",
    "        FOREIGN KEY (weapon_id) REFERENCES weapon (weapon_id),\n",
    "      CONSTRAINT city_incident_fk\n",
    "        FOREIGN KEY (city_id) REFERENCES city (city_id),\n",
    "      CONSTRAINT ucr_descr_incident_fk\n",
    "        FOREIGN KEY (ucr_descr_id) REFERENCES ucr_descr (ucr_descr_id),\n",
    "      CONSTRAINT beat_incident_fk\n",
    "        FOREIGN KEY (beat_id) REFERENCES beat (beat_id),\n",
    "      CONSTRAINT district_incident_fk\n",
    "        FOREIGN KEY (district_id) REFERENCES district (district_id),\n",
    "      CONSTRAINT sector_incident_fk\n",
    "        FOREIGN KEY (sector_id) REFERENCES sector (sector_id),\n",
    "      CONSTRAINT zip_code_incident_fk\n",
    "        FOREIGN KEY (zip_code_id) REFERENCES zip_code (zip_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE mo_group\n",
    "    (\n",
    "      mo_group_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT mo_group_pk PRIMARY KEY (mo_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE mo_item\n",
    "    (\n",
    "      mo_item_id serial NOT NULL,\n",
    "      descr text,\n",
    "      mo_group_id int NOT NULL,\n",
    "      CONSTRAINT mo_item_pk PRIMARY KEY (mo_item_id),\n",
    "      CONSTRAINT mo_group_mo_item_fk FOREIGN KEY (mo_group_id) REFERENCES mo_group (mo_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE incident_mo_item\n",
    "    (\n",
    "      incident_mo_item_id bigint NOT NULL,\n",
    "      incident_id bigint,\n",
    "      mo_item_id int,\n",
    "      \n",
    "      CONSTRAINT incident_mo_item_pk PRIMARY KEY (incident_mo_item_id),\n",
    "      \n",
    "      CONSTRAINT incident_incident_mo_item_fk FOREIGN KEY (incident_id) REFERENCES incident (incident_id),\n",
    "      CONSTRAINT mo_item_incident_mo_item_fk FOREIGN KEY (mo_item_id) REFERENCES mo_item (mo_item_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call_source\n",
    "    (\n",
    "      call_source_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT call_source_pk PRIMARY KEY (call_source_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE squad\n",
    "    (\n",
    "      squad_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT squad_pk PRIMARY KEY (squad_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call_unit\n",
    "    (\n",
    "      call_unit_id serial NOT NULL,\n",
    "      descr text,\n",
    "      squad_id int,\n",
    "      CONSTRAINT call_unit_pk PRIMARY KEY (call_unit_id),\n",
    "      CONSTRAINT squad_call_unit_fk FOREIGN KEY (squad_id) REFERENCES squad (squad_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE close_code\n",
    "    (\n",
    "      close_code_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT close_code_pk PRIMARY KEY (close_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE nature\n",
    "    (\n",
    "      nature_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT nature_pk PRIMARY KEY (nature_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE priority\n",
    "    (\n",
    "      priority_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT priority_pk PRIMARY KEY (priority_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call\n",
    "    (\n",
    "      call_id bigint NOT NULL,\n",
    "      year_received int,\n",
    "      month_received int,\n",
    "      week_received int,\n",
    "      dow_received int,\n",
    "      hour_received int,\n",
    "      case_id bigint,\n",
    "      call_source_id int,\n",
    "      primary_unit_id int,\n",
    "      first_dispatched_id int,\n",
    "      reporting_unit_id int,\n",
    "      street_num int,\n",
    "      street_name text,\n",
    "      city_id int,\n",
    "      zip_code_id int,\n",
    "      crossroad1 text,\n",
    "      crossroad2 text,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      beat_id int,\n",
    "      district_id int,\n",
    "      sector_id int,\n",
    "      business text,\n",
    "      nature_id int,\n",
    "      priority_id int,\n",
    "      report_only boolean,\n",
    "      cancelled boolean,\n",
    "      time_received timestamp without time zone,\n",
    "      time_routed timestamp without time zone,\n",
    "      time_finished timestamp without time zone,\n",
    "      first_unit_dispatch timestamp without time zone,\n",
    "      first_unit_enroute timestamp without time zone,\n",
    "      first_unit_arrive timestamp without time zone,\n",
    "      first_unit_transport timestamp without time zone,\n",
    "      last_unit_clear timestamp without time zone,\n",
    "      time_closed timestamp without time zone,\n",
    "      overall_response_time interval,\n",
    "      officer_response_time interval,\n",
    "      close_code_id int,\n",
    "      close_comments text,\n",
    "      \n",
    "      CONSTRAINT call_pk PRIMARY KEY (call_id),\n",
    "      \n",
    "      CONSTRAINT call_source_call_fk\n",
    "        FOREIGN KEY (call_source_id) REFERENCES call_source (call_source_id),\n",
    "      CONSTRAINT call_unit_call_primary_unit_fk\n",
    "        FOREIGN KEY (primary_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT call_unit_call_first_dispatched_fk\n",
    "        FOREIGN KEY (first_dispatched_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT call_unit_call_reporting_unit_fk\n",
    "        FOREIGN KEY (reporting_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT city_call_fk\n",
    "        FOREIGN KEY (city_id) REFERENCES city (city_id),\n",
    "      CONSTRAINT close_code_call_fk\n",
    "        FOREIGN KEY (close_code_id) REFERENCES close_code (close_code_id),\n",
    "      --There is some mismatch here that might be valid; no constraint for now\n",
    "      --CONSTRAINT incident_call_fk\n",
    "      --  FOREIGN KEY (case_id) REFERENCES incident (case_id),\n",
    "      CONSTRAINT nature_call_fk\n",
    "        FOREIGN KEY (nature_id) REFERENCES nature (nature_id),\n",
    "      CONSTRAINT beat_call_fk\n",
    "        FOREIGN KEY (beat_id) REFERENCES beat (beat_id),\n",
    "      CONSTRAINT district_call_fk\n",
    "        FOREIGN KEY (district_id) REFERENCES district (district_id),\n",
    "      CONSTRAINT sector_call_fk\n",
    "        FOREIGN KEY (sector_id) REFERENCES sector (sector_id),\n",
    "      CONSTRAINT priority_call_fk\n",
    "        FOREIGN KEY (priority_id) REFERENCES priority (priority_id),\n",
    "      CONSTRAINT zip_code_call_fk\n",
    "        FOREIGN KEY (zip_code_id) REFERENCES zip_code (zip_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE note_author\n",
    "    (\n",
    "      note_author_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT note_author_pk PRIMARY KEY (note_author_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE note\n",
    "    (\n",
    "      note_id serial NOT NULL,\n",
    "      body text,\n",
    "      time_recorded timestamp without time zone,\n",
    "      note_author_id int,\n",
    "      call_id bigint,\n",
    "      CONSTRAINT note_pk PRIMARY KEY (note_id),\n",
    "      \n",
    "      CONSTRAINT call_note_fk FOREIGN KEY (call_id) REFERENCES call (call_id),\n",
    "      CONSTRAINT note_author_note_fk FOREIGN KEY (note_author_id) REFERENCES note_author (note_author_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE officer\n",
    "    (\n",
    "        officer_id bigint NOT NULL,\n",
    "        name text,\n",
    "        name_aka text,\n",
    "        \n",
    "        CONSTRAINT officer_pk PRIMARY KEY (officer_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE shift\n",
    "    (\n",
    "      shift_id bigint NOT NULL,\n",
    "      \n",
    "      CONSTRAINT shift_pk PRIMARY KEY (shift_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE shift_unit\n",
    "    (\n",
    "        shift_unit_id bigint NOT NULL,\n",
    "        call_unit_id int,\n",
    "        officer_id int,\n",
    "        in_time timestamp without time zone,\n",
    "        out_time timestamp without time zone,\n",
    "        bureau_id int,\n",
    "        division_id int,\n",
    "        unit_id int,\n",
    "        shift_id bigint,\n",
    "        \n",
    "        CONSTRAINT shift_unit_pk PRIMARY KEY (shift_unit_id),\n",
    "        CONSTRAINT shift_shift_unit_fk FOREIGN KEY (shift_id) REFERENCES shift (shift_id),\n",
    "        CONSTRAINT call_unit_shift_unit_fk FOREIGN KEY (call_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "        CONSTRAINT officer_shift_unit_fk FOREIGN KEY (officer_id) REFERENCES officer (officer_id),\n",
    "        CONSTRAINT bureau_shift_unit_fk FOREIGN KEY (bureau_id) REFERENCES bureau (bureau_id),\n",
    "        CONSTRAINT division_shift_unit_fk FOREIGN KEY (division_id) REFERENCES division (division_id),\n",
    "        CONSTRAINT unit_shift_unit_fk FOREIGN KEY (unit_id) REFERENCES unit (unit_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE transaction\n",
    "    (\n",
    "      transaction_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT transaction_pk PRIMARY KEY (transaction_id)\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call_log\n",
    "    (\n",
    "      call_log_id bigint NOT NULL,\n",
    "      transaction_id int,\n",
    "      shift_id bigint,\n",
    "      time_recorded timestamp without time zone,\n",
    "      call_id bigint,\n",
    "      call_unit_id int,\n",
    "      close_code_id int,\n",
    "      \n",
    "      CONSTRAINT call_log_pk PRIMARY KEY (call_log_id),\n",
    "      \n",
    "      CONSTRAINT shift_call_log_fk FOREIGN KEY (shift_id) REFERENCES shift (shift_id),\n",
    "      CONSTRAINT call_unit_call_log_fk FOREIGN KEY (call_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT call_call_log_fk FOREIGN KEY (call_id) REFERENCES call (call_id),\n",
    "      CONSTRAINT close_code_call_log_fk FOREIGN KEY (close_code_id) REFERENCES close_code (close_code_id),\n",
    "      CONSTRAINT transaction_call_log_fk FOREIGN KEY (transaction_id) REFERENCES transaction (transaction_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE oos_code\n",
    "    (\n",
    "      oos_code_id serial NOT NULL,\n",
    "      descr text,\n",
    "      \n",
    "      CONSTRAINT oos_code_pk PRIMARY KEY (oos_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE out_of_service\n",
    "    (\n",
    "      oos_id bigint NOT NULL,\n",
    "      call_unit_id int,\n",
    "      shift_id bigint,\n",
    "      oos_code_id int,\n",
    "      location text,\n",
    "      comments text,\n",
    "      start_time timestamp without time zone,\n",
    "      end_time timestamp without time zone,\n",
    "      duration interval,\n",
    "      \n",
    "      CONSTRAINT oos_pk PRIMARY KEY (oos_id),\n",
    "      \n",
    "      CONSTRAINT shift_out_of_service_fk FOREIGN KEY (shift_id) REFERENCES shift (shift_id),\n",
    "      CONSTRAINT call_unit_oos_fk FOREIGN KEY (call_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT oos_code_oos_fk FOREIGN KEY (oos_code_id) REFERENCES oos_code (oos_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "      \n",
    "    \n",
    "reset_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Small lookup tables\n",
    "case_status, division, unit, bureau, investigation_status, call_source, and close_code\n",
    "\n",
    "The nested lookup tables are weapon/weapon_group and premise/premise_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading LWMAIN.CSSTATUS.csv into case_status\n",
      "loading LWMAIN.EMDIVISION.csv into division\n",
      "loading LWMAIN.EMSECTION.csv into unit\n",
      "loading LWMAIN.EMUNIT.csv into bureau\n",
      "loading LWMAIN.INVSTSTATS.csv into investigation_status\n",
      "loading inmain.callsource.tsv into call_source\n",
      "loading inmain.closecode.tsv into close_code\n",
      "loading outserv.oscode.tsv into oos_code\n"
     ]
    }
   ],
   "source": [
    "# There are a million of these, so let's make life easier and reuse all that code\n",
    "\n",
    "# We need to save the mapping between DPD's short codes and our database ids so we can apply it to the records\n",
    "# in the main tables\n",
    "#\n",
    "# These have the DPD's codes as keys and our internal database PKs as values\n",
    "case_status_code_mapping = {}\n",
    "division_code_mapping = {}\n",
    "unit_code_mapping = {}\n",
    "bureau_code_mapping = {}\n",
    "investigation_status_code_mapping = {}\n",
    "call_source_code_mapping = {}\n",
    "close_code_mapping = {}\n",
    "oos_code_mapping = {}\n",
    "\n",
    "lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.CSSTATUS.csv\",\n",
    "        \"table\": \"case_status\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": case_status_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMDIVISION.csv\",\n",
    "        \"table\": \"division\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": division_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMSECTION.csv\",\n",
    "        \"table\": \"unit\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": unit_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMUNIT.csv\",\n",
    "        \"table\": \"bureau\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": bureau_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.INVSTSTATS.csv\",\n",
    "        \"table\": \"investigation_status\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": investigation_status_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"inmain.callsource.tsv\",\n",
    "        \"table\": \"call_source\",\n",
    "        \"mapping\": {\"Description\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": call_source_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"inmain.closecode.tsv\",\n",
    "        \"table\": \"close_code\",\n",
    "        \"mapping\": {\"Description\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": close_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"outserv.oscode.tsv\",\n",
    "        \"table\": \"oos_code\",\n",
    "        \"mapping\": {\"Description\": \"descr\"},\n",
    "        \"code_column\": \"Code\",\n",
    "        \"code_mapping\": oos_code_mapping\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in lookup_jobs:\n",
    "    print(\"loading %s into %s\" % (job['file'], job['table']))\n",
    "    \n",
    "    if job['file'].endswith(\".csv\"):\n",
    "        data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    elif job['file'].endswith(\".tsv\"):\n",
    "        data = pd.read_csv(\"../csv_data/%s\" % (job['file']), sep='\\t')\n",
    "    \n",
    "    # Keep track of the ids, as the data is ordered, so these will be the same assigned by the incrementing\n",
    "    # primary key in the database.\n",
    "    id_ = 1    \n",
    "    for (i,row) in data.iterrows():\n",
    "        job['code_mapping'][row[job['code_column']]] = id_\n",
    "        id_ += 1\n",
    "\n",
    "    # Keep only the desired columns\n",
    "    keep_columns = set(job['mapping'].keys())\n",
    "    for c in data.columns:\n",
    "        if c not in keep_columns:\n",
    "            data = data.drop(c, axis=1)\n",
    "            \n",
    "    # Change the column names to the ones we want and insert the data\n",
    "    data.rename(columns=job['mapping'], inplace=True)\n",
    "    data.to_sql(job['table'], engine, index=False, if_exists='append')\n",
    "    \n",
    "# They neglected to give us this code which is frequently in the database\n",
    "investigation_status_code_mapping['CBA'] = None\n",
    "\n",
    "# Some more that are in the db but not the lookup table they gave us\n",
    "for bogus_code in ('15:13.0', 'SLFIN', 'EYE', 'WALK', '911', 'A'):\n",
    "    call_source_code_mapping[bogus_code] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading LWMAIN.PREMISE.csv into premise and premise_group\n",
      "loading LWMAIN.WEAPON.csv into weapon and weapon_group\n"
     ]
    }
   ],
   "source": [
    "#These have to create \"nested\" tables and are a little tougher, but we can still reuse the code\n",
    "\n",
    "# Still need to keep track of the mappings\n",
    "weapon_code_mapping = {}\n",
    "premise_code_mapping = {}\n",
    "\n",
    "nested_lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.PREMISE.csv\",\n",
    "        \"outer_table\": \"premise\",\n",
    "        \"inner_table\": \"premise_group\",\n",
    "        \"outer_cols\": [\"premise_group_id\",\"descr\"],\n",
    "        \"inner_col\": \"descr\",\n",
    "        \"inner_id\": \"premise_group_id\",\n",
    "        \"code_mapping\": premise_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.WEAPON.csv\",\n",
    "        \"outer_table\": \"weapon\",\n",
    "        \"inner_table\": \"weapon_group\",\n",
    "        \"outer_cols\": [\"weapon_group_id\",\"descr\"],\n",
    "        \"inner_col\": \"descr\",\n",
    "        \"inner_id\": \"weapon_group_id\",\n",
    "        \"code_mapping\": weapon_code_mapping\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in nested_lookup_jobs:\n",
    "    print(\"loading %s into %s and %s\" % (job['file'], job['outer_table'], job['inner_table']))\n",
    "    data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    \n",
    "    # load the group table by getting all the unique groups\n",
    "    inner_data = data['descriptn_a'].drop_duplicates()\n",
    "    inner_data.name = job['inner_col']\n",
    "    inner_data.to_sql(job['inner_table'], engine, index=False, if_exists='append')\n",
    "    \n",
    "    # Learn the mapping between groups and group_ids in the database so we can insert the proper\n",
    "    # group_ids with the outer tables\n",
    "    groups = {}\n",
    "    for row in db.query(\"SELECT * FROM %s\" % (job['inner_table'])):\n",
    "        groups[row[job['inner_col']]] = row[job['inner_id']]\n",
    "       \n",
    "    # Figure out what the database ids will be, so we can convert DPD's columns to the database ids in the\n",
    "    # main table load\n",
    "    id_ = 1\n",
    "    for (i,row) in data.iterrows():\n",
    "        job['code_mapping'][row['code_agcy']] = id_\n",
    "        id_ += 1\n",
    "    \n",
    "    # Concatenate and rename the series we want\n",
    "    outer_data = pd.concat([data['descriptn_a'], data['descriptn_b']], axis=1, keys=job['outer_cols'])\n",
    "    \n",
    "    # use the groups mapping to turn group names into ids from our database\n",
    "    outer_data[job['inner_id']] = outer_data[job['inner_id']].map(lambda x: groups[x])\n",
    "    \n",
    "    # Store the records\n",
    "    outer_data.to_sql(job['outer_table'], engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmain.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lookup tables\n",
    "city, beat, district, sector, and ucr_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "chunksize=20000\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "import csv\n",
    "\n",
    "cities = set()\n",
    "beats = set()\n",
    "districts = set()\n",
    "sectors = set()\n",
    "zip_codes = set()\n",
    "ucr_descr_pairs = {}\n",
    "\n",
    "with open('../csv_data/cfs_2014_lwmain.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "    reader = csv.reader(f)\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        if first_row:\n",
    "            header = row\n",
    "            first_row = False\n",
    "            continue\n",
    "            \n",
    "        # Strip whitespace and convert empty strings to None\n",
    "        row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "        if row[9]:\n",
    "            cities.add(row[9])\n",
    "        if row[10]:\n",
    "            zip_codes.add(row[10])\n",
    "        if row[13]:\n",
    "            beats.add(row[13])\n",
    "        if row[14]:\n",
    "            districts.add(row[14])\n",
    "        if row[15]:\n",
    "            sectors.add(row[15])\n",
    "            \n",
    "        #ucr_code: 30, ucr_short_descr: 31, ucr_long_descr: 32\n",
    "        if (row[31],row[32]) not in ucr_descr_pairs:\n",
    "            ucr_descr_pairs[(row[31],row[32])] = row[30]\n",
    "\n",
    "try:\n",
    "    for set_, table in ((cities, 'city'),\n",
    "                        (beats, 'beat'),\n",
    "                        (districts, 'district'),\n",
    "                        (sectors, 'sector'),\n",
    "                        (zip_codes, 'zip_code')):\n",
    "    \n",
    "        table_ref = db[table]\n",
    "        db.begin()\n",
    "        for s in set_:\n",
    "            table_ref.insert({'descr': s})\n",
    "        db.commit()\n",
    "    \n",
    "    ucr_code = db['ucr_code']\n",
    "    for u in set(ucr_descr_pairs.values()):\n",
    "        ucr_code.insert({'descr': u})\n",
    "    \n",
    "    ucr_code_mapping = {}\n",
    "    \n",
    "    for row in db.query(\"SELECT * FROM ucr_code;\"):\n",
    "        ucr_code_mapping[row['descr']] = row['ucr_code_id']\n",
    "        \n",
    "    ucr_descr = db['ucr_descr']\n",
    "    for pair in ucr_descr_pairs.keys():\n",
    "        ucr_descr.insert({'short_descr': pair[0], 'long_descr': pair[1],\n",
    "                          'ucr_code_id': ucr_code_mapping[ucr_descr_pairs[pair]] })\n",
    "    \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "\n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main table\n",
    "incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 seconds: completed 10000 rows\n",
      "29 seconds: completed 20000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv    \n",
    "\n",
    "def combine_date_time(str_date, str_time):\n",
    "    date = dt.datetime.strptime(str_date, \"%m/%d/%y\")\n",
    "    time = dt.datetime.strptime(str_time, \"%I:%M %p\")\n",
    "    return dt.datetime(date.year, date.month, date.day, time.hour, time.minute)\n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else d\n",
    "\n",
    "# We have several columns we need to convert to int that can also be None\n",
    "def safe_int(x):\n",
    "    return int(x) if x else None\n",
    "\n",
    "att_com_mapping = {\n",
    "    'COM': True,\n",
    "    'ATT': False,\n",
    "    '': None\n",
    "}\n",
    "\n",
    "city_code_mapping = {}\n",
    "beat_code_mapping = {}\n",
    "district_code_mapping = {}\n",
    "sector_code_mapping = {}\n",
    "zip_code_mapping = {}\n",
    "ucr_descr_code_mapping = {}\n",
    "\n",
    "# Populate the mappings from the database\n",
    "for row in db.query(\"SELECT * FROM city;\"):\n",
    "    city_code_mapping[row['descr']] = row['city_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM beat;\"):\n",
    "    beat_code_mapping[row['descr']] = row['beat_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM district;\"):\n",
    "    district_code_mapping[row['descr']] = row['district_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM sector;\"):\n",
    "    sector_code_mapping[row['descr']] = row['sector_id']\n",
    "\n",
    "for row in db.query(\"SELECT * FROM zip_code;\"):\n",
    "    zip_code_mapping[row['descr']] = row['zip_code_id']\n",
    "\n",
    "# the pairs of short/long_descr are unique, so that needs to be our key\n",
    "for row in db.query(\"SELECT * FROM ucr_descr;\"):\n",
    "    ucr_descr_code_mapping[(row['short_descr'], row['long_descr'])] = row['ucr_descr_id']\n",
    "\n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "incident = db['incident']\n",
    "db.begin()\n",
    "\n",
    "try:\n",
    "    with open('../csv_data/cfs_2014_lwmain.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "            \n",
    "            #for i in range(len(header)):\n",
    "            #    print(i, header[i])\n",
    "            \n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "            time_filed = combine_date_time(row[2], row[3])\n",
    "            db_row = {\n",
    "                'incident_id': safe_int(row[0]),\n",
    "                'case_id': safe_int(row[1]),\n",
    "                'time_filed': time_filed,\n",
    "                'year_filed': time_filed.year,\n",
    "                'month_filed': time_filed.month,\n",
    "                'week_filed': time_filed.isocalendar()[1],\n",
    "                'dow_filed': time_filed.weekday(),\n",
    "                'street_num': row[7],\n",
    "                'street_name': row[8],\n",
    "                'city_id': safe_map(city_code_mapping, row[9]),\n",
    "                'zip_code_id': safe_map(zip_code_mapping, row[10]),\n",
    "                'geox': row[11],\n",
    "                'geoy': row[12],\n",
    "                'beat_id': safe_map(beat_code_mapping, row[13]),\n",
    "                'district_id': safe_map(district_code_mapping, row[14]),\n",
    "                'sector_id': safe_map(sector_code_mapping, row[15]),\n",
    "                'premise_id': safe_map(premise_code_mapping, safe_int(row[16])),\n",
    "                'weapon_id': safe_map(weapon_code_mapping, safe_int(row[17])),\n",
    "                'domestic': True if row[18]=='Y' else False if row[18]=='N' else None,\n",
    "                'juvenile': True if row[19]=='Y' else False if row[19]=='N' else None,\n",
    "                'gang_related': True if row[20]=='YES' else False if row[20]=='NO' else None,\n",
    "                'emp_bureau_id': safe_map(bureau_code_mapping, row[21]),\n",
    "                'emp_division_id': safe_map(division_code_mapping, row[22]),\n",
    "                'emp_unit_id': safe_map(unit_code_mapping, row[23]),\n",
    "                'num_officers': (lambda x: None if x in ('',None) else safe_int(x))(row[24]),\n",
    "                'investigation_status_id': safe_map(investigation_status_code_mapping,row[25]),\n",
    "                'investigator_unit_id': safe_map(unit_code_mapping, row[26]),\n",
    "                'case_status_id': safe_map(case_status_code_mapping, safe_int(row[27])),\n",
    "                'ucr_descr_id': safe_map(ucr_descr_code_mapping, (row[31],row[32])),\n",
    "                'committed': safe_map(att_com_mapping, row[33])\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # have to insert one by one to properly handle the duplicate PKs in the data\n",
    "                db.query(\"SAVEPOINT integrity_checkpoint;\")\n",
    "                incident.insert(db_row, ensure=False) # we know the right columns are already there\n",
    "            except IntegrityError:\n",
    "                #ignore the duplicate pks; the lower chrgid comes first, so we already have the record we want\n",
    "                #postgres complains if we keep inserting records into an aborted transaction\n",
    "                db.query(\"ROLLBACK TO SAVEPOINT integrity_checkpoint;\")\n",
    "                \n",
    "            db.query(\"RELEASE SAVEPOINT integrity_checkpoint;\")\n",
    "            \n",
    "            j+=1\n",
    "            if j % 10000 == 0:\n",
    "                print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))\n",
    "            \n",
    "            \n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmodop.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lookup tables\n",
    "mo_group and mo_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "mo_items_pairs = []\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('../csv_data/cfs_2014_lwmodop.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        if first_row:\n",
    "            header = row\n",
    "            first_row = False\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Strip whitespace and convert empty strings to None\n",
    "        row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "        if (row[5], row[3]) not in mo_items_pairs:\n",
    "            # (mo_item, mo_group)\n",
    "            mo_items_pairs.append((row[5], row[3]))\n",
    "            \n",
    "try:\n",
    "    mo_group = db['mo_group']\n",
    "    db.begin()\n",
    "    for g in set([p[1] for p in mo_items_pairs]):\n",
    "        mo_group.insert({'descr': g})\n",
    "    \n",
    "    mo_group_mapping = {}\n",
    "    for row in db.query(\"SELECT * FROM mo_group;\"):\n",
    "        mo_group_mapping[row['descr']] = row['mo_group_id']\n",
    "        \n",
    "    mo_item = db['mo_item']\n",
    "    for i, g in mo_items_pairs:\n",
    "        mo_item.insert({'descr': i, 'mo_group_id': mo_group_mapping[g] })\n",
    "        \n",
    "    db.commit()\n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "    \n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main table\n",
    "incident_mo_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 seconds: completed 10000 rows\n",
      "7 seconds: completed 20000 rows\n",
      "11 seconds: completed 30000 rows\n",
      "15 seconds: completed 40000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv    \n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else d\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "# Populate the mappings from the database\n",
    "mo_item_code_mapping = {}\n",
    "\n",
    "# the pairs of group/item_descr are unique, so that needs to be our key\n",
    "for row in db.query(\"SELECT * FROM mo_item;\"):\n",
    "    mo_item_code_mapping[(row['descr'])] = row['mo_item_id']\n",
    "\n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "mo = db['incident_mo_item']\n",
    "db_rows = []\n",
    "db.begin()\n",
    "\n",
    "try:\n",
    "    with open('../csv_data/cfs_2014_lwmodop.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "            \n",
    "            #for i in range(len(header)):\n",
    "            #    print(i, header[i])\n",
    "            \n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "            db_row = {\n",
    "                'incident_id': row[0],\n",
    "                'incident_mo_item_id': row[1],\n",
    "                'mo_item_id': safe_map(mo_item_code_mapping, row[5])\n",
    "            }\n",
    "            \n",
    "            # have to insert one by one to properly handle the duplicate PKs in the data\n",
    "            mo.insert(db_row, ensure=False) # we know the right columns are already there\n",
    "            \n",
    "            j+=1\n",
    "            if j % 10000 == 0:\n",
    "                print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))    \n",
    "            \n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_inmain.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lookup tables\n",
    "We did close_code and call_source earlier with the other tables that come directly from a .csv file.\n",
    "\n",
    "We'll need to do a full load of nature and call_unit, and we need to update city with the new cities in this file even though we loaded it previously.  Also need to do a pass through all the notes to get the note_authors.\n",
    "\n",
    "Also update beat, district, sector, and zip_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "timestamp_expr = re.compile(\"(.*?)\\[(\\d{2}/\\d{2}/(?:\\d{2}|\\d{4}) \\d{2}:\\d{2}:\\d{2}) (.*?)\\]\")\n",
    "\n",
    "def split_notes(notes):\n",
    "    \"\"\"\n",
    "    Return a list of tuples.  Each tuple represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    tuples = []\n",
    "    regex_split = re.findall(timestamp_expr, notes)\n",
    "    for tup in regex_split:\n",
    "        text = tup[0].split()\n",
    "        text = text if text else None  # turn blanks into null\n",
    "        try:\n",
    "            timestamp = dt.datetime.strptime(tup[1], \"%m/%d/%y %H:%M:%S\")\n",
    "        except ValueError: # 4 digit year\n",
    "            timestamp = dt.datetime.strptime(tup[1], \"%m/%d/%Y %H:%M:%S\")\n",
    "        author = tup[2] if tup[2] else None\n",
    "        tuples.append((text, timestamp, author))\n",
    "    return tuples\n",
    "    \n",
    "natures = set()\n",
    "call_units = set()\n",
    "priorities = set()\n",
    "note_authors = set()\n",
    "\n",
    "cities = set()\n",
    "db_cities = set()\n",
    "# we already have most of the cities/beats/districts/sectors from the incident data, but there are more\n",
    "for row in db.query(\"SELECT * FROM city;\"):\n",
    "    db_cities.add(row['descr'])\n",
    "    \n",
    "beats = set()\n",
    "db_beats = set()\n",
    "for row in db.query(\"SELECT * FROM beat;\"):\n",
    "    db_beats.add(row['descr'])\n",
    "    \n",
    "districts = set()\n",
    "db_districts = set()\n",
    "for row in db.query(\"SELECT * FROM district;\"):\n",
    "    db_districts.add(row['descr'])\n",
    "    \n",
    "sectors = set()\n",
    "db_sectors = set()\n",
    "for row in db.query(\"SELECT * FROM sector;\"):\n",
    "    db_sectors.add(row['descr'])\n",
    "    \n",
    "zip_codes = set()\n",
    "db_zip_codes = set()\n",
    "for row in db.query(\"SELECT * FROM zip_code;\"):\n",
    "    db_zip_codes.add(row['descr'])\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('../csv_data/cfs_2014_inmain.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "    reader = csv.reader(f)\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        if first_row:\n",
    "            header = row\n",
    "            first_row = False\n",
    "            continue\n",
    "            \n",
    "        # Strip whitespace and convert empty strings to None\n",
    "        row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "        if row[23]:\n",
    "            natures.add(row[23])\n",
    "        for d in (row[5], row[6], row[50]): # these all reference the call_unit table\n",
    "            call_units.add(d)\n",
    "        if row[27]:\n",
    "            for note in split_notes(row[27]):\n",
    "                if note[2] is not None:\n",
    "                    note_authors.add(note[2])\n",
    "        if row[10] and row[10] not in db_cities:\n",
    "            cities.add(row[10])\n",
    "            \n",
    "        if row[11] and row[11] not in db_zip_codes:\n",
    "            zip_codes.add(row[11])\n",
    "            \n",
    "        if row[18] and row[18] not in db_beats:\n",
    "            beats.add(row[18])\n",
    "            \n",
    "        if row[19] and row[19] not in db_districts:\n",
    "            districts.add(row[19])\n",
    "            \n",
    "        if row[20] and row[20] not in db_sectors:\n",
    "            sectors.add(row[20])\n",
    "        \n",
    "        if row[24]:\n",
    "            priorities.add(row[24])\n",
    "            \n",
    "try:\n",
    "    for set_, table in ((natures, 'nature'),\n",
    "                        (priorities, 'priority'),\n",
    "                        (call_units, 'call_unit'),\n",
    "                        (note_authors, 'note_author'),\n",
    "                        (cities, 'city'),\n",
    "                        (beats, 'beat'),\n",
    "                        (districts, 'district'),\n",
    "                        (sectors, 'sector'),\n",
    "                        (zip_codes, 'zip_code')):\n",
    "        table_ref = db[table]\n",
    "        db.begin()\n",
    "        for s in set_:\n",
    "            table_ref.insert({'descr': s})\n",
    "        db.commit()\n",
    "    \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "    \n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main table\n",
    "call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 seconds: completed 10000 rows\n",
      "43 seconds: completed 20000 rows\n",
      "64 seconds: completed 30000 rows\n",
      "86 seconds: completed 40000 rows\n",
      "107 seconds: completed 50000 rows\n",
      "129 seconds: completed 60000 rows\n",
      "151 seconds: completed 70000 rows\n",
      "173 seconds: completed 80000 rows\n",
      "195 seconds: completed 90000 rows\n",
      "217 seconds: completed 100000 rows\n",
      "239 seconds: completed 110000 rows\n",
      "262 seconds: completed 120000 rows\n",
      "284 seconds: completed 130000 rows\n",
      "307 seconds: completed 140000 rows\n",
      "330 seconds: completed 150000 rows\n",
      "353 seconds: completed 160000 rows\n",
      "376 seconds: completed 170000 rows\n",
      "399 seconds: completed 180000 rows\n",
      "421 seconds: completed 190000 rows\n",
      "444 seconds: completed 200000 rows\n",
      "467 seconds: completed 210000 rows\n",
      "489 seconds: completed 220000 rows\n",
      "512 seconds: completed 230000 rows\n",
      "534 seconds: completed 240000 rows\n",
      "556 seconds: completed 250000 rows\n",
      "579 seconds: completed 260000 rows\n",
      "601 seconds: completed 270000 rows\n",
      "623 seconds: completed 280000 rows\n",
      "646 seconds: completed 290000 rows\n",
      "668 seconds: completed 300000 rows\n",
      "690 seconds: completed 310000 rows\n",
      "712 seconds: completed 320000 rows\n",
      "734 seconds: completed 330000 rows\n",
      "756 seconds: completed 340000 rows\n",
      "778 seconds: completed 350000 rows\n",
      "800 seconds: completed 360000 rows\n",
      "822 seconds: completed 370000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "timestamp_expr = re.compile(\"(.*?)\\[(\\d{2}/\\d{2}/(?:\\d{2}|\\d{4}) \\d{2}:\\d{2}:\\d{2}) (.*?)\\]\")\n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else None\n",
    "\n",
    "# We have several columns we need to convert to int that can also be None\n",
    "def safe_int(x):\n",
    "    return int(x) if x else None\n",
    "\n",
    "def safe_float(x):\n",
    "    return float(x) if x else None\n",
    "\n",
    "def safe_bool(x):\n",
    "    return True if x == '1' else False if x == '0' else None\n",
    "\n",
    "def safe_datetime(x):\n",
    "    # to_datetime returns a pandas Timestamp object, and we want a vanilla datetime\n",
    "    return pd.to_datetime(x).to_datetime() if x not in ('NULL', None) else None\n",
    "\n",
    "def clean_case_id(c):\n",
    "    if c:\n",
    "        c = str(c).replace('-','').replace(' ','')\n",
    "        try:\n",
    "            return int(c)\n",
    "        except ValueError: #got some weird rows with non-digits in the case_id that def. won't map back to incident\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def split_notes(notes):\n",
    "    \"\"\"\n",
    "    Return a list of tuples.  Each tuple represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    tuples = []\n",
    "    if notes is None:\n",
    "        return []\n",
    "    regex_split = re.findall(timestamp_expr, notes)\n",
    "    for tup in regex_split:\n",
    "        text = tup[0].strip()\n",
    "        text = text if text else None  # turn blanks into null\n",
    "        try:\n",
    "            timestamp = dt.datetime.strptime(tup[1], \"%m/%d/%y %H:%M:%S\")\n",
    "        except ValueError: # 4 digit year\n",
    "            timestamp = dt.datetime.strptime(tup[1], \"%m/%d/%Y %H:%M:%S\")\n",
    "        author = tup[2]\n",
    "        tuples.append((text, timestamp, author))\n",
    "    return tuples\n",
    "\n",
    "nature_code_mapping = {}\n",
    "call_unit_code_mapping = {}\n",
    "note_author_mapping = {}\n",
    "city_code_mapping = {}\n",
    "beat_mapping = {}\n",
    "district_mapping = {}\n",
    "sector_mapping = {}\n",
    "zip_code_mapping = {}\n",
    "priority_mapping = {}\n",
    "\n",
    "# Populate the mappings from the database\n",
    "for row in db.query(\"SELECT * FROM nature;\"):\n",
    "    nature_code_mapping[row['descr']] = row['nature_id']\n",
    "\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    call_unit_code_mapping[row['descr']] = row['call_unit_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM note_author;\"):\n",
    "    note_author_mapping[row['descr']] = row['note_author_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM city;\"):\n",
    "    city_code_mapping[row['descr']] = row['city_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM beat;\"):\n",
    "    beat_mapping[row['descr']] = row['beat_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM district;\"):\n",
    "    district_mapping[row['descr']] = row['district_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM sector;\"):\n",
    "    sector_mapping[row['descr']] = row['sector_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM zip_code;\"):\n",
    "    zip_code_mapping[row['descr']] = row['zip_code_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM priority;\"):\n",
    "    priority_mapping[row['descr']] = row['priority_id']\n",
    "\n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "note_authors_set = set()\n",
    "\n",
    "call_rows = []\n",
    "note_rows = []\n",
    "call = db['call']\n",
    "note = db['note']\n",
    "db.begin()\n",
    "\n",
    "try:\n",
    "    with open('../csv_data/cfs_2014_inmain.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "            \n",
    "            #for i in range(len(header)):\n",
    "            #    print(i, header[i])\n",
    "            \n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "            time_received = safe_datetime(row[1])  # we need a datetime, not a pandas timestamp\n",
    "            first_unit_dispatch = safe_datetime(row[32])\n",
    "            first_unit_arrive = safe_datetime(row[39])\n",
    "            if first_unit_arrive is not None and time_received is not None:\n",
    "                overall_response_time = first_unit_arrive - time_received\n",
    "            else:\n",
    "                overall_response_time = None\n",
    "                \n",
    "            if first_unit_arrive is not None and first_unit_dispatch is not None:\n",
    "                officer_response_time = first_unit_arrive - first_unit_dispatch\n",
    "            else:\n",
    "                officer_response_time = None\n",
    "            call_id = safe_int(row[0]) # going to be using this again with the notes\n",
    "            db_row = {\n",
    "                'call_id': call_id,\n",
    "                'time_received': time_received,\n",
    "                'year_received': time_received.year,\n",
    "                'hour_received': time_received.hour,\n",
    "                'month_received': time_received.month,\n",
    "                'week_received': time_received.isocalendar()[1],\n",
    "                'dow_received': time_received.weekday(),\n",
    "                'case_id': clean_case_id(row[3]),\n",
    "                'call_source_id': safe_map(call_source_code_mapping, row[4]),\n",
    "                'primary_unit_id': safe_map(call_unit_code_mapping, row[5]),\n",
    "                'first_dispatched_id': safe_map(call_unit_code_mapping, row[6]),\n",
    "                'reporting_unit_id': safe_map(call_unit_code_mapping, row[50]),\n",
    "                'street_num': safe_int(row[7]),\n",
    "                'street_name': row[8],\n",
    "                'city_id': safe_map(city_code_mapping, row[10]),\n",
    "                'zip_code_id': safe_map(zip_code_mapping, row[11]),\n",
    "                'crossroad1': row[12],\n",
    "                'crossroad2': row[13],\n",
    "                'geox': safe_float(row[14]),\n",
    "                'geoy': safe_float(row[15]),\n",
    "                'beat_id': safe_map(beat_mapping, row[18]),\n",
    "                'district_id': safe_map(district_mapping, row[19]),\n",
    "                'sector_id': safe_map(sector_mapping, row[20]),\n",
    "                'business': row[21],\n",
    "                'nature_id': safe_map(nature_code_mapping, row[23]),\n",
    "                'priority_id': safe_map(priority_mapping, row[24]),\n",
    "                'report_only': safe_bool(row[25]),\n",
    "                'cancelled': safe_bool(row[26]),\n",
    "                'time_routed': safe_datetime(row[28]),\n",
    "                'time_finished': safe_datetime(row[30]),\n",
    "                'first_unit_dispatch': safe_datetime(row[32]),\n",
    "                'first_unit_enroute': safe_datetime(row[36]),\n",
    "                'first_unit_arrive': first_unit_arrive,\n",
    "                'first_unit_transport': safe_datetime(row[42]),\n",
    "                'last_unit_clear': safe_datetime(row[45]),\n",
    "                'time_closed': safe_datetime(row[49]),\n",
    "                'overall_response_time': overall_response_time,\n",
    "                'officer_response_time': officer_response_time,\n",
    "                'close_code_id': safe_map(close_code_mapping, row[51]),\n",
    "                'close_comments': row[52]\n",
    "            }\n",
    "            notes = split_notes(row[27])\n",
    "            \n",
    "            #try:\n",
    "#               have to insert one by one to properly handle the duplicate PKs in the data\n",
    "#               db.query(\"SAVEPOINT integrity_checkpoint;\")\n",
    "#               call.insert(db_row, ensure=False) # we know the right columns are already there\n",
    "            call_rows.append(db_row)\n",
    "            \n",
    "            for n in notes:\n",
    "                note_author_mapped = safe_map(note_author_mapping, n[2])\n",
    "                note_db_row = {'body': n[0],\n",
    "                             'time_recorded': n[1],\n",
    "                             'call_id': call_id,\n",
    "                             'note_author_id': note_author_mapped}\n",
    "                #note.insert(note_db_row, ensure=False)\n",
    "                note_rows.append(note_db_row)\n",
    "            #except IntegrityError:\n",
    "                # we seem to be missing some incident data\n",
    "                #print(\"insert of call_id %d failed due to integrity error\" % (call_id))\n",
    "                #db.query(\"ROLLBACK TO SAVEPOINT integrity_checkpoint;\")\n",
    "                \n",
    "            #db.query(\"RELEASE SAVEPOINT integrity_checkpoint;\")\n",
    "            \n",
    "            \n",
    "\n",
    "            j+=1\n",
    "            if j % 10000 == 0:\n",
    "                call.insert_many(call_rows, chunk_size=10000, ensure=False)\n",
    "                note.insert_many(note_rows, chunk_size=10000, ensure=False)\n",
    "                call_rows = []\n",
    "                note_rows = []\n",
    "                print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))\n",
    "            \n",
    "    call.insert_many(call_rows, ensure=False)\n",
    "    note.insert_many(note_rows, ensure=False)\n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_unitper.csv\n",
    "\n",
    "##Lookup tables\n",
    "Need to update call_unit and load shift and officer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "officers = {}\n",
    "shifts = set()\n",
    "\n",
    "call_units = set()\n",
    "db_call_units = set()\n",
    "# we already have most of the call_units from the call data, but there are more\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    db_call_units.add(row['descr'])\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "with open('../csv_data/cfs_2014_unitper.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "    reader = csv.reader(f)\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        \n",
    "        if first_row:\n",
    "            header = row\n",
    "            first_row = False\n",
    "            continue\n",
    "\n",
    "        # Strip whitespace and convert empty strings to None\n",
    "        row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "        if row[1]:\n",
    "            shifts.add(row[1])\n",
    "        \n",
    "        if row[2] and row[2] not in db_call_units:\n",
    "            call_units.add(row[2])\n",
    "        \n",
    "        cur_id = row[3]\n",
    "        # Clean the officer's name\n",
    "        cur_name = ','.join([t.strip() for t in row[4].split(',')]) if row[4] else ''\n",
    "        \n",
    "        if cur_id not in officers:\n",
    "            if cur_name.isdigit():\n",
    "                officers[cur_id] = {'name_aka': [cur_name]}\n",
    "            else:\n",
    "                officers[cur_id] = {'name': cur_name, 'name_aka': []}\n",
    "        else:\n",
    "            #if cur_name in officers[cur_id]['name_aka'] and cur_name == officers[cur_id]['name']:\n",
    "            #    print(cur_name, officers[cur_id]['name'], officers[cur_id]['name_aka'])\n",
    "                \n",
    "            if ('name' in officers[cur_id] or cur_name.isdigit())   \\\n",
    "                and cur_name not in officers[cur_id]['name_aka']    \\\n",
    "                and cur_name != officers[cur_id]['name']            \\\n",
    "                and cur_name is not None:\n",
    "                    officers[cur_id]['name_aka'].append(cur_name)\n",
    "            elif 'name' not in officers[cur_id] and not cur_name.isdigit():\n",
    "                officers[cur_id]['name'] = cur_name\n",
    "\n",
    "try:\n",
    "    call_unit = db['call_unit']\n",
    "    db.begin()\n",
    "    for c in call_units:\n",
    "        call_unit.insert({'descr': c})\n",
    "        \n",
    "    officer = db['officer']\n",
    "    for o_id in officers.keys():\n",
    "        if 'name_aka' in officers[o_id]:\n",
    "            db_rec = {\n",
    "                'officer_id': o_id,\n",
    "                'name': officers[o_id]['name'],\n",
    "                'name_aka': ','.join([ '\"' + n + '\"' for n in officers[o_id]['name_aka'] ])\n",
    "            }\n",
    "        else:\n",
    "            db_rec = {\n",
    "                'officer_id': o_id,\n",
    "                'name': officers[o_id]['name']\n",
    "            }\n",
    "        officer.insert(db_rec)\n",
    "        \n",
    "    shift = db['shift']\n",
    "    for s in shifts:\n",
    "        shift.insert({'shift_id': s})\n",
    "        \n",
    "    db.commit()\n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "\n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main table\n",
    "shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 seconds: completed 10000 rows\n",
      "18 seconds: completed 20000 rows\n",
      "27 seconds: completed 30000 rows\n",
      "37 seconds: completed 40000 rows\n",
      "46 seconds: completed 50000 rows\n",
      "55 seconds: completed 60000 rows\n",
      "65 seconds: completed 70000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else None    \n",
    "\n",
    "def safe_int(x):\n",
    "    return int(x) if x else None\n",
    "\n",
    "def safe_datetime(x):\n",
    "    # to_datetime returns a pandas Timestamp object, and we want a vanilla datetime\n",
    "    return pd.to_datetime(x).to_datetime() if x not in ('NULL', None) else None\n",
    "\n",
    "# Populate the mappings from the database\n",
    "call_unit_mapping = {}\n",
    "\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    call_unit_mapping[row['descr']] = row['call_unit_id']\n",
    "    \n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "shift_unit = db['shift_unit']\n",
    "db_rows = []\n",
    "db.begin()\n",
    "\n",
    "try:\n",
    "    with open('../csv_data/cfs_2014_unitper.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "            \n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "            \n",
    "            db_row = {\n",
    "                'shift_unit_id': safe_int(row[0]),\n",
    "                'shift_id': safe_int(row[1]),\n",
    "                'call_unit_id': safe_map(call_unit_mapping, row[2]),\n",
    "                'officer_id': safe_int(row[3]),\n",
    "                'in_time': safe_datetime(row[6]),\n",
    "                'out_time': safe_datetime(row[7]),\n",
    "                'bureau_id': safe_map(bureau_code_mapping, row[8]),\n",
    "                'division_id': safe_map(division_code_mapping, row[9]),\n",
    "                'unit_id': safe_map(unit_code_mapping, row[10])\n",
    "            }\n",
    "            \n",
    "            # have to insert one by one to properly handle the duplicate PKs in the data\n",
    "            shift_unit.insert(db_row, ensure=False) # we know the right columns are already there\n",
    "            \n",
    "            j+=1\n",
    "            if j % 10000 == 0:\n",
    "                print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))    \n",
    "            \n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_xxx2014_incilog.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lookup tables\n",
    "Need to update call_unit and do a full load of transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "months = (\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\")\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "transactions = set()\n",
    "\n",
    "call_units = set()\n",
    "db_call_units = set()\n",
    "# we already have most of the call_units from the call data, but there are more\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    db_call_units.add(row['descr'])\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "for month in months:  \n",
    "\n",
    "    with open('../csv_data/cfs_%s2014_incilog.csv' % (month), 'r', encoding='ISO-8859-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "\n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "            if row[2]:\n",
    "                transactions.add(row[2])\n",
    "            \n",
    "            if row[6] and row[6] not in db_call_units:\n",
    "                call_units.add(row[6])\n",
    "\n",
    "try:\n",
    "    call_unit = db['call_unit']\n",
    "    db.begin()\n",
    "    for c in call_units:\n",
    "        call_unit.insert({'descr': c})\n",
    "    db.commit()\n",
    "\n",
    "    transaction = db['transaction']\n",
    "    db.begin()\n",
    "    for t in transactions:\n",
    "        transaction.insert({'descr': t})\n",
    "    db.commit()\n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "\n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main table\n",
    "call_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for jan\n",
      "3 seconds: completed 10000 rows\n",
      "7 seconds: completed 20000 rows\n",
      "11 seconds: completed 30000 rows\n",
      "14 seconds: completed 40000 rows\n",
      "18 seconds: completed 50000 rows\n",
      "22 seconds: completed 60000 rows\n",
      "25 seconds: completed 70000 rows\n",
      "29 seconds: completed 80000 rows\n",
      "33 seconds: completed 90000 rows\n",
      "36 seconds: completed 100000 rows\n",
      "40 seconds: completed 110000 rows\n",
      "44 seconds: completed 120000 rows\n",
      "47 seconds: completed 130000 rows\n",
      "51 seconds: completed 140000 rows\n",
      "55 seconds: completed 150000 rows\n",
      "59 seconds: completed 160000 rows\n",
      "62 seconds: completed 170000 rows\n",
      "loading data for feb\n",
      "66 seconds: completed 180000 rows\n",
      "70 seconds: completed 190000 rows\n",
      "73 seconds: completed 200000 rows\n",
      "77 seconds: completed 210000 rows\n",
      "81 seconds: completed 220000 rows\n",
      "84 seconds: completed 230000 rows\n",
      "88 seconds: completed 240000 rows\n",
      "92 seconds: completed 250000 rows\n",
      "95 seconds: completed 260000 rows\n",
      "99 seconds: completed 270000 rows\n",
      "103 seconds: completed 280000 rows\n",
      "106 seconds: completed 290000 rows\n",
      "110 seconds: completed 300000 rows\n",
      "114 seconds: completed 310000 rows\n",
      "118 seconds: completed 320000 rows\n",
      "loading data for mar\n",
      "121 seconds: completed 330000 rows\n",
      "125 seconds: completed 340000 rows\n",
      "129 seconds: completed 350000 rows\n",
      "133 seconds: completed 360000 rows\n",
      "136 seconds: completed 370000 rows\n",
      "140 seconds: completed 380000 rows\n",
      "144 seconds: completed 390000 rows\n",
      "147 seconds: completed 400000 rows\n",
      "151 seconds: completed 410000 rows\n",
      "155 seconds: completed 420000 rows\n",
      "158 seconds: completed 430000 rows\n",
      "162 seconds: completed 440000 rows\n",
      "166 seconds: completed 450000 rows\n",
      "170 seconds: completed 460000 rows\n",
      "173 seconds: completed 470000 rows\n",
      "177 seconds: completed 480000 rows\n",
      "loading data for apr\n",
      "181 seconds: completed 490000 rows\n",
      "184 seconds: completed 500000 rows\n",
      "188 seconds: completed 510000 rows\n",
      "192 seconds: completed 520000 rows\n",
      "195 seconds: completed 530000 rows\n",
      "199 seconds: completed 540000 rows\n",
      "203 seconds: completed 550000 rows\n",
      "206 seconds: completed 560000 rows\n",
      "210 seconds: completed 570000 rows\n",
      "214 seconds: completed 580000 rows\n",
      "218 seconds: completed 590000 rows\n",
      "221 seconds: completed 600000 rows\n",
      "225 seconds: completed 610000 rows\n",
      "229 seconds: completed 620000 rows\n",
      "233 seconds: completed 630000 rows\n",
      "236 seconds: completed 640000 rows\n",
      "240 seconds: completed 650000 rows\n",
      "loading data for may\n",
      "244 seconds: completed 660000 rows\n",
      "248 seconds: completed 670000 rows\n",
      "251 seconds: completed 680000 rows\n",
      "255 seconds: completed 690000 rows\n",
      "259 seconds: completed 700000 rows\n",
      "262 seconds: completed 710000 rows\n",
      "266 seconds: completed 720000 rows\n",
      "270 seconds: completed 730000 rows\n",
      "273 seconds: completed 740000 rows\n",
      "277 seconds: completed 750000 rows\n",
      "281 seconds: completed 760000 rows\n",
      "284 seconds: completed 770000 rows\n",
      "288 seconds: completed 780000 rows\n",
      "292 seconds: completed 790000 rows\n",
      "296 seconds: completed 800000 rows\n",
      "299 seconds: completed 810000 rows\n",
      "303 seconds: completed 820000 rows\n",
      "307 seconds: completed 830000 rows\n",
      "loading data for jun\n",
      "311 seconds: completed 840000 rows\n",
      "314 seconds: completed 850000 rows\n",
      "318 seconds: completed 860000 rows\n",
      "322 seconds: completed 870000 rows\n",
      "326 seconds: completed 880000 rows\n",
      "329 seconds: completed 890000 rows\n",
      "333 seconds: completed 900000 rows\n",
      "337 seconds: completed 910000 rows\n",
      "341 seconds: completed 920000 rows\n",
      "344 seconds: completed 930000 rows\n",
      "348 seconds: completed 940000 rows\n",
      "352 seconds: completed 950000 rows\n",
      "355 seconds: completed 960000 rows\n",
      "359 seconds: completed 970000 rows\n",
      "363 seconds: completed 980000 rows\n",
      "366 seconds: completed 990000 rows\n",
      "loading data for jul\n",
      "370 seconds: completed 1000000 rows\n",
      "374 seconds: completed 1010000 rows\n",
      "377 seconds: completed 1020000 rows\n",
      "381 seconds: completed 1030000 rows\n",
      "385 seconds: completed 1040000 rows\n",
      "389 seconds: completed 1050000 rows\n",
      "392 seconds: completed 1060000 rows\n",
      "396 seconds: completed 1070000 rows\n",
      "400 seconds: completed 1080000 rows\n",
      "403 seconds: completed 1090000 rows\n",
      "407 seconds: completed 1100000 rows\n",
      "411 seconds: completed 1110000 rows\n",
      "414 seconds: completed 1120000 rows\n",
      "418 seconds: completed 1130000 rows\n",
      "loading data for aug\n",
      "422 seconds: completed 1140000 rows\n",
      "425 seconds: completed 1150000 rows\n",
      "429 seconds: completed 1160000 rows\n",
      "433 seconds: completed 1170000 rows\n",
      "436 seconds: completed 1180000 rows\n",
      "440 seconds: completed 1190000 rows\n",
      "444 seconds: completed 1200000 rows\n",
      "447 seconds: completed 1210000 rows\n",
      "451 seconds: completed 1220000 rows\n",
      "455 seconds: completed 1230000 rows\n",
      "458 seconds: completed 1240000 rows\n",
      "462 seconds: completed 1250000 rows\n",
      "466 seconds: completed 1260000 rows\n",
      "469 seconds: completed 1270000 rows\n",
      "473 seconds: completed 1280000 rows\n",
      "loading data for sep\n",
      "477 seconds: completed 1290000 rows\n",
      "480 seconds: completed 1300000 rows\n",
      "484 seconds: completed 1310000 rows\n",
      "488 seconds: completed 1320000 rows\n",
      "491 seconds: completed 1330000 rows\n",
      "495 seconds: completed 1340000 rows\n",
      "498 seconds: completed 1350000 rows\n",
      "502 seconds: completed 1360000 rows\n",
      "506 seconds: completed 1370000 rows\n",
      "509 seconds: completed 1380000 rows\n",
      "513 seconds: completed 1390000 rows\n",
      "517 seconds: completed 1400000 rows\n",
      "520 seconds: completed 1410000 rows\n",
      "524 seconds: completed 1420000 rows\n",
      "loading data for oct\n",
      "528 seconds: completed 1430000 rows\n",
      "531 seconds: completed 1440000 rows\n",
      "535 seconds: completed 1450000 rows\n",
      "539 seconds: completed 1460000 rows\n",
      "542 seconds: completed 1470000 rows\n",
      "546 seconds: completed 1480000 rows\n",
      "550 seconds: completed 1490000 rows\n",
      "553 seconds: completed 1500000 rows\n",
      "557 seconds: completed 1510000 rows\n",
      "561 seconds: completed 1520000 rows\n",
      "565 seconds: completed 1530000 rows\n",
      "568 seconds: completed 1540000 rows\n",
      "572 seconds: completed 1550000 rows\n",
      "576 seconds: completed 1560000 rows\n",
      "loading data for nov\n",
      "579 seconds: completed 1570000 rows\n",
      "583 seconds: completed 1580000 rows\n",
      "587 seconds: completed 1590000 rows\n",
      "591 seconds: completed 1600000 rows\n",
      "594 seconds: completed 1610000 rows\n",
      "598 seconds: completed 1620000 rows\n",
      "602 seconds: completed 1630000 rows\n",
      "606 seconds: completed 1640000 rows\n",
      "609 seconds: completed 1650000 rows\n",
      "613 seconds: completed 1660000 rows\n",
      "617 seconds: completed 1670000 rows\n",
      "620 seconds: completed 1680000 rows\n",
      "624 seconds: completed 1690000 rows\n",
      "628 seconds: completed 1700000 rows\n",
      "loading data for dec\n",
      "632 seconds: completed 1710000 rows\n",
      "635 seconds: completed 1720000 rows\n",
      "639 seconds: completed 1730000 rows\n",
      "642 seconds: completed 1740000 rows\n",
      "646 seconds: completed 1750000 rows\n",
      "650 seconds: completed 1760000 rows\n",
      "653 seconds: completed 1770000 rows\n",
      "657 seconds: completed 1780000 rows\n",
      "661 seconds: completed 1790000 rows\n",
      "664 seconds: completed 1800000 rows\n",
      "668 seconds: completed 1810000 rows\n",
      "672 seconds: completed 1820000 rows\n",
      "676 seconds: completed 1830000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "months = (\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\")\n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else d\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "def safe_int(x):\n",
    "    return int(x) if x else None\n",
    "\n",
    "def safe_datetime(x):\n",
    "    # to_datetime returns a pandas Timestamp object, and we want a vanilla datetime\n",
    "    return pd.to_datetime(x).to_datetime() if x not in ('NULL', None) else None\n",
    "\n",
    "# Populate the mappings from the database\n",
    "call_unit_mapping = {}\n",
    "transaction_code_mapping = {}\n",
    "\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    call_unit_mapping[row['descr']] = row['call_unit_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM transaction;\"):\n",
    "    transaction_code_mapping[row['descr']] = row['transaction_id']\n",
    "    \n",
    "# We have fire and EMS call_log data, which we don't have calls for.  We need to ignore this data.\n",
    "valid_call_ids = set()\n",
    "for row in db.query(\"SELECT call_id FROM call;\"):\n",
    "    valid_call_ids.add(row['call_id'])\n",
    "    \n",
    "# Same deal for shifts, but in this case, we'll just set the shift_ids to null in the offending records instead of\n",
    "# dropping them.\n",
    "valid_shift_ids = set()\n",
    "for row in db.query(\"SELECT shift_id FROM shift;\"):\n",
    "    valid_shift_ids.add(row['shift_id'])\n",
    "\n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "call_log = db['call_log']\n",
    "db_rows = []\n",
    "db.begin()\n",
    "\n",
    "try:\n",
    "    for month in months:\n",
    "        print(\"loading data for %s\" % (month))\n",
    "        with open('../csv_data/cfs_%s2014_incilog.csv' % (month), 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            first_row = True\n",
    "            for row in reader:\n",
    "                if first_row:\n",
    "                    header = row\n",
    "                    first_row = False\n",
    "                    continue\n",
    "\n",
    "                #for i in range(len(header)):\n",
    "                #    print(i, header[i])\n",
    "\n",
    "                # Strip whitespace and convert empty strings to None\n",
    "                row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "                \n",
    "                call_id = safe_int(row[5])\n",
    "                shift_id = safe_int(row[8])\n",
    "                if call_id in valid_call_ids:\n",
    "                    db_rows.append({\n",
    "                        'call_log_id': safe_int(row[0]),\n",
    "                        'transaction_id': safe_map(transaction_code_mapping, row[2]),\n",
    "                        'time_recorded': safe_datetime(row[3]),\n",
    "                        'call_id': call_id,\n",
    "                        'call_unit_id': safe_map(call_unit_mapping, row[6]),\n",
    "                        'shift_id': shift_id if shift_id in valid_shift_ids else None,\n",
    "                        'close_code_id': safe_map(close_code_mapping, row[9])\n",
    "                    })\n",
    "                    j+=1\n",
    "                    if j % 10000 == 0:\n",
    "                        call_log.insert_many(db_rows, chunk_size=10000, ensure=False)\n",
    "                        db_rows=[]\n",
    "                        print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))\n",
    "    \n",
    "    call_log.insert_many(db_rows, ensure=False)\n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_outserv.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Lookup tables\n",
    "Need to update call_unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "call_units = set()\n",
    "db_call_units = set()\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    db_call_units.add(row['descr'])\n",
    "    \n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "with open('../csv_data/cfs_2014_outserv.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "    reader = csv.reader(f)\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        if first_row:\n",
    "            header = row\n",
    "            first_row = False\n",
    "            continue\n",
    "\n",
    "        # Strip whitespace and convert empty strings to None\n",
    "        row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "        if row[1] and row[1] not in db_call_units:\n",
    "            call_units.add(row[1])\n",
    "\n",
    "try:\n",
    "    call_unit = db['call_unit']\n",
    "    db.begin()\n",
    "    for c in call_units:\n",
    "        call_unit.insert({'descr': c})\n",
    "    db.commit()\n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "\n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 seconds: completed 10000 rows\n",
      "14 seconds: completed 20000 rows\n",
      "21 seconds: completed 30000 rows\n",
      "27 seconds: completed 40000 rows\n",
      "34 seconds: completed 50000 rows\n",
      "41 seconds: completed 60000 rows\n",
      "48 seconds: completed 70000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# We have several columns we need to convert to int that can also be None\n",
    "def safe_int(x):\n",
    "    return int(x) if x else None\n",
    "\n",
    "def safe_datetime(x):\n",
    "    # to_datetime returns a pandas Timestamp object, and we want a vanilla datetime\n",
    "    return pd.to_datetime(x).to_datetime() if x not in ('NULL', None) else None\n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else None\n",
    "\n",
    "call_unit_mapping = {}\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    call_unit_mapping[row['descr']] = row['call_unit_id']\n",
    "    \n",
    "# Set shift_ids to null if they violate the fk constraint\n",
    "valid_shift_ids = set()\n",
    "for row in db.query(\"SELECT shift_id FROM shift;\"):\n",
    "    valid_shift_ids.add(row['shift_id'])\n",
    "    \n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "oos_rows = []\n",
    "\n",
    "oos = db['out_of_service']\n",
    "\n",
    "try:\n",
    "    with open('../csv_data/cfs_2014_outserv.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "            \n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "            shift_id = safe_int(row[8])\n",
    "            db_row = {\n",
    "                'oos_id': safe_int(row[0]),\n",
    "                'call_unit_id': safe_map(call_unit_mapping, row[1]),\n",
    "                'oos_code_id': safe_map(oos_code_mapping, row[2]),\n",
    "                'location': row[3],\n",
    "                'comments': row[4],\n",
    "                'start_time': safe_datetime(row[5]),\n",
    "                'end_time': safe_datetime(row[6]),\n",
    "                'duration': safe_datetime(row[6]) - safe_datetime(row[5]),\n",
    "                'shift_id': shift_id if shift_id in valid_shift_ids else None\n",
    "            }\n",
    "            oos_rows.append(db_row)\n",
    "            \n",
    "            j+=1\n",
    "            if j % 10000 == 0:\n",
    "                oos.insert_many(oos_rows, chunk_size=10000, ensure=False)\n",
    "                oos_rows = []\n",
    "                print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))\n",
    "    oos.insert_many(oos_rows, ensure=False)\n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Other tables\n",
    "##squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_unit_squad_regexes = {\n",
    "    'A': '^A[1-5][0-9]{2}$',\n",
    "    'B': '^B[1-5][0-9]{2}$',\n",
    "    'C': '^C[1-5][0-9]{2}$',\n",
    "    'D': '^D[1-5][0-9]{2}$',\n",
    "    'BIKE': '^L5[0-9]{2}$',\n",
    "    'HEAT': '^H[1-4][0-9]{2}$',\n",
    "    'K9': '^K[0-9]{2}$',\n",
    "    'MOTORS': '^MTR[2-8]$',\n",
    "    'TACT': '^T[2-8]$',\n",
    "    'VIR': '^ED6[0-6]$'\n",
    "}\n",
    "\n",
    "squad_table = db['squad']\n",
    "\n",
    "for squad in call_unit_squad_regexes.keys():\n",
    "    squad_table.insert({'descr': squad})\n",
    "\n",
    "squad_mapping = {}\n",
    "for row in db.query(\"SELECT * FROM squad;\"):\n",
    "    squad_mapping[row['descr']] = row['squad_id']\n",
    "\n",
    "for squad, re in call_unit_squad_regexes.items():\n",
    "    db.query(\"\"\"\n",
    "    UPDATE call_unit\n",
    "    SET squad_id = %s\n",
    "    WHERE descr ~ '%s'\n",
    "    \"\"\" % (squad_mapping[squad], re))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Enable GIS extensions (PostGIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.query(\"CREATE EXTENSION postgis;\")\n",
    "db.query(\"CREATE EXTENSION postgis_topology;\")\n",
    "db.query(\"CREATE EXTENSION fuzzystrmatch;\")\n",
    "db.query(\"CREATE EXTENSION postgis_tiger_geocoder;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use PostGIS to convert the NC state planar coordinates to latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.persistence.util.ResultIter at 0x111f92208>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(\"\"\"DROP TABLE IF EXISTS call_latlong CASCADE;\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "CREATE TABLE call_latlong AS (\n",
    "    SELECT call_id, st_x(point) AS longitude, st_y(point) AS latitude, point\n",
    "    FROM (\n",
    "        SELECT call_id, \n",
    "        st_Transform(ST_SetSRID(ST_MakePoint(geox, geoy), 2264), 4326)::geometry(Point, 4326) AS point\n",
    "        FROM call\n",
    "    ) AS a\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "ALTER TABLE call_latlong\n",
    "ADD CONSTRAINT call_latlong_pk PRIMARY KEY (call_id);\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "ALTER TABLE call_latlong\n",
    "ADD CONSTRAINT call_call_latlong_fk FOREIGN KEY (call_id) REFERENCES call (call_id);\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "db.query(\"\"\"DROP TABLE IF EXISTS incident_latlong CASCADE;\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "CREATE TABLE incident_latlong AS (\n",
    "    SELECT incident_id, st_x(point) AS longitude, st_y(point) AS latitude, point\n",
    "    FROM (\n",
    "        SELECT incident_id, \n",
    "        -- We have to divide incident x and y by 100 to get the proper numbers\n",
    "        st_Transform(ST_SetSRID(ST_MakePoint(geox/100, geoy/100), 2264), 4326)::geometry(Point, 4326) AS point\n",
    "        FROM incident\n",
    "    ) AS a\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "ALTER TABLE incident_latlong\n",
    "ADD CONSTRAINT incident_latlong_pk PRIMARY KEY (incident_id);\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "ALTER TABLE incident_latlong\n",
    "ADD CONSTRAINT incident_incident_latlong_fk FOREIGN KEY (incident_id) REFERENCES incident (incident_id);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Loading the beats shapefiles via PostGIS: (the shapefile.sql script creates the `beat_temp` table, which is used to make the final `beat_geom` table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the beat_temp table with the exact info from the shapefile; we'll only be keeping part of it\n",
    "with open(\"../shapefiles/beats_districts/shapefile.sql\", 'r') as f:\n",
    "    db.query(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.persistence.util.ResultIter at 0x1069e1e80>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(\"\"\"DROP TABLE IF EXISTS beat_geom CASCADE;\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "CREATE TABLE beat_geom AS\n",
    "SELECT\n",
    "  --generate_series(1, (SELECT COUNT(*) FROM beat_temp)) AS beat_geom_id,\n",
    "  row_number() over () AS beat_geom_id,\n",
    "  beat_id,\n",
    "  CASE\n",
    "    WHEN contiguous = 'yes' THEN TRUE\n",
    "    WHEN contiguous = 'no' THEN FALSE\n",
    "    ELSE NULL\n",
    "  END AS contiguous,\n",
    "  geom\n",
    "FROM\n",
    "  beat_temp,\n",
    "  beat\n",
    "WHERE beat_temp.cad = beat.descr;\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "ALTER TABLE beat_geom\n",
    "ADD CONSTRAINT beat_geom_pk PRIMARY KEY (beat_geom_id);\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "ALTER TABLE beat_geom\n",
    "ADD CONSTRAINT beat_beat_geom_fk FOREIGN KEY (beat_id) REFERENCES beat (beat_id);\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "DROP TABLE beat_temp;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Misc. Other Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the incident_id column to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../scripts/call_incident_id.sql\", 'r') as f:\n",
    "    db.query(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the relationships between beats, districts, and sectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.persistence.util.ResultIter at 0x101fb8be0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(\"\"\"\n",
    "UPDATE beat\n",
    "SET district_id = (\n",
    "  SELECT district_id\n",
    "  FROM district\n",
    "  WHERE district.descr = 'D' || SUBSTRING(beat.descr::text FROM 1 FOR 1)\n",
    ")\n",
    "WHERE beat.descr NOT IN ('DSO', 'OOJ');\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "UPDATE beat\n",
    "SET sector_id = (\n",
    "  SELECT sector_id\n",
    "  FROM sector\n",
    "  WHERE sector.descr = 'NTH')\n",
    "WHERE beat.district_id IN (\n",
    "  SELECT district_id\n",
    "  FROM district\n",
    "  WHERE district.descr IN ('D2', 'D1', 'D5')\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "UPDATE beat\n",
    "SET sector_id = (\n",
    "  SELECT sector_id\n",
    "  FROM sector\n",
    "  WHERE sector.descr = 'STH')\n",
    "WHERE beat.district_id IN (\n",
    "  SELECT district_id\n",
    "  FROM district\n",
    "  WHERE district.descr IN ('D3', 'D4')\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "UPDATE district\n",
    "SET sector_id = (\n",
    "SELECT sector_id\n",
    "  FROM sector\n",
    "  WHERE sector.descr = 'STH')\n",
    "WHERE district.descr IN ('D3', 'D4');\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "UPDATE district\n",
    "SET sector_id = (\n",
    "SELECT sector_id\n",
    "  FROM sector\n",
    "  WHERE sector.descr = 'NTH')\n",
    "WHERE district.descr IN ('D2', 'D1', 'D5');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (index_name, table_name, column_name)\n",
    "ndx_jobs = [\n",
    "    ('call_call_source_id_ndx', 'call', 'call_source_id'),\n",
    "    ('call_primary_unit_id_ndx', 'call', 'primary_unit_id'),\n",
    "    ('call_first_dispatched_unit_id_ndx', 'call', 'first_dispatched_id'),\n",
    "    ('call_reporting_unit_id_ndx', 'call', 'reporting_unit_id'),\n",
    "    ('call_city_id_ndx', 'call', 'city_id'),\n",
    "    ('call_beat_id_ndx', 'call', 'beat_id'),\n",
    "    ('call_district_id_ndx', 'call', 'district_id'),\n",
    "    ('call_sector_id_ndx', 'call', 'sector_id'),\n",
    "    ('call_nature_id_ndx', 'call', 'nature_id'),\n",
    "    ('call_close_code_id_ndx', 'call', 'close_code_id'),\n",
    "    ('call_zip_code_id_ndx', 'call', 'zip_code_id'),\n",
    "    # call_incident_id_ndx is created with the incident_id column\n",
    "    ('incident_city_id_ndx', 'incident', 'city_id'),\n",
    "    ('incident_beat_id_ndx', 'incident', 'beat_id'),\n",
    "    ('incident_district_id_ndx', 'incident', 'district_id'),\n",
    "    ('incident_sector_id_ndx', 'incident', 'sector_id'),\n",
    "    ('incident_premise_id_ndx', 'incident', 'premise_id'),\n",
    "    ('incident_weapon_id_ndx', 'incident', 'weapon_id'),\n",
    "    ('incident_emp_bureau_id_ndx', 'incident', 'emp_bureau_id'),\n",
    "    ('incident_emp_division_id_ndx', 'incident', 'emp_division_id'),\n",
    "    ('incident_emp_unit_id_ndx', 'incident', 'emp_unit_id'),\n",
    "    ('incident_investigation_status_id_ndx', 'incident', 'investigation_status_id'),\n",
    "    ('incident_investigator_unit_id_ndx', 'incident', 'investigator_unit_id'),\n",
    "    ('incident_case_status_id_ndx', 'incident', 'case_status_id'),\n",
    "    ('incident_ucr_descr_id_ndx', 'incident', 'ucr_descr_id'),\n",
    "    ('incident_zip_code_id_ndx', 'incident', 'zip_code_id')\n",
    "]\n",
    "\n",
    "for job in ndx_jobs:\n",
    "    db.query(\"CREATE INDEX %s ON %s (%s);\" % (job[0], job[1], job[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
