{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Loading, Cleaning, and Normalization\n",
    "Now that we have a better idea of what the data contains, we're going to load it in a format that will be more efficient for analysis.  Changes still needed:\n",
    " - note splitting regex still has issues.  `SELECT descr, COUNT(*) FROM note_author GROUP BY descr ORDER BY COUNT` will show the weird ones\n",
    " \n",
    "We'll load each table as a two-step process.  First, we scan each table and accumulate a set for each lookup table associated.  We'll then load these lookup tables.  Second, we'll load the main table.  This should be less complicated than trying to accumulate the lookup tables during the chunked-out load of the main table.\n",
    "\n",
    "Main table: call\n",
    "Lookup tables: call_unit, city, nature\n",
    "Lookup tables loaded separately: call_source\n",
    "\n",
    "Main table: note\n",
    "Lookup tables: note_author\n",
    "\n",
    "Main table: call_log\n",
    "Lookup tables: transaction\n",
    "Lookup tables loaded separately: close_code\n",
    "\n",
    "Main table: incident\n",
    "Lookup tables: city (should already have everything from call), ucr_descr\n",
    "Lookup tables loaded separately: premise, weapon, bureau, division, unit, investigation_status, case_status\n",
    "\n",
    "Main table: modus_operandi\n",
    "Lookup tables: mo_item\n",
    "\n",
    "Main table: out_of_service\n",
    "Lookup tables: call_unit (update), os_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use dataset to stuff the data into a local instance of postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the tables before touching the data so they have all the proper constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Database DDL\n",
    "\n",
    "Code to create the database schema is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jnance/anaconda/lib/python3.4/site-packages/sqlalchemy/dialects/postgresql/base.py:2421: SAWarning: Did not recognize type 'geometry' of column 'extent'\n",
      "  (attype, name))\n"
     ]
    }
   ],
   "source": [
    "# CHANGE CREDENTIALS AS APPROPRIATE\n",
    "sqlalchemy_uri = 'postgresql://datascientist:1234thumbwar@freyja.rtp.rti.org:5432/cfs'\n",
    "#sqlalchemy_uri = 'postgresql://jnance:@localhost:5432/cfs'\n",
    "\n",
    "db = dataset.connect(sqlalchemy_uri)\n",
    "engine = create_engine(sqlalchemy_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_db():\n",
    "    \"\"\"\n",
    "    Remove and recreate tables to prepare for reloading the db\n",
    "    \"\"\"\n",
    "    db.query(\"DROP TABLE IF EXISTS note CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS note_author CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call_source CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call_unit CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS city CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS call_log CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS transaction CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS close_code CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS ucr_descr CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS incident CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS modus_operandi CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS mo_item CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS bureau CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS case_status CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS division CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS unit CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS investigation_status CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS weapon CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS weapon_group CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS premise CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS premise_group CASCADE;\")\n",
    "    db.query(\"DROP TABLE IF EXISTS NATURE CASCADE;\")\n",
    "\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE ucr_descr\n",
    "    (\n",
    "      ucr_descr_id serial NOT NULL,\n",
    "      short_descr text,\n",
    "      long_descr text,\n",
    "      CONSTRAINT ucr_descr_pk PRIMARY KEY (ucr_descr_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE bureau\n",
    "    (\n",
    "      bureau_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT bureau_pk PRIMARY KEY (bureau_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE division\n",
    "    (\n",
    "      division_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT division_pk PRIMARY KEY (division_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE investigation_status\n",
    "    (\n",
    "      investigation_status_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT investigation_status_pk PRIMARY KEY (investigation_status_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE case_status\n",
    "    (\n",
    "      case_status_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT case_status_pk PRIMARY KEY (case_status_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE unit\n",
    "    (\n",
    "      unit_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT unit_pk PRIMARY KEY (unit_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE weapon_group\n",
    "    (\n",
    "      weapon_group_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT weapon_group_pk PRIMARY KEY (weapon_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE premise_group\n",
    "    (\n",
    "      premise_group_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT premise_group_pk PRIMARY KEY (premise_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE weapon\n",
    "    (\n",
    "      weapon_id serial NOT NULL,\n",
    "      descr text,\n",
    "      weapon_group_id int,\n",
    "      CONSTRAINT weapon_pk PRIMARY KEY (weapon_id),\n",
    "      CONSTRAINT weapon_group_weapon_fk FOREIGN KEY (weapon_group_id) REFERENCES weapon_group (weapon_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE premise\n",
    "    (\n",
    "      premise_id serial NOT NULL,\n",
    "      descr text,\n",
    "      premise_group_id int,\n",
    "      CONSTRAINT premise_pk PRIMARY KEY (premise_id),\n",
    "      CONSTRAINT premise_group_premise_fk FOREIGN KEY (premise_group_id) REFERENCES premise_group (premise_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE city\n",
    "    (\n",
    "      city_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT city_pk PRIMARY KEY (city_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE incident\n",
    "    (\n",
    "      incident_id bigint NOT NULL,\n",
    "      case_id bigint UNIQUE,\n",
    "      time_filed timestamp without time zone,\n",
    "      month_filed int,\n",
    "      week_filed int,\n",
    "      dow_filed int,\n",
    "      street_num int,\n",
    "      street_name text,\n",
    "      city_id int,\n",
    "      zip int,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      beat text,\n",
    "      district text,\n",
    "      sector text,\n",
    "      premise_id int,\n",
    "      weapon_id int,\n",
    "      domestic boolean,\n",
    "      juvenile boolean,\n",
    "      gang_related boolean,\n",
    "      emp_bureau_id int,\n",
    "      emp_division_id int,\n",
    "      emp_unit_id int,\n",
    "      num_officers int,\n",
    "      investigation_status_id int,\n",
    "      investigator_unit_id int,\n",
    "      case_status_id int,\n",
    "      ucr_code int,\n",
    "      ucr_descr_id int,\n",
    "      committed boolean,\n",
    "      \n",
    "      CONSTRAINT incident_pk PRIMARY KEY (incident_id),\n",
    "      \n",
    "      CONSTRAINT case_status_incident_fk\n",
    "        FOREIGN KEY (case_status_id) REFERENCES case_status (case_status_id),\n",
    "      CONSTRAINT bureau_incident_fk\n",
    "        FOREIGN KEY (emp_bureau_id) REFERENCES bureau (bureau_id),\n",
    "      CONSTRAINT division_incident_fk\n",
    "        FOREIGN KEY (emp_division_id) REFERENCES division (division_id),\n",
    "      CONSTRAINT unit_incident_emp_fk\n",
    "        FOREIGN KEY (emp_unit_id) REFERENCES unit (unit_id),\n",
    "      CONSTRAINT unit_incident_investigator_fk\n",
    "        FOREIGN KEY (investigator_unit_id) REFERENCES unit (unit_id),\n",
    "      CONSTRAINT investigation_status_incident_fk\n",
    "        FOREIGN KEY (investigation_status_id) REFERENCES investigation_status (investigation_status_id),\n",
    "      CONSTRAINT premise_incident_fk\n",
    "        FOREIGN KEY (premise_id) REFERENCES premise (premise_id),\n",
    "      CONSTRAINT weapon_incident_fk\n",
    "        FOREIGN KEY (weapon_id) REFERENCES weapon (weapon_id),\n",
    "      CONSTRAINT city_incident_fk\n",
    "        FOREIGN KEY (city_id) REFERENCES city (city_id),\n",
    "      CONSTRAINT ucr_descr_incident_fk\n",
    "        FOREIGN KEY (ucr_descr_id) REFERENCES ucr_descr (ucr_descr_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE mo_item\n",
    "    (\n",
    "      mo_item_id serial NOT NULL,\n",
    "      item_descr text,\n",
    "      mo_group_id serial NOT NULL,\n",
    "      group_descr text,\n",
    "      CONSTRAINT mo_item_pk PRIMARY KEY (mo_item_id, mo_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE modus_operandi\n",
    "    (\n",
    "      incident_id bigint,\n",
    "      mo_id bigint,\n",
    "      mo_group_id int,\n",
    "      mo_item_id int,\n",
    "      \n",
    "      CONSTRAINT mo_pk PRIMARY KEY (mo_id),\n",
    "      \n",
    "      CONSTRAINT incident_modus_operandi_fk FOREIGN KEY (incident_id) REFERENCES incident (incident_id),\n",
    "      CONSTRAINT mo_item_modus_operandi_fk FOREIGN KEY (mo_item_id, mo_group_id) \n",
    "        REFERENCES mo_item (mo_item_id, mo_group_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call_source\n",
    "    (\n",
    "      call_source_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT call_source_pk PRIMARY KEY (call_source_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call_unit\n",
    "    (\n",
    "      call_unit_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT call_unit_pk PRIMARY KEY (call_unit_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE close_code\n",
    "    (\n",
    "      close_code_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT close_code_pk PRIMARY KEY (close_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE nature\n",
    "    (\n",
    "      nature_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT nature_pk PRIMARY KEY (nature_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call\n",
    "    (\n",
    "      call_id bigint NOT NULL,\n",
    "      month_received int,\n",
    "      week_received int,\n",
    "      dow_received int,\n",
    "      hour_received int,\n",
    "      case_id bigint,\n",
    "      call_source_id int,\n",
    "      primary_unit_id int,\n",
    "      first_dispatched_id int,\n",
    "      reporting_unit_id int,\n",
    "      street_num int,\n",
    "      street_name text,\n",
    "      city_id int,\n",
    "      zip int,\n",
    "      crossroad1 text,\n",
    "      crossroad2 text,\n",
    "      geox double precision,\n",
    "      geoy double precision,\n",
    "      beat text,\n",
    "      district text,\n",
    "      sector text,\n",
    "      business text,\n",
    "      nature_id int,\n",
    "      priority text,\n",
    "      report_only boolean,\n",
    "      cancelled boolean,\n",
    "      time_received timestamp without time zone,\n",
    "      time_routed timestamp without time zone,\n",
    "      time_finished timestamp without time zone,\n",
    "      first_unit_dispatch timestamp without time zone,\n",
    "      first_unit_enroute timestamp without time zone,\n",
    "      first_unit_arrive timestamp without time zone,\n",
    "      first_unit_transport timestamp without time zone,\n",
    "      last_unit_clear timestamp without time zone,\n",
    "      time_closed timestamp without time zone,\n",
    "      close_code_id int,\n",
    "      close_comments text,\n",
    "      \n",
    "      CONSTRAINT call_pk PRIMARY KEY (call_id),\n",
    "      \n",
    "      CONSTRAINT call_source_call_fk\n",
    "        FOREIGN KEY (call_source_id) REFERENCES call_source (call_source_id),\n",
    "      CONSTRAINT call_unit_call_primary_unit_fk\n",
    "        FOREIGN KEY (primary_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT call_unit_call_first_dispatched_fk\n",
    "        FOREIGN KEY (first_dispatched_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT call_unit_call_reporting_unit_fk\n",
    "        FOREIGN KEY (reporting_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT city_call_fk\n",
    "        FOREIGN KEY (city_id) REFERENCES city (city_id),\n",
    "      CONSTRAINT close_code_call_fk\n",
    "        FOREIGN KEY (close_code_id) REFERENCES close_code (close_code_id),\n",
    "      --There is some mismatch here that might be valid; no constraint for now\n",
    "      --CONSTRAINT incident_call_fk\n",
    "      --  FOREIGN KEY (case_id) REFERENCES incident (case_id),\n",
    "      CONSTRAINT nature_call_fk\n",
    "        FOREIGN KEY (nature_id) REFERENCES nature (nature_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE note_author\n",
    "    (\n",
    "      note_author_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT note_author_pk PRIMARY KEY (note_author_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE note\n",
    "    (\n",
    "      note_id serial NOT NULL,\n",
    "      body text,\n",
    "      time_recorded timestamp without time zone,\n",
    "      note_author_id int,\n",
    "      call_id bigint,\n",
    "      CONSTRAINT note_pk PRIMARY KEY (note_id),\n",
    "      \n",
    "      CONSTRAINT call_note_fk FOREIGN KEY (call_id) REFERENCES call (call_id),\n",
    "      CONSTRAINT note_author_note_fk FOREIGN KEY (note_author_id) REFERENCES note_author (note_author_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE transaction\n",
    "    (\n",
    "      transaction_id serial NOT NULL,\n",
    "      descr text,\n",
    "      CONSTRAINT transaction_pk PRIMARY KEY (transaction_id)\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE call_log\n",
    "    (\n",
    "      call_log_id bigint NOT NULL,\n",
    "      transaction_id int,\n",
    "      time_recorded timestamp without time zone,\n",
    "      call_id bigint,\n",
    "      call_unit_id int,\n",
    "      close_code_id int,\n",
    "      \n",
    "      CONSTRAINT call_log_pk PRIMARY KEY (call_log_id),\n",
    "      \n",
    "      CONSTRAINT call_unit_call_log_fk FOREIGN KEY (call_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT call_call_log_fk FOREIGN KEY (call_id) REFERENCES call (call_id),\n",
    "      CONSTRAINT close_code_call_log_fk FOREIGN KEY (close_code_id) REFERENCES close_code (close_code_id),\n",
    "      CONSTRAINT transaction_call_log_fk FOREIGN KEY (transaction_id) REFERENCES transaction (transaction_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE oos_code\n",
    "    (\n",
    "      oos_code_id serial NOT NULL,\n",
    "      descr text,\n",
    "      \n",
    "      CONSTRAINT oos_code_pk PRIMARY KEY (oos_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    db.query(\"\"\"\n",
    "    CREATE TABLE out_of_service\n",
    "    (\n",
    "      oos_id bigint NOT NULL,\n",
    "      call_unit_id int,\n",
    "      oos_code_id int,\n",
    "      location text,\n",
    "      comments text,\n",
    "      start_time timestamp without time zone,\n",
    "      end_time timestamp without time zone,\n",
    "      duration interval,\n",
    "      \n",
    "      CONSTRAINT oos_pk PRIMARY KEY (oos_id),\n",
    "      \n",
    "      CONSTRAINT call_unit_oos_fk FOREIGN KEY (call_unit_id) REFERENCES call_unit (call_unit_id),\n",
    "      CONSTRAINT oos_code_oos_fk FOREIGN KEY (oos_code_id) REFERENCES oos_code (oos_code_id)\n",
    "    );\n",
    "    \"\"\")\n",
    "      \n",
    "    \n",
    "reset_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Small lookup tables\n",
    "case_status, division, unit, bureau, investigation_status, call_source, and close_code\n",
    "\n",
    "The nested lookup tables are weapon/weapon_group and premise/premise_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading outserv.oscode.tsv into oos_code\n"
     ]
    }
   ],
   "source": [
    "# There are a million of these, so let's make life easier and reuse all that code\n",
    "\n",
    "# We need to save the mapping between DPD's short codes and our database ids so we can apply it to the records\n",
    "# in the main tables\n",
    "#\n",
    "# These have the DPD's codes as keys and our internal database PKs as values\n",
    "case_status_code_mapping = {}\n",
    "division_code_mapping = {}\n",
    "unit_code_mapping = {}\n",
    "bureau_code_mapping = {}\n",
    "investigation_status_code_mapping = {}\n",
    "call_source_code_mapping = {}\n",
    "close_code_mapping = {}\n",
    "oos_code_mapping = {}\n",
    "\n",
    "lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.CSSTATUS.csv\",\n",
    "        \"table\": \"case_status\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": case_status_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMDIVISION.csv\",\n",
    "        \"table\": \"division\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": division_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMSECTION.csv\",\n",
    "        \"table\": \"unit\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": unit_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.EMUNIT.csv\",\n",
    "        \"table\": \"bureau\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": bureau_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.INVSTSTATS.csv\",\n",
    "        \"table\": \"investigation_status\",\n",
    "        \"mapping\": {\"descriptn\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": investigation_status_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"inmain.callsource.tsv\",\n",
    "        \"table\": \"call_source\",\n",
    "        \"mapping\": {\"Description\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": call_source_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"inmain.closecode.tsv\",\n",
    "        \"table\": \"close_code\",\n",
    "        \"mapping\": {\"Description\": \"descr\"},\n",
    "        \"code_column\": \"code_agcy\",\n",
    "        \"code_mapping\": close_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"outserv.oscode.tsv\",\n",
    "        \"table\": \"oos_code\",\n",
    "        \"mapping\": {\"Description\": \"descr\"},\n",
    "        \"code_column\": \"Code\",\n",
    "        \"code_mapping\": oos_code_mapping\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in lookup_jobs:\n",
    "    print(\"loading %s into %s\" % (job['file'], job['table']))\n",
    "    \n",
    "    if job['file'].endswith(\".csv\"):\n",
    "        data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    elif job['file'].endswith(\".tsv\"):\n",
    "        data = pd.read_csv(\"../csv_data/%s\" % (job['file']), sep='\\t')\n",
    "    \n",
    "    # Keep track of the ids, as the data is ordered, so these will be the same assigned by the incrementing\n",
    "    # primary key in the database.\n",
    "    id_ = 1    \n",
    "    for (i,row) in data.iterrows():\n",
    "        job['code_mapping'][row[job['code_column']]] = id_\n",
    "        id_ += 1\n",
    "\n",
    "    # Keep only the desired columns\n",
    "    keep_columns = set(job['mapping'].keys())\n",
    "    for c in data.columns:\n",
    "        if c not in keep_columns:\n",
    "            data = data.drop(c, axis=1)\n",
    "            \n",
    "    # Change the column names to the ones we want and insert the data\n",
    "    data.rename(columns=job['mapping'], inplace=True)\n",
    "    data.to_sql(job['table'], engine, index=False, if_exists='append')\n",
    "    \n",
    "# They neglected to give us this code which is frequently in the database\n",
    "investigation_status_code_mapping['CBA'] = None\n",
    "\n",
    "# Some more that are in the db but not the lookup table they gave us\n",
    "for bogus_code in ('15:13.0', 'SLFIN', 'EYE', 'WALK', '911', 'A'):\n",
    "    call_source_code_mapping[bogus_code] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading LWMAIN.PREMISE.csv into premise and premise_group\n",
      "loading LWMAIN.WEAPON.csv into weapon and weapon_group\n"
     ]
    }
   ],
   "source": [
    "#These have to create \"nested\" tables and are a little tougher, but we can still reuse the code\n",
    "\n",
    "# Still need to keep track of the mappings\n",
    "weapon_code_mapping = {}\n",
    "premise_code_mapping = {}\n",
    "\n",
    "nested_lookup_jobs = [\n",
    "    {\n",
    "        \"file\": \"LWMAIN.PREMISE.csv\",\n",
    "        \"outer_table\": \"premise\",\n",
    "        \"inner_table\": \"premise_group\",\n",
    "        \"outer_cols\": [\"premise_group_id\",\"descr\"],\n",
    "        \"inner_col\": \"descr\",\n",
    "        \"inner_id\": \"premise_group_id\",\n",
    "        \"code_mapping\": premise_code_mapping\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"LWMAIN.WEAPON.csv\",\n",
    "        \"outer_table\": \"weapon\",\n",
    "        \"inner_table\": \"weapon_group\",\n",
    "        \"outer_cols\": [\"weapon_group_id\",\"descr\"],\n",
    "        \"inner_col\": \"descr\",\n",
    "        \"inner_id\": \"weapon_group_id\",\n",
    "        \"code_mapping\": weapon_code_mapping\n",
    "    }\n",
    "]\n",
    "\n",
    "for job in nested_lookup_jobs:\n",
    "    print(\"loading %s into %s and %s\" % (job['file'], job['outer_table'], job['inner_table']))\n",
    "    data = pd.read_csv(\"../csv_data/%s\" % (job['file']))\n",
    "    \n",
    "    # load the group table by getting all the unique groups\n",
    "    inner_data = data['descriptn_a'].drop_duplicates()\n",
    "    inner_data.name = job['inner_col']\n",
    "    inner_data.to_sql(job['inner_table'], engine, index=False, if_exists='append')\n",
    "    \n",
    "    # Learn the mapping between groups and group_ids in the database so we can insert the proper\n",
    "    # group_ids with the outer tables\n",
    "    groups = {}\n",
    "    for row in db.query(\"SELECT * FROM %s\" % (job['inner_table'])):\n",
    "        groups[row[job['inner_col']]] = row[job['inner_id']]\n",
    "       \n",
    "    # Figure out what the database ids will be, so we can convert DPD's columns to the database ids in the\n",
    "    # main table load\n",
    "    id_ = 1\n",
    "    for (i,row) in data.iterrows():\n",
    "        job['code_mapping'][row['code_agcy']] = id_\n",
    "        id_ += 1\n",
    "    \n",
    "    # Concatenate and rename the series we want\n",
    "    outer_data = pd.concat([data['descriptn_a'], data['descriptn_b']], axis=1, keys=job['outer_cols'])\n",
    "    \n",
    "    # use the groups mapping to turn group names into ids from our database\n",
    "    outer_data[job['inner_id']] = outer_data[job['inner_id']].map(lambda x: groups[x])\n",
    "    \n",
    "    # Store the records\n",
    "    outer_data.to_sql(job['outer_table'], engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmain.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lookup tables\n",
    "city and ucr_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "chunksize=20000\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "city = pd.DataFrame({'descr': []})\n",
    "ucr_descr_pairs = pd.DataFrame({'short_descr':[], 'long_descr': []})\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "# We'll start out by doing a pass through the file and loading the lookup tables we need (ucr_descr, city)\n",
    "for incident in pd.read_csv('../csv_data/cfs_2014_lwmain.csv', chunksize=chunksize, \n",
    "                       iterator=True, encoding='ISO-8859-1', low_memory=False):\n",
    "    \n",
    "    #Strip extraneous white space and turn resulting blanks into NULLs\n",
    "    incident = incident.applymap(safe_strip).applymap(lambda x: None if x == '' or pd.isnull(x) else x)\n",
    "    \n",
    "    # Turn the ucr_descrs into pairs, since it's the pairs that are unique\n",
    "    ucr_descr_pairs = ucr_descr_pairs.append(\n",
    "        pd.concat([incident['arr_chrg'], incident['chrgdesc']], axis=1).rename(\n",
    "            columns={'arr_chrg': 'short_descr', 'chrgdesc': 'long_descr'}))\n",
    "    ucr_descr_pairs = ucr_descr_pairs.drop_duplicates()\n",
    "    \n",
    "    # Add the cities in the current chunk to the dataframe\n",
    "    city = pd.concat([city, pd.DataFrame(incident['city']).rename(columns={'city':'descr'})], axis=0)\n",
    "    city = city.drop_duplicates()\n",
    "\n",
    "# we don't need nulls in a lookup table\n",
    "city = city[~city.descr.isnull()]\n",
    "\n",
    "#store the records\n",
    "city.to_sql('city', engine, index=False, if_exists='append')\n",
    "ucr_descr_pairs.to_sql('ucr_descr', engine, index=False, if_exists='append')\n",
    "\n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main table\n",
    "incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 seconds: completed 10000 rows\n",
      "83 seconds: completed 20000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv    \n",
    "\n",
    "def combine_date_time(str_date, str_time):\n",
    "    date = dt.datetime.strptime(str_date, \"%m/%d/%y\")\n",
    "    time = dt.datetime.strptime(str_time, \"%I:%M %p\")\n",
    "    return dt.datetime(date.year, date.month, date.day, time.hour, time.minute)\n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else d\n",
    "\n",
    "# We have several columns we need to convert to int that can also be None\n",
    "def safe_int(x):\n",
    "    return int(x) if x else None\n",
    "\n",
    "att_com_mapping = {\n",
    "    'COM': True,\n",
    "    'ATT': False,\n",
    "    '': None\n",
    "}\n",
    "\n",
    "city_code_mapping = {}\n",
    "ucr_descr_code_mapping = {}\n",
    "\n",
    "# Populate the mappings from the database\n",
    "for row in db.query(\"SELECT * FROM city;\"):\n",
    "    city_code_mapping[row['descr']] = row['city_id']\n",
    "\n",
    "# the pairs of short/long_descr are unique, so that needs to be our key\n",
    "for row in db.query(\"SELECT * FROM ucr_descr;\"):\n",
    "    ucr_descr_code_mapping[(row['short_descr'], row['long_descr'])] = row['ucr_descr_id']\n",
    "\n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "incident = db['incident']\n",
    "db.begin()\n",
    "\n",
    "try:\n",
    "    with open('../csv_data/cfs_2014_lwmain.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "            \n",
    "            #for i in range(len(header)):\n",
    "            #    print(i, header[i])\n",
    "            \n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "            time_filed = combine_date_time(row[2], row[3])\n",
    "            db_row = {\n",
    "                'incident_id': safe_int(row[0]),\n",
    "                'case_id': safe_int(row[1]),\n",
    "                'time_filed': time_filed,\n",
    "                'month_filed': time_filed.month,\n",
    "                'week_filed': time_filed.isocalendar()[1],\n",
    "                'dow_filed': time_filed.weekday(),\n",
    "                'street_num': row[7],\n",
    "                'street_name': row[8],\n",
    "                'city_id': safe_map(city_code_mapping, row[9]),\n",
    "                'zip': None if row[10] == 'NC' else row[10],\n",
    "                'geox': row[11],\n",
    "                'geoy': row[12],\n",
    "                'beat': row[13],\n",
    "                'district': row[14],\n",
    "                'sector': row[15],\n",
    "                'premise_id': safe_map(premise_code_mapping, safe_int(row[16])),\n",
    "                'weapon_id': safe_map(weapon_code_mapping, safe_int(row[17])),\n",
    "                'domestic': True if row[18]=='Y' else False if row[18]=='N' else None,\n",
    "                'juvenile': True if row[19]=='Y' else False if row[19]=='N' else None,\n",
    "                'gang_related': True if row[20]=='YES' else False if row[20]=='NO' else None,\n",
    "                'emp_bureau_id': safe_map(bureau_code_mapping, row[21]),\n",
    "                'emp_division_id': safe_map(division_code_mapping, row[22]),\n",
    "                'emp_unit_id': safe_map(unit_code_mapping, row[23]),\n",
    "                'num_officers': (lambda x: None if x in ('',None) else safe_int(x))(row[24]),\n",
    "                'investigation_status_id': safe_map(investigation_status_code_mapping,row[25]),\n",
    "                'investigator_unit_id': safe_map(unit_code_mapping, row[26]),\n",
    "                'case_status_id': safe_map(case_status_code_mapping, safe_int(row[27])),\n",
    "                'ucr_code': row[30],\n",
    "                'ucr_descr_id': safe_map(ucr_descr_code_mapping, (row[31],row[32])),\n",
    "                'committed': safe_map(att_com_mapping, row[33])\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # have to insert one by one to properly handle the duplicate PKs in the data\n",
    "                db.query(\"SAVEPOINT integrity_checkpoint;\")\n",
    "                incident.insert(db_row, ensure=False) # we know the right columns are already there\n",
    "            except IntegrityError:\n",
    "                #ignore the duplicate pks; the lower chrgid comes first, so we already have the record we want\n",
    "                #postgres complains if we keep inserting records into an aborted transaction\n",
    "                db.query(\"ROLLBACK TO SAVEPOINT integrity_checkpoint;\")\n",
    "                \n",
    "            db.query(\"RELEASE SAVEPOINT integrity_checkpoint;\")\n",
    "            \n",
    "            j+=1\n",
    "            if j % 10000 == 0:\n",
    "                print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))\n",
    "            \n",
    "            \n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_lwmodop.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lookup tables\n",
    "I would use pandas for this, as it's similar to the other lookup tables, but something about pandas' read_csv function creates weird escape characters with this file only.\n",
    "The only lookup table for this file is mo_item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "chunksize=10000\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "mo_item_pairs = set()\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('../csv_data/cfs_2014_lwmodop.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        if first_row:\n",
    "            header = row\n",
    "            first_row = False\n",
    "            continue\n",
    "\n",
    "        #for i in range(len(header)):\n",
    "        #    print(i, header[i])\n",
    "\n",
    "        # Strip whitespace and convert empty strings to None\n",
    "        row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "        mo_item_pairs.add((row[3], row[5]))\n",
    "            \n",
    "try:\n",
    "    mo_item = db['mo_item']\n",
    "    db.begin()\n",
    "    for pair in mo_item_pairs:\n",
    "        mo_item.insert({'group_descr': pair[0], 'item_descr': pair[1]})\n",
    "    db.commit()\n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "    \n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main table\n",
    "modus_operandi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 seconds: completed 10000 rows\n",
      "8 seconds: completed 20000 rows\n",
      "12 seconds: completed 30000 rows\n",
      "16 seconds: completed 40000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv    \n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else d\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "# Populate the mappings from the database\n",
    "mo_item_code_mapping = {}\n",
    "\n",
    "# the pairs of group/item_descr are unique, so that needs to be our key\n",
    "for row in db.query(\"SELECT * FROM mo_item;\"):\n",
    "    mo_item_code_mapping[(row['group_descr'], row['item_descr'])] = (row['mo_group_id'], row['mo_item_id'])\n",
    "\n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "mo = db['modus_operandi']\n",
    "db_rows = []\n",
    "db.begin()\n",
    "\n",
    "try:\n",
    "    with open('../csv_data/cfs_2014_lwmodop.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "            \n",
    "            #for i in range(len(header)):\n",
    "            #    print(i, header[i])\n",
    "            \n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "            mo_pair = safe_map(mo_item_code_mapping, (row[3], row[5]))\n",
    "            db_row = {\n",
    "                'incident_id': row[0],\n",
    "                'mo_id': row[1],\n",
    "                'mo_group_id': mo_pair[0],\n",
    "                'mo_item_id': mo_pair[1]\n",
    "            }\n",
    "            \n",
    "            # have to insert one by one to properly handle the duplicate PKs in the data\n",
    "            mo.insert(db_row, ensure=False) # we know the right columns are already there\n",
    "            \n",
    "            j+=1\n",
    "            if j % 10000 == 0:\n",
    "                print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))    \n",
    "            \n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_inmain.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lookup tables\n",
    "We did close_code and call_source earlier with the other tables that come directly from a .csv file.\n",
    "\n",
    "We'll need to do a full load of nature and call_unit, and we need to update city with the new cities in this file even though we loaded it previously.  Also need to do a pass through all the notes to get the note_authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "timestamp_expr = re.compile(\"(.*?)\\[(\\d{2}/\\d{2}/(?:\\d{2}|\\d{4}) \\d{2}:\\d{2}:\\d{2}) (.*?)\\]\")\n",
    "\n",
    "def split_notes(notes):\n",
    "    \"\"\"\n",
    "    Return a list of tuples.  Each tuple represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    tuples = []\n",
    "    regex_split = re.findall(timestamp_expr, notes)\n",
    "    for tup in regex_split:\n",
    "        text = tup[0].split()\n",
    "        text = text if text else None  # turn blanks into null\n",
    "        try:\n",
    "            timestamp = dt.datetime.strptime(tup[1], \"%m/%d/%y %H:%M:%S\")\n",
    "        except ValueError: # 4 digit year\n",
    "            timestamp = dt.datetime.strptime(tup[1], \"%m/%d/%Y %H:%M:%S\")\n",
    "        author = tup[2] if tup[2] else None\n",
    "        tuples.append((text, timestamp, author))\n",
    "    return tuples\n",
    "    \n",
    "natures = set()\n",
    "call_units = set()\n",
    "note_authors = set()\n",
    "\n",
    "cities = set()\n",
    "db_cities = set()\n",
    "# we already have most of the cities from the incident data, but there are more\n",
    "for row in db.query(\"SELECT * FROM city;\"):\n",
    "    db_cities.add(row['descr'])\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('../csv_data/cfs_2014_inmain.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "    reader = csv.reader(f)\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        if first_row:\n",
    "            header = row\n",
    "            first_row = False\n",
    "            continue\n",
    "            \n",
    "        # Strip whitespace and convert empty strings to None\n",
    "        row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "        if row[23]:\n",
    "            natures.add(row[23])\n",
    "        for d in (row[5], row[6], row[50]): # these all reference the call_unit table\n",
    "            call_units.add(d)\n",
    "        if row[27]:\n",
    "            for note in split_notes(row[27]):\n",
    "                if note[2] is not None:\n",
    "                    note_authors.add(note[2])\n",
    "        if row[10] and row[10] not in db_cities:\n",
    "            cities.add(row[10])\n",
    "            \n",
    "try:\n",
    "    nature = db['nature']\n",
    "    db.begin()\n",
    "    for n in natures:\n",
    "        nature.insert({'descr': n})\n",
    "    db.commit()\n",
    "    \n",
    "    call_unit = db['call_unit']\n",
    "    db.begin()\n",
    "    for c in call_units:\n",
    "        call_unit.insert({'descr': c})\n",
    "    db.commit()\n",
    "    \n",
    "    note_author = db['note_author']\n",
    "    db.begin()\n",
    "    for na in note_authors:\n",
    "        note_author.insert({'descr': na})\n",
    "    db.commit()\n",
    "    \n",
    "    city = db['city']\n",
    "    db.begin()\n",
    "    for c in cities:\n",
    "        city.insert({'descr': c})\n",
    "    db.commit()\n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "    \n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main table\n",
    "call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 seconds: completed 10000 rows\n",
      "88 seconds: completed 20000 rows\n",
      "131 seconds: completed 30000 rows\n",
      "176 seconds: completed 40000 rows\n",
      "220 seconds: completed 50000 rows\n",
      "264 seconds: completed 60000 rows\n",
      "311 seconds: completed 70000 rows\n",
      "353 seconds: completed 80000 rows\n",
      "399 seconds: completed 90000 rows\n",
      "445 seconds: completed 100000 rows\n",
      "490 seconds: completed 110000 rows\n",
      "536 seconds: completed 120000 rows\n",
      "580 seconds: completed 130000 rows\n",
      "627 seconds: completed 140000 rows\n",
      "675 seconds: completed 150000 rows\n",
      "722 seconds: completed 160000 rows\n",
      "770 seconds: completed 170000 rows\n",
      "816 seconds: completed 180000 rows\n",
      "862 seconds: completed 190000 rows\n",
      "909 seconds: completed 200000 rows\n",
      "956 seconds: completed 210000 rows\n",
      "1001 seconds: completed 220000 rows\n",
      "1047 seconds: completed 230000 rows\n",
      "1094 seconds: completed 240000 rows\n",
      "1140 seconds: completed 250000 rows\n",
      "1186 seconds: completed 260000 rows\n",
      "1232 seconds: completed 270000 rows\n",
      "1279 seconds: completed 280000 rows\n",
      "1326 seconds: completed 290000 rows\n",
      "1373 seconds: completed 300000 rows\n",
      "1419 seconds: completed 310000 rows\n",
      "1466 seconds: completed 320000 rows\n",
      "1514 seconds: completed 330000 rows\n",
      "1560 seconds: completed 340000 rows\n",
      "1607 seconds: completed 350000 rows\n",
      "1653 seconds: completed 360000 rows\n",
      "1702 seconds: completed 370000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "timestamp_expr = re.compile(\"(.*?)\\[(\\d{2}/\\d{2}/(?:\\d{2}|\\d{4}) \\d{2}:\\d{2}:\\d{2}) (.*?)\\]\")\n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else None\n",
    "\n",
    "# We have several columns we need to convert to int that can also be None\n",
    "def safe_int(x):\n",
    "    return int(x) if x else None\n",
    "\n",
    "def safe_float(x):\n",
    "    return float(x) if x else None\n",
    "\n",
    "def safe_bool(x):\n",
    "    return True if x == '1' else False if x == '0' else None\n",
    "\n",
    "def safe_datetime(x):\n",
    "    # to_datetime returns a pandas Timestamp object, and we want a vanilla datetime\n",
    "    return pd.to_datetime(x).to_datetime() if x not in ('NULL', None) else None\n",
    "\n",
    "def clean_case_id(c):\n",
    "    if c:\n",
    "        c = str(c).replace('-','').replace(' ','')\n",
    "        try:\n",
    "            return int(c)\n",
    "        except ValueError: #got some weird rows with non-digits in the case_id that def. won't map back to incident\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def split_notes(notes):\n",
    "    \"\"\"\n",
    "    Return a list of tuples.  Each tuple represents a single note and contains the corresponding call_id,\n",
    "    the timestamp, the note-taker, and the text of the note.\n",
    "    \"\"\"\n",
    "    tuples = []\n",
    "    if notes is None:\n",
    "        return []\n",
    "    regex_split = re.findall(timestamp_expr, notes)\n",
    "    for tup in regex_split:\n",
    "        text = tup[0].strip()\n",
    "        text = text if text else None  # turn blanks into null\n",
    "        try:\n",
    "            timestamp = dt.datetime.strptime(tup[1], \"%m/%d/%y %H:%M:%S\")\n",
    "        except ValueError: # 4 digit year\n",
    "            timestamp = dt.datetime.strptime(tup[1], \"%m/%d/%Y %H:%M:%S\")\n",
    "        author = tup[2]\n",
    "        tuples.append((text, timestamp, author))\n",
    "    return tuples\n",
    "\n",
    "nature_code_mapping = {}\n",
    "call_unit_code_mapping = {}\n",
    "note_author_mapping = {}\n",
    "city_code_mapping = {}\n",
    "\n",
    "# Populate the mappings from the database\n",
    "for row in db.query(\"SELECT * FROM nature;\"):\n",
    "    nature_code_mapping[row['descr']] = row['nature_id']\n",
    "\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    call_unit_code_mapping[row['descr']] = row['call_unit_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM note_author;\"):\n",
    "    note_author_mapping[row['descr']] = row['note_author_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM city;\"):\n",
    "    city_code_mapping[row['descr']] = row['city_id']\n",
    "\n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "note_authors_set = set()\n",
    "\n",
    "call_rows = []\n",
    "note_rows = []\n",
    "call = db['call']\n",
    "note = db['note']\n",
    "db.begin()\n",
    "\n",
    "try:\n",
    "    with open('../csv_data/cfs_2014_inmain.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "            \n",
    "            #for i in range(len(header)):\n",
    "            #    print(i, header[i])\n",
    "            \n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "            time_received = pd.to_datetime(row[1]).to_datetime()  # we need a datetime, not a pandas timestamp\n",
    "            call_id = safe_int(row[0]) # going to be using this again with the notes\n",
    "            db_row = {\n",
    "                'call_id': call_id,\n",
    "                'time_received': time_received,\n",
    "                'hour_received': time_received.hour,\n",
    "                'month_received': time_received.month,\n",
    "                'week_received': time_received.isocalendar()[1],\n",
    "                'dow_received': time_received.weekday(),\n",
    "                'case_id': clean_case_id(row[3]),\n",
    "                'call_source_id': safe_map(call_source_code_mapping, row[4]),\n",
    "                'primary_unit_id': safe_map(call_unit_code_mapping, row[5]),\n",
    "                'first_dispatched_id': safe_map(call_unit_code_mapping, row[6]),\n",
    "                'reporting_unit_id': safe_map(call_unit_code_mapping, row[50]),\n",
    "                'street_num': safe_int(row[7]),\n",
    "                'street_name': row[8],\n",
    "                'city_id': safe_map(city_code_mapping, row[10]),\n",
    "                'zip': safe_int(row[11]),\n",
    "                'crossroad1': row[12],\n",
    "                'crossroad2': row[13],\n",
    "                'geox': safe_float(row[14]),\n",
    "                'geoy': safe_float(row[15]),\n",
    "                'beat': row[18],\n",
    "                'district': row[19],\n",
    "                'sector': row[20],\n",
    "                'business': row[21],\n",
    "                'nature_id': safe_map(nature_code_mapping, row[23]),\n",
    "                'priority': row[24],\n",
    "                'report_only': safe_bool(row[25]),\n",
    "                'cancelled': safe_bool(row[26]),\n",
    "                'time_routed': safe_datetime(row[28]),\n",
    "                'time_finished': safe_datetime(row[30]),\n",
    "                'first_unit_dispatch': safe_datetime(row[32]),\n",
    "                'first_unit_enroute': safe_datetime(row[36]),\n",
    "                'first_unit_arrive': safe_datetime(row[39]),\n",
    "                'first_unit_transport': safe_datetime(row[42]),\n",
    "                'last_unit_clear': safe_datetime(row[45]),\n",
    "                'time_closed': safe_datetime(row[49]),\n",
    "                'close_code_id': safe_map(close_code_mapping, row[51]),\n",
    "                'close_comm': row[52]\n",
    "            }\n",
    "            notes = split_notes(row[27])\n",
    "            \n",
    "            #try:\n",
    "#               have to insert one by one to properly handle the duplicate PKs in the data\n",
    "#               db.query(\"SAVEPOINT integrity_checkpoint;\")\n",
    "#               call.insert(db_row, ensure=False) # we know the right columns are already there\n",
    "            call_rows.append(db_row)\n",
    "            \n",
    "            for n in notes:\n",
    "                note_author_mapped = safe_map(note_author_mapping, n[2])\n",
    "                note_db_row = {'body': n[0],\n",
    "                             'time_recorded': n[1],\n",
    "                             'call_id': call_id,\n",
    "                             'note_author_id': note_author_mapped}\n",
    "                #note.insert(note_db_row, ensure=False)\n",
    "                note_rows.append(note_db_row)\n",
    "            #except IntegrityError:\n",
    "                # we seem to be missing some incident data\n",
    "                #print(\"insert of call_id %d failed due to integrity error\" % (call_id))\n",
    "                #db.query(\"ROLLBACK TO SAVEPOINT integrity_checkpoint;\")\n",
    "                \n",
    "            #db.query(\"RELEASE SAVEPOINT integrity_checkpoint;\")\n",
    "            \n",
    "            \n",
    "\n",
    "            j+=1\n",
    "            if j % 10000 == 0:\n",
    "                call.insert_many(call_rows, chunk_size=10000, ensure=False)\n",
    "                note.insert_many(note_rows, chunk_size=10000, ensure=False)\n",
    "                call_rows = []\n",
    "                note_rows = []\n",
    "                print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))\n",
    "            \n",
    "    call.insert_many(call_rows, ensure=False)\n",
    "    note.insert_many(note_rows, ensure=False)\n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_xxx2014_incilog.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lookup tables\n",
    "Need to update call_unit and do a full load of transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "months = (\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\")\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "\n",
    "transactions = set()\n",
    "\n",
    "call_units = set()\n",
    "db_call_units = set()\n",
    "# we already have most of the call_units from the call data, but there are more\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    db_call_units.add(row['descr'])\n",
    "\n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "for month in months:  \n",
    "\n",
    "    with open('../csv_data/cfs_%s2014_incilog.csv' % (month), 'r', encoding='ISO-8859-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "\n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "            if row[2]:\n",
    "                transactions.add(row[2])\n",
    "            \n",
    "            if row[6] and row[6] not in db_call_units:\n",
    "                call_units.add(row[6])\n",
    "\n",
    "try:\n",
    "    call_unit = db['call_unit']\n",
    "    db.begin()\n",
    "    for c in call_units:\n",
    "        call_unit.insert({'descr': c})\n",
    "    db.commit()\n",
    "\n",
    "    transaction = db['transaction']\n",
    "    db.begin()\n",
    "    for t in transactions:\n",
    "        transaction.insert({'descr': t})\n",
    "    db.commit()\n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "\n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main table\n",
    "call_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for jan\n",
      "9 seconds: completed 10000 rows\n",
      "18 seconds: completed 20000 rows\n",
      "27 seconds: completed 30000 rows\n",
      "35 seconds: completed 40000 rows\n",
      "44 seconds: completed 50000 rows\n",
      "52 seconds: completed 60000 rows\n",
      "61 seconds: completed 70000 rows\n",
      "70 seconds: completed 80000 rows\n",
      "78 seconds: completed 90000 rows\n",
      "87 seconds: completed 100000 rows\n",
      "96 seconds: completed 110000 rows\n",
      "106 seconds: completed 120000 rows\n",
      "115 seconds: completed 130000 rows\n",
      "124 seconds: completed 140000 rows\n",
      "132 seconds: completed 150000 rows\n",
      "141 seconds: completed 160000 rows\n",
      "150 seconds: completed 170000 rows\n",
      "loading data for feb\n",
      "159 seconds: completed 180000 rows\n",
      "167 seconds: completed 190000 rows\n",
      "176 seconds: completed 200000 rows\n",
      "185 seconds: completed 210000 rows\n",
      "194 seconds: completed 220000 rows\n",
      "203 seconds: completed 230000 rows\n",
      "212 seconds: completed 240000 rows\n",
      "221 seconds: completed 250000 rows\n",
      "230 seconds: completed 260000 rows\n",
      "238 seconds: completed 270000 rows\n",
      "247 seconds: completed 280000 rows\n",
      "257 seconds: completed 290000 rows\n",
      "266 seconds: completed 300000 rows\n",
      "275 seconds: completed 310000 rows\n",
      "284 seconds: completed 320000 rows\n",
      "loading data for mar\n",
      "293 seconds: completed 330000 rows\n",
      "302 seconds: completed 340000 rows\n",
      "311 seconds: completed 350000 rows\n",
      "320 seconds: completed 360000 rows\n",
      "330 seconds: completed 370000 rows\n",
      "338 seconds: completed 380000 rows\n",
      "347 seconds: completed 390000 rows\n",
      "356 seconds: completed 400000 rows\n",
      "365 seconds: completed 410000 rows\n",
      "374 seconds: completed 420000 rows\n",
      "383 seconds: completed 430000 rows\n",
      "392 seconds: completed 440000 rows\n",
      "401 seconds: completed 450000 rows\n",
      "410 seconds: completed 460000 rows\n",
      "419 seconds: completed 470000 rows\n",
      "428 seconds: completed 480000 rows\n",
      "loading data for apr\n",
      "437 seconds: completed 490000 rows\n",
      "446 seconds: completed 500000 rows\n",
      "455 seconds: completed 510000 rows\n",
      "464 seconds: completed 520000 rows\n",
      "473 seconds: completed 530000 rows\n",
      "482 seconds: completed 540000 rows\n",
      "491 seconds: completed 550000 rows\n",
      "500 seconds: completed 560000 rows\n",
      "509 seconds: completed 570000 rows\n",
      "518 seconds: completed 580000 rows\n",
      "527 seconds: completed 590000 rows\n",
      "537 seconds: completed 600000 rows\n",
      "545 seconds: completed 610000 rows\n",
      "554 seconds: completed 620000 rows\n",
      "563 seconds: completed 630000 rows\n",
      "572 seconds: completed 640000 rows\n",
      "581 seconds: completed 650000 rows\n",
      "loading data for may\n",
      "590 seconds: completed 660000 rows\n",
      "599 seconds: completed 670000 rows\n",
      "608 seconds: completed 680000 rows\n",
      "617 seconds: completed 690000 rows\n",
      "626 seconds: completed 700000 rows\n",
      "634 seconds: completed 710000 rows\n",
      "643 seconds: completed 720000 rows\n",
      "652 seconds: completed 730000 rows\n",
      "661 seconds: completed 740000 rows\n",
      "670 seconds: completed 750000 rows\n",
      "679 seconds: completed 760000 rows\n",
      "688 seconds: completed 770000 rows\n",
      "697 seconds: completed 780000 rows\n",
      "706 seconds: completed 790000 rows\n",
      "715 seconds: completed 800000 rows\n",
      "724 seconds: completed 810000 rows\n",
      "733 seconds: completed 820000 rows\n",
      "742 seconds: completed 830000 rows\n",
      "loading data for jun\n",
      "752 seconds: completed 840000 rows\n",
      "761 seconds: completed 850000 rows\n",
      "769 seconds: completed 860000 rows\n",
      "778 seconds: completed 870000 rows\n",
      "787 seconds: completed 880000 rows\n",
      "796 seconds: completed 890000 rows\n",
      "805 seconds: completed 900000 rows\n",
      "815 seconds: completed 910000 rows\n",
      "824 seconds: completed 920000 rows\n",
      "833 seconds: completed 930000 rows\n",
      "842 seconds: completed 940000 rows\n",
      "851 seconds: completed 950000 rows\n",
      "860 seconds: completed 960000 rows\n",
      "869 seconds: completed 970000 rows\n",
      "878 seconds: completed 980000 rows\n",
      "887 seconds: completed 990000 rows\n",
      "loading data for jul\n",
      "897 seconds: completed 1000000 rows\n",
      "905 seconds: completed 1010000 rows\n",
      "915 seconds: completed 1020000 rows\n",
      "924 seconds: completed 1030000 rows\n",
      "933 seconds: completed 1040000 rows\n",
      "942 seconds: completed 1050000 rows\n",
      "951 seconds: completed 1060000 rows\n",
      "960 seconds: completed 1070000 rows\n",
      "969 seconds: completed 1080000 rows\n",
      "978 seconds: completed 1090000 rows\n",
      "986 seconds: completed 1100000 rows\n",
      "995 seconds: completed 1110000 rows\n",
      "1004 seconds: completed 1120000 rows\n",
      "1013 seconds: completed 1130000 rows\n",
      "loading data for aug\n",
      "1022 seconds: completed 1140000 rows\n",
      "1032 seconds: completed 1150000 rows\n",
      "1040 seconds: completed 1160000 rows\n",
      "1049 seconds: completed 1170000 rows\n",
      "1058 seconds: completed 1180000 rows\n",
      "1067 seconds: completed 1190000 rows\n",
      "1076 seconds: completed 1200000 rows\n",
      "1085 seconds: completed 1210000 rows\n",
      "1094 seconds: completed 1220000 rows\n",
      "1103 seconds: completed 1230000 rows\n",
      "1112 seconds: completed 1240000 rows\n",
      "1121 seconds: completed 1250000 rows\n",
      "1130 seconds: completed 1260000 rows\n",
      "1139 seconds: completed 1270000 rows\n",
      "1148 seconds: completed 1280000 rows\n",
      "loading data for sep\n",
      "1157 seconds: completed 1290000 rows\n",
      "1166 seconds: completed 1300000 rows\n",
      "1175 seconds: completed 1310000 rows\n",
      "1184 seconds: completed 1320000 rows\n",
      "1193 seconds: completed 1330000 rows\n",
      "1202 seconds: completed 1340000 rows\n",
      "1211 seconds: completed 1350000 rows\n",
      "1220 seconds: completed 1360000 rows\n",
      "1229 seconds: completed 1370000 rows\n",
      "1238 seconds: completed 1380000 rows\n",
      "1247 seconds: completed 1390000 rows\n",
      "1256 seconds: completed 1400000 rows\n",
      "1265 seconds: completed 1410000 rows\n",
      "1274 seconds: completed 1420000 rows\n",
      "loading data for oct\n",
      "1283 seconds: completed 1430000 rows\n",
      "1292 seconds: completed 1440000 rows\n",
      "1300 seconds: completed 1450000 rows\n",
      "1309 seconds: completed 1460000 rows\n",
      "1318 seconds: completed 1470000 rows\n",
      "1327 seconds: completed 1480000 rows\n",
      "1336 seconds: completed 1490000 rows\n",
      "1345 seconds: completed 1500000 rows\n",
      "1354 seconds: completed 1510000 rows\n",
      "1363 seconds: completed 1520000 rows\n",
      "1373 seconds: completed 1530000 rows\n",
      "1381 seconds: completed 1540000 rows\n",
      "1390 seconds: completed 1550000 rows\n",
      "1399 seconds: completed 1560000 rows\n",
      "loading data for nov\n",
      "1408 seconds: completed 1570000 rows\n",
      "1417 seconds: completed 1580000 rows\n",
      "1426 seconds: completed 1590000 rows\n",
      "1435 seconds: completed 1600000 rows\n",
      "1443 seconds: completed 1610000 rows\n",
      "1452 seconds: completed 1620000 rows\n",
      "1461 seconds: completed 1630000 rows\n",
      "1470 seconds: completed 1640000 rows\n",
      "1479 seconds: completed 1650000 rows\n",
      "1488 seconds: completed 1660000 rows\n",
      "1497 seconds: completed 1670000 rows\n",
      "1506 seconds: completed 1680000 rows\n",
      "1515 seconds: completed 1690000 rows\n",
      "1524 seconds: completed 1700000 rows\n",
      "loading data for dec\n",
      "1533 seconds: completed 1710000 rows\n",
      "1542 seconds: completed 1720000 rows\n",
      "1550 seconds: completed 1730000 rows\n",
      "1560 seconds: completed 1740000 rows\n",
      "1569 seconds: completed 1750000 rows\n",
      "1579 seconds: completed 1760000 rows\n",
      "1588 seconds: completed 1770000 rows\n",
      "1597 seconds: completed 1780000 rows\n",
      "1607 seconds: completed 1790000 rows\n",
      "1616 seconds: completed 1800000 rows\n",
      "1626 seconds: completed 1810000 rows\n",
      "1635 seconds: completed 1820000 rows\n",
      "1644 seconds: completed 1830000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "months = (\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\")\n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else d\n",
    "\n",
    "def safe_strip(str_):\n",
    "    try:\n",
    "        return str_.strip()\n",
    "    except AttributeError:\n",
    "        return str_\n",
    "    \n",
    "def safe_int(x):\n",
    "    return int(x) if x else None\n",
    "\n",
    "def safe_datetime(x):\n",
    "    # to_datetime returns a pandas Timestamp object, and we want a vanilla datetime\n",
    "    return pd.to_datetime(x).to_datetime() if x not in ('NULL', None) else None\n",
    "\n",
    "# Populate the mappings from the database\n",
    "call_unit_mapping = {}\n",
    "transaction_code_mapping = {}\n",
    "\n",
    "# the pairs of group/item_descr are unique, so that needs to be our key\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    call_unit_mapping[row['descr']] = row['call_unit_id']\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM transaction;\"):\n",
    "    transaction_code_mapping[row['descr']] = row['transaction_id']\n",
    "    \n",
    "# We have fire and EMS call_log data, which we don't have calls for.  We need to ignore this data.\n",
    "valid_call_ids = set()\n",
    "for row in db.query(\"SELECT call_id FROM call;\"):\n",
    "    valid_call_ids.add(row['call_id'])\n",
    "    \n",
    "\n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "call_log = db['call_log']\n",
    "db_rows = []\n",
    "db.begin()\n",
    "\n",
    "try:\n",
    "    for month in months:\n",
    "        print(\"loading data for %s\" % (month))\n",
    "        with open('../csv_data/cfs_%s2014_incilog.csv' % (month), 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            first_row = True\n",
    "            for row in reader:\n",
    "                if first_row:\n",
    "                    header = row\n",
    "                    first_row = False\n",
    "                    continue\n",
    "\n",
    "                #for i in range(len(header)):\n",
    "                #    print(i, header[i])\n",
    "\n",
    "                # Strip whitespace and convert empty strings to None\n",
    "                row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "                \n",
    "                call_id = safe_int(row[5])\n",
    "                if call_id in valid_call_ids:\n",
    "                    db_rows.append({\n",
    "                        'call_log_id': safe_int(row[0]),\n",
    "                        'transaction_id': safe_map(transaction_code_mapping, row[2]),\n",
    "                        'time_recorded': safe_datetime(row[3]),\n",
    "                        'call_id': call_id,\n",
    "                        'call_unit_id': safe_map(call_unit_mapping, row[6]),\n",
    "                        'close_code_id': safe_map(close_code_mapping, row[9])\n",
    "                    })\n",
    "                    j+=1\n",
    "                    if j % 10000 == 0:\n",
    "                        call_log.insert_many(db_rows, chunk_size=10000, ensure=False)\n",
    "                        db_rows=[]\n",
    "                        print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))    \n",
    "    \n",
    "    call_log.insert_many(db_rows, ensure=False)\n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cfs_2014_outserv.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Lookup tables\n",
    "Need to update call_unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookup tables\n",
      "lookup tables loaded\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "call_units = set()\n",
    "db_call_units = set()\n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    db_call_units.add(row['descr'])\n",
    "    \n",
    "print(\"loading lookup tables\")\n",
    "\n",
    "with open('../csv_data/cfs_2014_outserv.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "    reader = csv.reader(f)\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        if first_row:\n",
    "            header = row\n",
    "            first_row = False\n",
    "            continue\n",
    "\n",
    "        # Strip whitespace and convert empty strings to None\n",
    "        row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "\n",
    "        if row[1] and row[1] not in db_call_units:\n",
    "            call_units.add(row[1])\n",
    "\n",
    "try:\n",
    "    call_unit = db['call_unit']\n",
    "    db.begin()\n",
    "    for c in call_units:\n",
    "        call_unit.insert({'descr': c})\n",
    "    db.commit()\n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e\n",
    "\n",
    "print(\"lookup tables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 seconds: completed 10000 rows\n",
      "13 seconds: completed 20000 rows\n",
      "19 seconds: completed 30000 rows\n",
      "26 seconds: completed 40000 rows\n",
      "33 seconds: completed 50000 rows\n",
      "39 seconds: completed 60000 rows\n",
      "46 seconds: completed 70000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# We have several columns we need to convert to int that can also be None\n",
    "def safe_int(x):\n",
    "    return int(x) if x else None\n",
    "\n",
    "def safe_datetime(x):\n",
    "    # to_datetime returns a pandas Timestamp object, and we want a vanilla datetime\n",
    "    return pd.to_datetime(x).to_datetime() if x not in ('NULL', None) else None\n",
    "\n",
    "# We'll use this to ensure we either map to a value of the foreign key or null\n",
    "def safe_map(m,d):\n",
    "    return m[d] if d else None\n",
    "\n",
    "call_unit_mapping = {}\n",
    "    \n",
    "for row in db.query(\"SELECT * FROM call_unit;\"):\n",
    "    call_unit_mapping[row['descr']] = row['call_unit_id']\n",
    "    \n",
    "start = dt.datetime.now()\n",
    "j = 0\n",
    "\n",
    "oos_rows = []\n",
    "\n",
    "oos = db['out_of_service']\n",
    "\n",
    "try:\n",
    "    with open('../csv_data/cfs_2014_outserv.csv', 'r', encoding='ISO-8859-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                header = row\n",
    "                first_row = False\n",
    "                continue\n",
    "            \n",
    "            # Strip whitespace and convert empty strings to None\n",
    "            row = list(map(lambda x: x if x else None, map(safe_strip, row)))\n",
    "            db_row = {\n",
    "                'oos_id': safe_int(row[0]),\n",
    "                'call_unit_id': safe_map(call_unit_mapping, row[1]),\n",
    "                'oos_code_id': safe_map(oos_code_mapping, row[2]),\n",
    "                'location': row[3],\n",
    "                'comments': row[4],\n",
    "                'start_time': safe_datetime(row[5]),\n",
    "                'end_time': safe_datetime(row[6]),\n",
    "                'duration': safe_datetime(row[6]) - safe_datetime(row[5])\n",
    "            }\n",
    "            oos_rows.append(db_row)\n",
    "            \n",
    "            j+=1\n",
    "            if j % 10000 == 0:\n",
    "                oos.insert_many(oos_rows, chunk_size=10000, ensure=False)\n",
    "                oos_rows = []\n",
    "                print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j))\n",
    "    oos.insert_many(oos_rows, ensure=False)\n",
    "    db.commit()\n",
    "            \n",
    "except Exception as e:\n",
    "    db.rollback()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Enable GIS extensions (PostGIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.persistence.util.ResultIter at 0x10b8da2b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(\"CREATE EXTENSION postgis;\")\n",
    "db.query(\"CREATE EXTENSION postgis_topology;\")\n",
    "db.query(\"CREATE EXTENSION fuzzystrmatch;\")\n",
    "db.query(\"CREATE EXTENSION postgis_tiger_geocoder;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use PostGIS to convert the NC state planar coordinates to latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.persistence.util.ResultIter at 0x10a5ace48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(\"\"\"DROP TABLE IF EXISTS call_latlong;\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "CREATE TABLE call_latlong AS (\n",
    "    SELECT call_id, st_x(point) AS longitude, st_y(point) AS latitude, point\n",
    "    FROM (\n",
    "        SELECT call_id, \n",
    "        st_Transform(ST_SetSRID(ST_MakePoint(geox, geoy), 2264), 4326)::geometry(Point, 4326) AS point\n",
    "        FROM call\n",
    "    ) AS a\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "db.query(\"\"\"DROP TABLE IF EXISTS incident_latlong;\"\"\")\n",
    "\n",
    "db.query(\"\"\"\n",
    "CREATE TABLE incident_latlong AS (\n",
    "    SELECT incident_id, st_x(point) AS longitude, st_y(point) AS latitude, point\n",
    "    FROM (\n",
    "        SELECT incident_id, \n",
    "        -- We have to divide incident x and y by 100 to get the proper numbers\n",
    "        st_Transform(ST_SetSRID(ST_MakePoint(geox/100, geoy/100), 2264), 4326)::geometry(Point, 4326) AS point\n",
    "        FROM incident\n",
    "    ) AS a\n",
    ");\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
